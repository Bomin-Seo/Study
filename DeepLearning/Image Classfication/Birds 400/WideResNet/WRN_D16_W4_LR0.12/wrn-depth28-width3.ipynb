{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport sys\nimport os\nimport time\nimport random \nimport pandas as pd\nimport torch\nfrom torch import nn, cuda, optim\nfrom torchvision import models,transforms,datasets\nfrom torch.utils.data import DataLoader,random_split\nfrom PIL import Image\nimport seaborn as sns\nimport torch.nn.functional as F\n\ndevice = 'cuda' if cuda.is_available() else 'cpu'","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-30T04:38:38.045246Z","iopub.execute_input":"2022-05-30T04:38:38.045603Z","iopub.status.idle":"2022-05-30T04:38:40.954979Z","shell.execute_reply.started":"2022-05-30T04:38:38.045569Z","shell.execute_reply":"2022-05-30T04:38:40.953980Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"data_dir = '../input/bird-data/Bird/train'\nclasses = []\nimg_per_class = []\n# for folder in os.listdir(data_dir+'consolidated'):/\nfor folder in os.listdir(data_dir):    \n    classes.append(folder)\n    img_per_class.append(len(os.listdir(f'{data_dir}/{folder}')))\nnum_classes = len(classes)\ndf = pd.DataFrame({'Classes':classes, 'Examples':img_per_class})\ndf","metadata":{"execution":{"iopub.status.busy":"2022-05-30T04:38:40.957286Z","iopub.execute_input":"2022-05-30T04:38:40.958189Z","iopub.status.idle":"2022-05-30T04:38:49.450701Z","shell.execute_reply.started":"2022-05-30T04:38:40.958151Z","shell.execute_reply":"2022-05-30T04:38:49.449879Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"class AverageMeter(object):\n    r\"\"\"Computes and stores the average and current value\n    \"\"\"\n    def __init__(self, name, fmt=':f'):\n        self.name = name\n        self.fmt = fmt\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\n    def __str__(self):\n        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n        return fmtstr.format(**self.__dict__)\n\n\nclass ProgressMeter(object):\n    def __init__(self, num_batches, *meters, prefix=\"\"):\n        self.batch_fmtstr = self._get_batch_fmtstr(num_batches)\n        self.meters = meters\n        self.prefix = prefix\n\n    def print(self, batch):\n        entries = [self.prefix + self.batch_fmtstr.format(batch)]\n        entries += [str(meter) for meter in self.meters]\n        print('\\t'.join(entries))\n\n    def _get_batch_fmtstr(self, num_batches):\n        num_digits = len(str(num_batches // 1))\n        fmt = '{:' + str(num_digits) + 'd}'\n        return '[' + fmt + '/' + fmt.format(num_batches) + ']'\n\n\ndef accuracy(output, target, topk=(1,)):\n    r\"\"\"Computes the accuracy over the $k$ top predictions for the specified values of k\n    \"\"\"\n    with torch.no_grad():\n        maxk = max(topk)\n        batch_size = target.size(0)\n\n        # _, pred = output.topk(maxk, 1, True, True)\n        # pred = pred.t()\n        # correct = pred.eq(target.view(1, -1).expand_as(pred))\n\n        # faster topk (ref: https://github.com/pytorch/pytorch/issues/22812)\n        _, idx = output.sort(descending=True)\n        pred = idx[:,:maxk]\n        pred = pred.t()\n        correct = pred.eq(target.view(1, -1).expand_as(pred))\n\n        res = []\n        for k in topk:\n            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n            res.append(correct_k.mul_(100.0 / batch_size))\n        return res","metadata":{"execution":{"iopub.status.busy":"2022-05-30T04:38:49.452060Z","iopub.execute_input":"2022-05-30T04:38:49.452443Z","iopub.status.idle":"2022-05-30T04:38:49.468657Z","shell.execute_reply.started":"2022-05-30T04:38:49.452407Z","shell.execute_reply":"2022-05-30T04:38:49.466485Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"%cd '../input/sam-wrn/sam-main/sam-main/example'","metadata":{"execution":{"iopub.status.busy":"2022-05-30T04:38:49.470583Z","iopub.execute_input":"2022-05-30T04:38:49.471094Z","iopub.status.idle":"2022-05-30T04:38:49.490041Z","shell.execute_reply.started":"2022-05-30T04:38:49.471060Z","shell.execute_reply":"2022-05-30T04:38:49.488995Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"import argparse\nimport torch\n\nfrom model.wide_res_net import WideResNet\nfrom model.smooth_cross_entropy import smooth_crossentropy\nfrom utility.log import Log\nfrom utility.initialize import initialize\nfrom utility.step_lr import StepLR\nfrom utility.bypass_bn import enable_running_stats, disable_running_stats\n\nimport sys; sys.path.append(\"..\")\nfrom sam import SAM","metadata":{"execution":{"iopub.status.busy":"2022-05-30T04:38:49.491360Z","iopub.execute_input":"2022-05-30T04:38:49.491702Z","iopub.status.idle":"2022-05-30T04:38:49.574737Z","shell.execute_reply.started":"2022-05-30T04:38:49.491668Z","shell.execute_reply":"2022-05-30T04:38:49.573918Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"%cd '/kaggle/working'","metadata":{"execution":{"iopub.status.busy":"2022-05-30T04:38:49.576066Z","iopub.execute_input":"2022-05-30T04:38:49.576644Z","iopub.status.idle":"2022-05-30T04:38:49.583659Z","shell.execute_reply.started":"2022-05-30T04:38:49.576607Z","shell.execute_reply":"2022-05-30T04:38:49.582624Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"parser = argparse.ArgumentParser()\nparser.add_argument(\"--adaptive\", default=True, type=bool, help=\"True if you want to use the Adaptive SAM.\")\nparser.add_argument(\"--batch_size\", default=32, type=int, help=\"Batch size used in the training and validation loop.\")\nparser.add_argument(\"--depth\", default=16, type=int, help=\"Number of layers.\")\nparser.add_argument(\"--dropout\", default=0.0, type=float, help=\"Dropout rate.\")\nparser.add_argument(\"--epochs\", default=50, type=int, help=\"Total number of epochs.\")\nparser.add_argument(\"--label_smoothing\", default=0.1, type=float, help=\"Use 0.0 for no label smoothing.\")\nparser.add_argument(\"--learning_rate\", default=0.12, type=float, help=\"Base learning rate at the start of the training.\")\nparser.add_argument(\"--momentum\", default=0.9, type=float, help=\"SGD Momentum.\")\nparser.add_argument(\"--threads\", default=2, type=int, help=\"Number of CPU threads for dataloaders.\")\nparser.add_argument(\"--rho\", default=2.0, type=int, help=\"Rho parameter for SAM.\")\nparser.add_argument(\"--weight_decay\", default=0.0005, type=float, help=\"L2 weight decay.\")\nparser.add_argument(\"--width_factor\", default=4, type=int, help=\"How many times wider compared to normal ResNet.\")\nargs = parser.parse_args(\"\")\n\ninitialize(args, seed=42)\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nmodel = WideResNet(args.depth, args.width_factor, args.dropout, in_channels=3, labels=400).to(device)\nlog = Log(log_each=10)\n\n\nbase_optimizer = torch.optim.SGD\noptimizer = SAM(model.parameters(), base_optimizer, rho=args.rho, adaptive=args.adaptive, lr=args.learning_rate, momentum=args.momentum, weight_decay=args.weight_decay)\nscheduler = StepLR(optimizer, args.learning_rate, args.epochs)","metadata":{"execution":{"iopub.status.busy":"2022-05-30T04:38:49.585601Z","iopub.execute_input":"2022-05-30T04:38:49.586100Z","iopub.status.idle":"2022-05-30T04:38:52.663641Z","shell.execute_reply.started":"2022-05-30T04:38:49.586061Z","shell.execute_reply":"2022-05-30T04:38:52.662849Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# Check number of parameters your model\npytorch_total_params = sum(p.numel() for p in model.parameters())\nprint(f\"Number of parameters: {pytorch_total_params}\")\nif int(pytorch_total_params) > 5000000:\n    print('Your model has the number of parameters more than 5 millions..')\n    sys.exit()\n    \ndevice = torch.device('cuda:0' if cuda.is_available() else 'cpu')\nmodel.to(device)\nprint(device)","metadata":{"execution":{"iopub.status.busy":"2022-05-30T04:38:52.665035Z","iopub.execute_input":"2022-05-30T04:38:52.665380Z","iopub.status.idle":"2022-05-30T04:38:52.676446Z","shell.execute_reply.started":"2022-05-30T04:38:52.665345Z","shell.execute_reply":"2022-05-30T04:38:52.675669Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"train_transform = transforms.Compose([transforms.Resize((32,32)),transforms.RandomRotation(45),transforms.RandomHorizontalFlip(),\n                                      transforms.RandomVerticalFlip(),transforms.ToTensor(),\n                                      transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\nval_transform = transforms.Compose([transforms.Resize((32,32)),transforms.RandomRotation(45),transforms.RandomHorizontalFlip(),\n                                    transforms.RandomVerticalFlip(),transforms.ToTensor(),\n                                    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","metadata":{"execution":{"iopub.status.busy":"2022-05-30T04:39:07.354982Z","iopub.execute_input":"2022-05-30T04:39:07.355613Z","iopub.status.idle":"2022-05-30T04:39:07.365060Z","shell.execute_reply.started":"2022-05-30T04:39:07.355576Z","shell.execute_reply":"2022-05-30T04:39:07.364295Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"data = datasets.ImageFolder(data_dir)\ntrain_size = int(len(data)*0.95)\nval_size = int((len(data)-train_size))\ntrain_data,val_data = random_split(data,[train_size,val_size])\ntorch.manual_seed(3334)\nprint(f'train size: {len(train_data)}\\nval size: {len(val_data)}')\n\ntrain_data.dataset.transform = train_transform\nval_data.dataset.transform = val_transform\nbatch_size = 32\ntrain_loader = DataLoader(train_data,batch_size=batch_size,shuffle=True)\nval_loader = DataLoader(val_data,batch_size=batch_size,shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2022-05-30T04:39:08.068449Z","iopub.execute_input":"2022-05-30T04:39:08.069484Z","iopub.status.idle":"2022-05-30T04:39:08.511681Z","shell.execute_reply.started":"2022-05-30T04:39:08.069438Z","shell.execute_reply":"2022-05-30T04:39:08.510862Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"def fit(model,criterion,optimizer,num_epochs=10):\n    print_freq = 100\n    start = time.time()\n    best_model = model.state_dict()\n    best_acc = 0\n    train_loss_over_time = []\n    val_loss_over_time = []\n    train_acc_over_time = []\n    val_acc_over_time = []\n\n\n    # each epoch has a training and validation phase\n    for epoch in range(num_epochs):\n        \n        print(\"\\n----- epoch: {}, lr: {} -----\".format(epoch, optimizer.param_groups[0][\"lr\"]))\n        batch_time = AverageMeter('Time', ':6.3f')\n        acc = AverageMeter('Accuracy', ':.4e')\n        progress = ProgressMeter(len(train_loader), batch_time, acc, prefix=\"Epoch: [{}]\".format(epoch))\n\n        for phase in ['train','val']:\n            \n            if phase == 'train':\n                data_loader = train_loader\n                model.train()                    # set the model to train mode\n                end = time.time()\n\n            else:\n                data_loader = val_loader\n                model.eval()                    # set the model to evaluate mode\n                end = time.time()\n            \n                \n            running_loss = 0.0\n            running_corrects = 0.0\n            \n            # iterate over the data\n            for i,(inputs,labels) in enumerate(data_loader):\n                inputs = inputs.to(device)\n                labels = labels.to(device)\n                \n                # zero the parameter gradients\n                optimizer.zero_grad()\n                \n                # forward\n                with torch.set_grad_enabled(phase == 'train'):\n                    enable_running_stats(model)\n                    outputs = model(inputs)\n                    _,pred = torch.max(outputs,dim=1)\n                    loss = criterion(outputs,labels)\n                    \n                    # backward + optimize only if in training phase\n                    if phase == 'train':\n                        loss.backward()\n                        optimizer.first_step(zero_grad=True)\n                        disable_running_stats(model)\n                        smooth_crossentropy(model(inputs), labels, smoothing=args.label_smoothing).mean().backward()\n                        optimizer.second_step(zero_grad=True)\n                        with torch.no_grad():\n                            scheduler(epoch)\n                \n                # calculating the loss and accuracy\n                running_loss += loss.item() * inputs.size(0)\n                running_corrects += torch.sum(pred == labels.data)\n\n                epoch_acc = (running_corrects.double()/len(train_data)).cpu().numpy()\n                acc.update(epoch_acc.item(), inputs.size(0))\n                \n                if phase == 'train':                          \n                    batch_time.update(time.time() - end)\n                    end = time.time()\n\n                    if i % print_freq == 0:\n                        progress.print(i)  \n\n            if phase == 'train':\n\n                epoch_loss = running_loss/len(train_data)\n                train_loss_over_time.append(epoch_loss)\n                epoch_acc = (running_corrects.double()/len(train_data)).cpu().numpy()\n                train_acc_over_time.append(epoch_acc)\n\n\n            else:\n                epoch_loss = running_loss/len(val_data)\n                val_loss_over_time.append(epoch_loss)\n                epoch_acc = (running_corrects.double()/len(val_data)).cpu().numpy()\n                val_acc_over_time.append(epoch_acc)\n          \n\n            print(f'{phase} loss: {epoch_loss:.3f}, acc: {epoch_acc:.3f}')\n            \n            if phase == 'val' and epoch_acc > best_acc:\n                best_acc = epoch_acc\n                torch.save(model.state_dict(), 'model_best.pt')\n            \n            torch.save(model.state_dict(),'model_latest.pt')\n            \n\n        print('-'*60)\n    print('\\n') \n    elapsed_time = time.time() - start\n    print('==> {:.2f} seconds to train this epoch\\n'.format(elapsed_time))\n    print(f'best accuracy: {best_acc:.3f}')\n\n\n    # load best model weights\n    model.load_state_dict(best_model)\n    loss = {'train':train_loss_over_time, 'val':val_loss_over_time}\n    acc = {'train':train_acc_over_time, 'val':val_acc_over_time}\n\n    return model,loss, acc","metadata":{"execution":{"iopub.status.busy":"2022-05-30T04:39:08.823424Z","iopub.execute_input":"2022-05-30T04:39:08.824101Z","iopub.status.idle":"2022-05-30T04:39:08.845328Z","shell.execute_reply.started":"2022-05-30T04:39:08.824067Z","shell.execute_reply":"2022-05-30T04:39:08.844441Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss()\n# optimizer = torch.optim.SGD(model.parameters(), lr=1e-4, momentum=0.9, nesterov=True, weight_decay=5e-4)\nepochs = 50\nhistory, loss, acc = fit(model, criterion, optimizer, num_epochs = epochs)","metadata":{"execution":{"iopub.status.busy":"2022-05-30T04:39:09.410380Z","iopub.execute_input":"2022-05-30T04:39:09.411137Z","iopub.status.idle":"2022-05-30T07:34:52.518267Z","shell.execute_reply.started":"2022-05-30T04:39:09.411102Z","shell.execute_reply":"2022-05-30T07:34:52.517351Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"train_loss = loss['train']\nval_loss = loss['val']\ntrain_acc = acc['train']\nval_acc = acc['val']\n\nepochs_range = range(epochs)\nplt.figure(figsize=(20,10))\n\nplt.subplot(1,2,1)\nplt.ylim(0,10)\nplt.xlim(0,50)\nplt.plot(epochs_range, train_loss, label='train_loss')\nplt.plot(epochs_range, val_loss, label='val_loss')\nplt.legend(loc=0)\nplt.title('Loss')\n\nplt.subplot(1,2,2)\nplt.plot(epochs_range, train_acc ,label='train_acc')\nplt.plot(epochs_range, val_acc, label='val_acc')\nplt.legend(loc=0)\nplt.ylim(0,1)\nplt.xlim(0,50)\nplt.title('Accuracy')","metadata":{"execution":{"iopub.status.busy":"2022-05-30T07:37:21.554018Z","iopub.execute_input":"2022-05-30T07:37:21.554376Z","iopub.status.idle":"2022-05-30T07:37:21.870486Z","shell.execute_reply.started":"2022-05-30T07:37:21.554348Z","shell.execute_reply":"2022-05-30T07:37:21.869705Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"seed = 0\ntorch.manual_seed(seed)\nif cuda:\n    torch.cuda.manual_seed(seed)\n\ntorch.manual_seed(3334)\ntest_transform = transforms.Compose([transforms.Resize((32,32)),transforms.RandomRotation(45),transforms.RandomHorizontalFlip(),\n                                    transforms.RandomVerticalFlip(),transforms.ToTensor(),\n                                    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n\n# splitting the data into train/validation/test sets\ntest_data_dir = '../input/bird-data/Bird/test'\n_data = datasets.ImageFolder(test_data_dir)\ntest1_size = int(len(_data)*1)\ntest2_size = int((len(_data)-test1_size))\ntest_data, test2_data = torch.utils.data.random_split(_data,[test1_size, test2_size])\ntorch.manual_seed(3334)\n\nprint(f'test size: {len(test_data)}')\n\ntest_data.dataset.transform = test_transform\nbatch_size = 32\ntest_loader = DataLoader(test_data, batch_size = batch_size, shuffle = False)\nprint(test_loader)","metadata":{"execution":{"iopub.status.busy":"2022-05-30T07:37:27.417466Z","iopub.execute_input":"2022-05-30T07:37:27.418389Z","iopub.status.idle":"2022-05-30T07:37:30.476605Z","shell.execute_reply.started":"2022-05-30T07:37:27.418341Z","shell.execute_reply":"2022-05-30T07:37:30.475701Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"import itertools\n# testing how good the model is\ndef evaluate(model,criterion):\n    model.eval()       # setting the model to evaluate mode\n    preds = []\n    Category = []\n\n    test_model = WideResNet(args.depth, args.width_factor, args.dropout, in_channels=3, labels=400).to(device)\n    #저장경로는 변경하셔도 됩니다.\n    test_model.load_state_dict(torch.load('./model_best.pt'))\n\n    for inputs, label_ in test_loader:\n        \n        inputs = inputs.to(device)\n        labels = label_.to(device)\n        # predicting\n        with torch.no_grad():\n\n            outputs = test_model(inputs)\n            _,pred = torch.max(outputs,dim=1)\n            preds.append(pred)\n\n    category = [t.cpu().numpy() for t in preds]\n    \n    t_category = list(itertools.chain(*category))\n   \n    Id = list(range(0, len(t_category)))\n\n    prediction = {\n      'Id': Id,\n      'Category': t_category \n    }\n\n    prediction_df = pd.DataFrame(prediction, columns=['Id','Category'])\n    #저장경로는 변경하셔도 됩니다.\n    prediction_df.to_csv('./prediction.csv', index=False)\n\n    print('Done!!')\n        \n    return preds\n\n# testing the model\npredictions = evaluate(model, criterion)","metadata":{"execution":{"iopub.status.busy":"2022-05-30T07:37:30.480524Z","iopub.execute_input":"2022-05-30T07:37:30.481128Z","iopub.status.idle":"2022-05-30T07:37:48.731356Z","shell.execute_reply.started":"2022-05-30T07:37:30.481093Z","shell.execute_reply":"2022-05-30T07:37:48.729760Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}