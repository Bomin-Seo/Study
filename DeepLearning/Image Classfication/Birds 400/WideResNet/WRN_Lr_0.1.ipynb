{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport sys\nimport os\nimport time\nimport random \nimport pandas as pd\nimport torch\nfrom torch import nn, cuda, optim\nfrom torchvision import models,transforms,datasets\nfrom torch.utils.data import DataLoader,random_split\nfrom PIL import Image\nimport seaborn as sns\nimport torch.nn.functional as F\n\ndevice = 'cuda' if cuda.is_available() else 'cpu'","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-29T05:15:43.455594Z","iopub.execute_input":"2022-05-29T05:15:43.456137Z","iopub.status.idle":"2022-05-29T05:15:46.306829Z","shell.execute_reply.started":"2022-05-29T05:15:43.456051Z","shell.execute_reply":"2022-05-29T05:15:46.305963Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"data_dir = '../input/bird-data/Bird/train'\nclasses = []\nimg_per_class = []\n# for folder in os.listdir(data_dir+'consolidated'):/\nfor folder in os.listdir(data_dir):    \n    classes.append(folder)\n    img_per_class.append(len(os.listdir(f'{data_dir}/{folder}')))\nnum_classes = len(classes)\ndf = pd.DataFrame({'Classes':classes, 'Examples':img_per_class})\ndf","metadata":{"execution":{"iopub.status.busy":"2022-05-29T05:15:49.569048Z","iopub.execute_input":"2022-05-29T05:15:49.569848Z","iopub.status.idle":"2022-05-29T05:15:55.209576Z","shell.execute_reply.started":"2022-05-29T05:15:49.569809Z","shell.execute_reply":"2022-05-29T05:15:55.208874Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"class AverageMeter(object):\n    r\"\"\"Computes and stores the average and current value\n    \"\"\"\n    def __init__(self, name, fmt=':f'):\n        self.name = name\n        self.fmt = fmt\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\n    def __str__(self):\n        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n        return fmtstr.format(**self.__dict__)\n\n\nclass ProgressMeter(object):\n    def __init__(self, num_batches, *meters, prefix=\"\"):\n        self.batch_fmtstr = self._get_batch_fmtstr(num_batches)\n        self.meters = meters\n        self.prefix = prefix\n\n    def print(self, batch):\n        entries = [self.prefix + self.batch_fmtstr.format(batch)]\n        entries += [str(meter) for meter in self.meters]\n        print('\\t'.join(entries))\n\n    def _get_batch_fmtstr(self, num_batches):\n        num_digits = len(str(num_batches // 1))\n        fmt = '{:' + str(num_digits) + 'd}'\n        return '[' + fmt + '/' + fmt.format(num_batches) + ']'\n\n\ndef accuracy(output, target, topk=(1,)):\n    r\"\"\"Computes the accuracy over the $k$ top predictions for the specified values of k\n    \"\"\"\n    with torch.no_grad():\n        maxk = max(topk)\n        batch_size = target.size(0)\n\n        # _, pred = output.topk(maxk, 1, True, True)\n        # pred = pred.t()\n        # correct = pred.eq(target.view(1, -1).expand_as(pred))\n\n        # faster topk (ref: https://github.com/pytorch/pytorch/issues/22812)\n        _, idx = output.sort(descending=True)\n        pred = idx[:,:maxk]\n        pred = pred.t()\n        correct = pred.eq(target.view(1, -1).expand_as(pred))\n\n        res = []\n        for k in topk:\n            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n            res.append(correct_k.mul_(100.0 / batch_size))\n        return res","metadata":{"execution":{"iopub.status.busy":"2022-05-29T05:15:58.825609Z","iopub.execute_input":"2022-05-29T05:15:58.825953Z","iopub.status.idle":"2022-05-29T05:15:58.840527Z","shell.execute_reply.started":"2022-05-29T05:15:58.825925Z","shell.execute_reply":"2022-05-29T05:15:58.839765Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"%cd '../input/sam-wrn/sam-main/sam-main/example'","metadata":{"execution":{"iopub.status.busy":"2022-05-29T05:16:05.464886Z","iopub.execute_input":"2022-05-29T05:16:05.465235Z","iopub.status.idle":"2022-05-29T05:16:05.477370Z","shell.execute_reply.started":"2022-05-29T05:16:05.465206Z","shell.execute_reply":"2022-05-29T05:16:05.476637Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"import argparse\nimport torch\n\nfrom model.wide_res_net import WideResNet\nfrom model.smooth_cross_entropy import smooth_crossentropy\nfrom utility.log import Log\nfrom utility.initialize import initialize\nfrom utility.step_lr import StepLR\nfrom utility.bypass_bn import enable_running_stats, disable_running_stats\n\nimport sys; sys.path.append(\"..\")\nfrom sam import SAM","metadata":{"execution":{"iopub.status.busy":"2022-05-29T05:16:07.562750Z","iopub.execute_input":"2022-05-29T05:16:07.563365Z","iopub.status.idle":"2022-05-29T05:16:07.626379Z","shell.execute_reply.started":"2022-05-29T05:16:07.563332Z","shell.execute_reply":"2022-05-29T05:16:07.625668Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"%cd '/kaggle/working'","metadata":{"execution":{"iopub.status.busy":"2022-05-29T05:16:09.362957Z","iopub.execute_input":"2022-05-29T05:16:09.363325Z","iopub.status.idle":"2022-05-29T05:16:09.368714Z","shell.execute_reply.started":"2022-05-29T05:16:09.363296Z","shell.execute_reply":"2022-05-29T05:16:09.367934Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"parser = argparse.ArgumentParser()\nparser.add_argument(\"--adaptive\", default=True, type=bool, help=\"True if you want to use the Adaptive SAM.\")\nparser.add_argument(\"--batch_size\", default=32, type=int, help=\"Batch size used in the training and validation loop.\")\nparser.add_argument(\"--depth\", default=16, type=int, help=\"Number of layers.\")\nparser.add_argument(\"--dropout\", default=0.0, type=float, help=\"Dropout rate.\")\nparser.add_argument(\"--epochs\", default=50, type=int, help=\"Total number of epochs.\")\nparser.add_argument(\"--label_smoothing\", default=0.1, type=float, help=\"Use 0.0 for no label smoothing.\")\nparser.add_argument(\"--learning_rate\", default=0.1, type=float, help=\"Base learning rate at the start of the training.\")\nparser.add_argument(\"--momentum\", default=0.9, type=float, help=\"SGD Momentum.\")\nparser.add_argument(\"--threads\", default=2, type=int, help=\"Number of CPU threads for dataloaders.\")\nparser.add_argument(\"--rho\", default=2.0, type=int, help=\"Rho parameter for SAM.\")\nparser.add_argument(\"--weight_decay\", default=0.0005, type=float, help=\"L2 weight decay.\")\nparser.add_argument(\"--width_factor\", default=4, type=int, help=\"How many times wider compared to normal ResNet.\")\nargs = parser.parse_args(\"\")\n\ninitialize(args, seed=42)\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nmodel = WideResNet(args.depth, args.width_factor, args.dropout, in_channels=3, labels=400).to(device)\nlog = Log(log_each=10)\n\n\nbase_optimizer = torch.optim.SGD\noptimizer = SAM(model.parameters(), base_optimizer, rho=args.rho, adaptive=args.adaptive, lr=args.learning_rate, momentum=args.momentum, weight_decay=args.weight_decay)\nscheduler = StepLR(optimizer, args.learning_rate, args.epochs)","metadata":{"execution":{"iopub.status.busy":"2022-05-29T06:20:19.201019Z","iopub.execute_input":"2022-05-29T06:20:19.201389Z","iopub.status.idle":"2022-05-29T06:20:19.297038Z","shell.execute_reply.started":"2022-05-29T06:20:19.201357Z","shell.execute_reply":"2022-05-29T06:20:19.296076Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"# Check number of parameters your model\npytorch_total_params = sum(p.numel() for p in model.parameters())\nprint(f\"Number of parameters: {pytorch_total_params}\")\nif int(pytorch_total_params) > 5000000:\n    print('Your model has the number of parameters more than 5 millions..')\n    sys.exit()\n    \ndevice = torch.device('cuda:0' if cuda.is_available() else 'cpu')\nmodel.to(device)\nprint(device)","metadata":{"execution":{"iopub.status.busy":"2022-05-29T06:20:21.608966Z","iopub.execute_input":"2022-05-29T06:20:21.609922Z","iopub.status.idle":"2022-05-29T06:20:21.619144Z","shell.execute_reply.started":"2022-05-29T06:20:21.609873Z","shell.execute_reply":"2022-05-29T06:20:21.618352Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"train_transform = transforms.Compose([transforms.Resize((32,32)),transforms.RandomRotation(45),transforms.RandomHorizontalFlip(),\n                                      transforms.RandomVerticalFlip(),transforms.ToTensor(),\n                                      transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\nval_transform = transforms.Compose([transforms.Resize((32,32)),transforms.RandomRotation(45),transforms.RandomHorizontalFlip(),\n                                    transforms.RandomVerticalFlip(),transforms.ToTensor(),\n                                    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])","metadata":{"execution":{"iopub.status.busy":"2022-05-29T06:20:23.843831Z","iopub.execute_input":"2022-05-29T06:20:23.844467Z","iopub.status.idle":"2022-05-29T06:20:23.850627Z","shell.execute_reply.started":"2022-05-29T06:20:23.844430Z","shell.execute_reply":"2022-05-29T06:20:23.849888Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"data = datasets.ImageFolder(data_dir)\ntrain_size = int(len(data)*0.9)\nval_size = int((len(data)-train_size))\ntrain_data,val_data = random_split(data,[train_size,val_size])\ntorch.manual_seed(3334)\nprint(f'train size: {len(train_data)}\\nval size: {len(val_data)}')\n\ntrain_data.dataset.transform = train_transform\nval_data.dataset.transform = val_transform\nbatch_size = 32\ntrain_loader = DataLoader(train_data,batch_size=batch_size,shuffle=True)\nval_loader = DataLoader(val_data,batch_size=batch_size,shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2022-05-29T06:20:26.551870Z","iopub.execute_input":"2022-05-29T06:20:26.552629Z","iopub.status.idle":"2022-05-29T06:20:27.131011Z","shell.execute_reply.started":"2022-05-29T06:20:26.552591Z","shell.execute_reply":"2022-05-29T06:20:27.130244Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"def fit(model,criterion,optimizer,num_epochs=10):\n    print_freq = 30\n    start = time.time()\n    best_model = model.state_dict()\n    best_acc = 0\n    train_loss_over_time = []\n    val_loss_over_time = []\n    train_acc_over_time = []\n    val_acc_over_time = []\n\n\n    # each epoch has a training and validation phase\n    for epoch in range(num_epochs):\n        \n        print(\"\\n----- epoch: {}, lr: {} -----\".format(epoch, optimizer.param_groups[0][\"lr\"]))\n        batch_time = AverageMeter('Time', ':6.3f')\n        acc = AverageMeter('Accuracy', ':.4e')\n        progress = ProgressMeter(len(train_loader), batch_time, acc, prefix=\"Epoch: [{}]\".format(epoch))\n\n        for phase in ['train','val']:\n            \n            if phase == 'train':\n                data_loader = train_loader\n                model.train()                    # set the model to train mode\n                end = time.time()\n\n            else:\n                data_loader = val_loader\n                model.eval()                    # set the model to evaluate mode\n                end = time.time()\n            \n                \n            running_loss = 0.0\n            running_corrects = 0.0\n            \n            # iterate over the data\n            for i,(inputs,labels) in enumerate(data_loader):\n                inputs = inputs.to(device)\n                labels = labels.to(device)\n                \n                # zero the parameter gradients\n                optimizer.zero_grad()\n                \n                # forward\n                with torch.set_grad_enabled(phase == 'train'):\n                    enable_running_stats(model)\n                    outputs = model(inputs)\n                    _,pred = torch.max(outputs,dim=1)\n                    loss = criterion(outputs,labels)\n                    \n                    # backward + optimize only if in training phase\n                    if phase == 'train':\n                        loss.backward()\n                        optimizer.first_step(zero_grad=True)\n                        disable_running_stats(model)\n                        smooth_crossentropy(model(inputs), labels, smoothing=args.label_smoothing).mean().backward()\n                        optimizer.second_step(zero_grad=True)\n                        with torch.no_grad():\n                            scheduler(epoch)\n                \n                # calculating the loss and accuracy\n                running_loss += loss.item() * inputs.size(0)\n                running_corrects += torch.sum(pred == labels.data)\n\n                epoch_acc = (running_corrects.double()/len(train_data)).cpu().numpy()\n                acc.update(epoch_acc.item(), inputs.size(0))\n                \n                if phase == 'train':                          \n                    batch_time.update(time.time() - end)\n                    end = time.time()\n\n                    if i % print_freq == 0:\n                        progress.print(i)  \n\n            if phase == 'train':\n\n                epoch_loss = running_loss/len(train_data)\n                train_loss_over_time.append(epoch_loss)\n                epoch_acc = (running_corrects.double()/len(train_data)).cpu().numpy()\n                train_acc_over_time.append(epoch_acc)\n\n\n            else:\n                epoch_loss = running_loss/len(val_data)\n                val_loss_over_time.append(epoch_loss)\n                epoch_acc = (running_corrects.double()/len(val_data)).cpu().numpy()\n                val_acc_over_time.append(epoch_acc)\n          \n\n            print(f'{phase} loss: {epoch_loss:.3f}, acc: {epoch_acc:.3f}')\n            \n            if phase == 'val' and epoch_acc > best_acc:\n                best_acc = epoch_acc\n                torch.save(model.state_dict(), 'model_best.pt')\n            \n            torch.save(model.state_dict(),'model_latest.pt')\n            \n\n        print('-'*60)\n    print('\\n') \n    elapsed_time = time.time() - start\n    print('==> {:.2f} seconds to train this epoch\\n'.format(elapsed_time))\n    print(f'best accuracy: {best_acc:.3f}')\n\n\n    # load best model weights\n    model.load_state_dict(best_model)\n    loss = {'train':train_loss_over_time, 'val':val_loss_over_time}\n    acc = {'train':train_acc_over_time, 'val':val_acc_over_time}\n\n    return model,loss, acc","metadata":{"execution":{"iopub.status.busy":"2022-05-29T06:26:31.988880Z","iopub.execute_input":"2022-05-29T06:26:31.989257Z","iopub.status.idle":"2022-05-29T06:26:32.008450Z","shell.execute_reply.started":"2022-05-29T06:26:31.989224Z","shell.execute_reply":"2022-05-29T06:26:32.007530Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss()\n# optimizer = torch.optim.SGD(model.parameters(), lr=1e-4, momentum=0.9, nesterov=True, weight_decay=5e-4)\nepochs = 50\nhistory, loss, acc = fit(model, criterion, optimizer, num_epochs = epochs)","metadata":{"execution":{"iopub.status.busy":"2022-05-29T06:26:34.625585Z","iopub.execute_input":"2022-05-29T06:26:34.625935Z","iopub.status.idle":"2022-05-29T09:18:45.624768Z","shell.execute_reply.started":"2022-05-29T06:26:34.625907Z","shell.execute_reply":"2022-05-29T09:18:45.623920Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"train_loss = loss['train']\nval_loss = loss['val']\ntrain_acc = acc['train']\nval_acc = acc['val']\n\nepochs_range = range(epochs)\nplt.figure(figsize=(20,10))\n\nplt.subplot(1,2,1)\nplt.ylim(0,10)\nplt.xlim(0,50)\nplt.plot(epochs_range, train_loss, label='train_loss')\nplt.plot(epochs_range, val_loss, label='val_loss')\nplt.legend(loc=0)\nplt.title('Loss')\n\nplt.subplot(1,2,2)\nplt.plot(epochs_range, train_acc ,label='train_acc')\nplt.plot(epochs_range, val_acc, label='val_acc')\nplt.legend(loc=0)\nplt.ylim(0,1)\nplt.xlim(0,50)\nplt.title('Accuracy')","metadata":{"execution":{"iopub.status.busy":"2022-05-29T09:33:05.730945Z","iopub.execute_input":"2022-05-29T09:33:05.731323Z","iopub.status.idle":"2022-05-29T09:33:06.045971Z","shell.execute_reply.started":"2022-05-29T09:33:05.731292Z","shell.execute_reply":"2022-05-29T09:33:06.045251Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"seed = 0\ntorch.manual_seed(seed)\nif cuda:\n    torch.cuda.manual_seed(seed)\n\ntorch.manual_seed(3334)\ntest_transform = transforms.Compose([transforms.Resize((32,32)),transforms.RandomRotation(45),transforms.RandomHorizontalFlip(),\n                                    transforms.RandomVerticalFlip(),transforms.ToTensor(),\n                                    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n\n# splitting the data into train/validation/test sets\ntest_data_dir = '../input/bird-data/Bird/test'\n_data = datasets.ImageFolder(test_data_dir)\ntest1_size = int(len(_data)*1)\ntest2_size = int((len(_data)-test1_size))\ntest_data, test2_data = torch.utils.data.random_split(_data,[test1_size, test2_size])\ntorch.manual_seed(3334)\n\nprint(f'test size: {len(test_data)}')\n\ntest_data.dataset.transform = test_transform\nbatch_size = 32\ntest_loader = DataLoader(test_data, batch_size = batch_size, shuffle = False)\nprint(test_loader)","metadata":{"execution":{"iopub.status.busy":"2022-05-29T09:34:05.259545Z","iopub.execute_input":"2022-05-29T09:34:05.260119Z","iopub.status.idle":"2022-05-29T09:34:07.214919Z","shell.execute_reply.started":"2022-05-29T09:34:05.260083Z","shell.execute_reply":"2022-05-29T09:34:07.214158Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"import itertools\n# testing how good the model is\ndef evaluate(model,criterion):\n    model.eval()       # setting the model to evaluate mode\n    preds = []\n    Category = []\n\n    test_model = WideResNet(args.depth, args.width_factor, args.dropout, in_channels=3, labels=400).to(device)\n    #저장경로는 변경하셔도 됩니다.\n    test_model.load_state_dict(torch.load('./model_best.pt'))\n\n    for inputs, label_ in test_loader:\n        \n        inputs = inputs.to(device)\n        labels = label_.to(device)\n        # predicting\n        with torch.no_grad():\n\n            outputs = test_model(inputs)\n            _,pred = torch.max(outputs,dim=1)\n            preds.append(pred)\n\n    category = [t.cpu().numpy() for t in preds]\n    \n    t_category = list(itertools.chain(*category))\n   \n    Id = list(range(0, len(t_category)))\n\n    prediction = {\n      'Id': Id,\n      'Category': t_category \n    }\n\n    prediction_df = pd.DataFrame(prediction, columns=['Id','Category'])\n    #저장경로는 변경하셔도 됩니다.\n    prediction_df.to_csv('./prediction.csv', index=False)\n\n    print('Done!!')\n        \n    return preds\n\n# testing the model\npredictions = evaluate(model, criterion)","metadata":{"execution":{"iopub.status.busy":"2022-05-29T09:38:02.875682Z","iopub.execute_input":"2022-05-29T09:38:02.876313Z","iopub.status.idle":"2022-05-29T09:38:07.955655Z","shell.execute_reply.started":"2022-05-29T09:38:02.876279Z","shell.execute_reply":"2022-05-29T09:38:07.954131Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"for epoch in range(args.epochs):\n    model.train()\n    log.train(len_dataset=len(train_loader))\n\n    for batch in train_loader:\n        inputs, targets = (b.to(device) for b in batch)\n\n        # first forward-backward step\n        enable_running_stats(model)\n        predictions = model(inputs)\n        loss = smooth_crossentropy(predictions, targets, smoothing=args.label_smoothing)\n        loss.mean().backward()\n        optimizer.first_step(zero_grad=True)\n\n        # second forward-backward step\n        disable_running_stats(model)\n        smooth_crossentropy(model(inputs), targets, smoothing=args.label_smoothing).mean().backward()\n        optimizer.second_step(zero_grad=True)\n\n        with torch.no_grad():\n            correct = torch.argmax(predictions.data, 1) == targets\n            log(model, loss.cpu(), correct.cpu(), scheduler.lr())\n            scheduler(epoch)\n\n    model.eval()\n    log.eval(len_dataset=len(val_loader))\n\n    with torch.no_grad():\n        for batch in val_loader:\n            inputs, targets = (b.to(device) for b in batch)\n\n            predictions = model(inputs)\n            loss = smooth_crossentropy(predictions, targets)\n            correct = torch.argmax(predictions, 1) == targets\n            log(model, loss.cpu(), correct.cpu())\n\nlog.flush()","metadata":{"execution":{"iopub.status.busy":"2022-05-29T00:35:01.768000Z","iopub.execute_input":"2022-05-29T00:35:01.768660Z","iopub.status.idle":"2022-05-29T03:27:31.360025Z","shell.execute_reply.started":"2022-05-29T00:35:01.768620Z","shell.execute_reply":"2022-05-29T03:27:31.359216Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"seed = 0\ntorch.manual_seed(seed)\nif cuda:\n    torch.cuda.manual_seed(seed)\n\ntorch.manual_seed(3334)\ntest_transform = transforms.Compose([transforms.Resize((32,32)),transforms.RandomRotation(45),transforms.RandomHorizontalFlip(),\n                                      transforms.RandomVerticalFlip(),transforms.ToTensor(),\n                                      transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n\ntest_data_dir = '../input/bird-data/Bird/test'\n\n\n_data = datasets.ImageFolder(test_data_dir)\ntest1_size = int(len(_data)*1)\ntest2_size = int((len(_data)-test1_size))\ntest_data, test2_data = torch.utils.data.random_split(_data,[test1_size, test2_size])\ntorch.manual_seed(3334)\n\nprint(f'test size: {len(test_data)}')\n\ntest_data.dataset.transform = test_transform\nbatch_size = 32\ntest_loader = DataLoader(test_data, batch_size = batch_size, shuffle = False)\nprint(test_loader)","metadata":{"execution":{"iopub.status.busy":"2022-05-29T03:35:06.056945Z","iopub.execute_input":"2022-05-29T03:35:06.057943Z","iopub.status.idle":"2022-05-29T03:35:07.893789Z","shell.execute_reply.started":"2022-05-29T03:35:06.057895Z","shell.execute_reply":"2022-05-29T03:35:07.892987Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"import itertools\n\ndef def evaluate(model,criterion):\n    Category = []\n    preds = []\n\n    for epoch in range(args.epochs):\n        model.eval()\n        log.eval(len_dataset=len(test_loader))\n\n        with torch.no_grad():\n            for batch in test_loader:\n                inputs, targets = (b.to(device) for b in batch)\n\n                predictions = model(inputs)\n                preds.append(predictions)\n                loss = smooth_crossentropy(predictions, targets)\n                correct = torch.argmax(predictions, 1) == targets\n                log(model, loss.cpu(), correct.cpu())\n\n    log.flush()\n\n    category = [t.cpu().numpy() for t in preds]\n    t_category = list(itertools.chain(*category))\n    Id = list(range(0, len(t_category)))\n    prediction = {'Id': Id, 'Category': t_category}\n    prediction_df = pd.DataFrame(prediction, columns=['Id','Category'])\n    prediction_df.to_csv('./prediction.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-05-29T03:53:19.135707Z","iopub.execute_input":"2022-05-29T03:53:19.136063Z","iopub.status.idle":"2022-05-29T04:03:25.666805Z","shell.execute_reply.started":"2022-05-29T03:53:19.136032Z","shell.execute_reply":"2022-05-29T04:03:25.665380Z"},"trusted":true},"execution_count":14,"outputs":[]}]}