{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "실전기계학습_Lab_P04_2018102111_서보민.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "실전 기계학습 Lab #P04"
      ],
      "metadata": {
        "id": "vixpYGaOHSZF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "소프트웨어융합학과 2018102111 서보민"
      ],
      "metadata": {
        "id": "HIqBVNjEHUAF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. [Exercise 11-1] 아래 논문을 읽고, FULL INCEPTION V3/V4를 구현한 다음 CIFAR-100에 대한 학\n",
        "습을 수행하시오. (소스코드와 결과를 첨부할 것)"
      ],
      "metadata": {
        "id": "TVwDZREWHYyw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class BasicConv2d(nn.Module):\n",
        "\n",
        "    def __init__(self, input_channels, output_channels, **kwargs):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(input_channels, output_channels, bias=False, **kwargs)\n",
        "        self.bn = nn.BatchNorm2d(output_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = self.bn(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "class Inception_Stem(nn.Module):\n",
        "\n",
        "    def __init__(self, input_channels):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Sequential(\n",
        "            BasicConv2d(input_channels, 32, kernel_size=3),\n",
        "            BasicConv2d(32, 32, kernel_size=3, padding=1),\n",
        "            BasicConv2d(32, 64, kernel_size=3, padding=1)\n",
        "        )\n",
        "\n",
        "        self.branch3x3_conv = BasicConv2d(64, 96, kernel_size=3, padding=1)\n",
        "        self.branch3x3_pool = nn.MaxPool2d(3, stride=1, padding=1)\n",
        "\n",
        "        self.branch7x7a = nn.Sequential(\n",
        "            BasicConv2d(160, 64, kernel_size=1),\n",
        "            BasicConv2d(64, 64, kernel_size=(7, 1), padding=(3, 0)),\n",
        "            BasicConv2d(64, 64, kernel_size=(1, 7), padding=(0, 3)),\n",
        "            BasicConv2d(64, 96, kernel_size=3, padding=1)\n",
        "        )\n",
        "\n",
        "        self.branch7x7b = nn.Sequential(\n",
        "            BasicConv2d(160, 64, kernel_size=1),\n",
        "            BasicConv2d(64, 96, kernel_size=3, padding=1)\n",
        "        )\n",
        "\n",
        "        self.branchpoola = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)\n",
        "        self.branchpoolb = BasicConv2d(192, 192, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = self.conv1(x)\n",
        "        x = [\n",
        "            self.branch3x3_conv(x),\n",
        "            self.branch3x3_pool(x)\n",
        "        ]\n",
        "        x = torch.cat(x, 1)\n",
        "\n",
        "        x = [\n",
        "            self.branch7x7a(x),\n",
        "            self.branch7x7b(x)\n",
        "        ]\n",
        "        x = torch.cat(x, 1)\n",
        "\n",
        "        x = [\n",
        "            self.branchpoola(x),\n",
        "            self.branchpoolb(x)\n",
        "        ]\n",
        "\n",
        "        x = torch.cat(x, 1)\n",
        "\n",
        "        return x\n",
        "\n",
        "class InceptionA(nn.Module):\n",
        "    def __init__(self, input_channels):\n",
        "        super().__init__()\n",
        "        self.branch3x3stack = nn.Sequential(\n",
        "            BasicConv2d(input_channels, 64, kernel_size=1),\n",
        "            BasicConv2d(64, 96, kernel_size=3, padding=1),\n",
        "            BasicConv2d(96, 96, kernel_size=3, padding=1)\n",
        "        )\n",
        "        self.branch3x3 = nn.Sequential(\n",
        "            BasicConv2d(input_channels, 64, kernel_size=1),\n",
        "            BasicConv2d(64, 96, kernel_size=3, padding=1)\n",
        "        )\n",
        "        self.branch1x1 = BasicConv2d(input_channels, 96, kernel_size=1)\n",
        "\n",
        "        self.branchpool = nn.Sequential(\n",
        "            nn.AvgPool2d(kernel_size=3, stride=1, padding=1),\n",
        "            BasicConv2d(input_channels, 96, kernel_size=1)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        x = [\n",
        "            self.branch3x3stack(x),\n",
        "            self.branch3x3(x),\n",
        "            self.branch1x1(x),\n",
        "            self.branchpool(x)\n",
        "        ]\n",
        "        return torch.cat(x, 1)\n",
        "\n",
        "class ReductionA(nn.Module):\n",
        "\n",
        "    def __init__(self, input_channels, k, l, m, n):\n",
        "        super().__init__()\n",
        "        self.branch3x3stack = nn.Sequential(\n",
        "            BasicConv2d(input_channels, k, kernel_size=1),\n",
        "            BasicConv2d(k, l, kernel_size=3, padding=1),\n",
        "            BasicConv2d(l, m, kernel_size=3, stride=2)\n",
        "        )\n",
        "\n",
        "        self.branch3x3 = BasicConv2d(input_channels, n, kernel_size=3, stride=2)\n",
        "        self.branchpool = nn.MaxPool2d(kernel_size=3, stride=2)\n",
        "        self.output_channels = input_channels + n + m\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = [\n",
        "            self.branch3x3stack(x),\n",
        "            self.branch3x3(x),\n",
        "            self.branchpool(x)\n",
        "        ]\n",
        "\n",
        "        return torch.cat(x, 1)\n",
        "\n",
        "class InceptionB(nn.Module):\n",
        "\n",
        "    def __init__(self, input_channels):\n",
        "        super().__init__()\n",
        "\n",
        "        self.branch7x7stack = nn.Sequential(\n",
        "            BasicConv2d(input_channels, 192, kernel_size=1),\n",
        "            BasicConv2d(192, 192, kernel_size=(1, 7), padding=(0, 3)),\n",
        "            BasicConv2d(192, 224, kernel_size=(7, 1), padding=(3, 0)),\n",
        "            BasicConv2d(224, 224, kernel_size=(1, 7), padding=(0, 3)),\n",
        "            BasicConv2d(224, 256, kernel_size=(7, 1), padding=(3, 0))\n",
        "        )\n",
        "\n",
        "        self.branch7x7 = nn.Sequential(\n",
        "            BasicConv2d(input_channels, 192, kernel_size=1),\n",
        "            BasicConv2d(192, 224, kernel_size=(1, 7), padding=(0, 3)),\n",
        "            BasicConv2d(224, 256, kernel_size=(7, 1), padding=(3, 0))\n",
        "        )\n",
        "\n",
        "        self.branch1x1 = BasicConv2d(input_channels, 384, kernel_size=1)\n",
        "\n",
        "        self.branchpool = nn.Sequential(\n",
        "            nn.AvgPool2d(3, stride=1, padding=1),\n",
        "            BasicConv2d(input_channels, 128, kernel_size=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = [\n",
        "            self.branch1x1(x),\n",
        "            self.branch7x7(x),\n",
        "            self.branch7x7stack(x),\n",
        "            self.branchpool(x)\n",
        "        ]\n",
        "\n",
        "        return torch.cat(x, 1)\n",
        "\n",
        "class ReductionB(nn.Module):\n",
        "\n",
        "    def __init__(self, input_channels):\n",
        "\n",
        "        super().__init__()\n",
        "        self.branch7x7 = nn.Sequential(\n",
        "            BasicConv2d(input_channels, 256, kernel_size=1),\n",
        "            BasicConv2d(256, 256, kernel_size=(1, 7), padding=(0, 3)),\n",
        "            BasicConv2d(256, 320, kernel_size=(7, 1), padding=(3, 0)),\n",
        "            BasicConv2d(320, 320, kernel_size=3, stride=2, padding=1)\n",
        "        )\n",
        "\n",
        "        self.branch3x3 = nn.Sequential(\n",
        "            BasicConv2d(input_channels, 192, kernel_size=1),\n",
        "            BasicConv2d(192, 192, kernel_size=3, stride=2, padding=1)\n",
        "        )\n",
        "\n",
        "        self.branchpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = [\n",
        "            self.branch3x3(x),\n",
        "            self.branch7x7(x),\n",
        "            self.branchpool(x)\n",
        "        ]\n",
        "\n",
        "        return torch.cat(x, 1)\n",
        "\n",
        "class InceptionC(nn.Module):\n",
        "\n",
        "    def __init__(self, input_channels):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.branch3x3stack = nn.Sequential(\n",
        "            BasicConv2d(input_channels, 384, kernel_size=1),\n",
        "            BasicConv2d(384, 448, kernel_size=(1, 3), padding=(0, 1)),\n",
        "            BasicConv2d(448, 512, kernel_size=(3, 1), padding=(1, 0)),\n",
        "        )\n",
        "        self.branch3x3stacka = BasicConv2d(512, 256, kernel_size=(1, 3), padding=(0, 1))\n",
        "        self.branch3x3stackb = BasicConv2d(512, 256, kernel_size=(3, 1), padding=(1, 0))\n",
        "\n",
        "        self.branch3x3 = BasicConv2d(input_channels, 384, kernel_size=1)\n",
        "        self.branch3x3a = BasicConv2d(384, 256, kernel_size=(3, 1), padding=(1, 0))\n",
        "        self.branch3x3b = BasicConv2d(384, 256, kernel_size=(1, 3), padding=(0, 1))\n",
        "\n",
        "        self.branch1x1 = BasicConv2d(input_channels, 256, kernel_size=1)\n",
        "\n",
        "        self.branchpool = nn.Sequential(\n",
        "            nn.AvgPool2d(kernel_size=3, stride=1, padding=1),\n",
        "            BasicConv2d(input_channels, 256, kernel_size=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        branch3x3stack_output = self.branch3x3stack(x)\n",
        "        branch3x3stack_output = [\n",
        "            self.branch3x3stacka(branch3x3stack_output),\n",
        "            self.branch3x3stackb(branch3x3stack_output)\n",
        "        ]\n",
        "        branch3x3stack_output = torch.cat(branch3x3stack_output, 1)\n",
        "\n",
        "        branch3x3_output = self.branch3x3(x)\n",
        "        branch3x3_output = [\n",
        "            self.branch3x3a(branch3x3_output),\n",
        "            self.branch3x3b(branch3x3_output)\n",
        "        ]\n",
        "        branch3x3_output = torch.cat(branch3x3_output, 1)\n",
        "\n",
        "        branch1x1_output = self.branch1x1(x)\n",
        "\n",
        "        branchpool = self.branchpool(x)\n",
        "\n",
        "        output = [\n",
        "            branch1x1_output,\n",
        "            branch3x3_output,\n",
        "            branch3x3stack_output,\n",
        "            branchpool\n",
        "        ]\n",
        "\n",
        "        return torch.cat(output, 1)\n",
        "\n",
        "class InceptionV4(nn.Module):\n",
        "\n",
        "    def __init__(self, A, B, C, k=192, l=224, m=256, n=384, class_nums=100):\n",
        "\n",
        "        super().__init__()\n",
        "        self.stem = Inception_Stem(3)\n",
        "        self.inception_a = self._generate_inception_module(384, 384, A, InceptionA)\n",
        "        self.reduction_a = ReductionA(384, k, l, m, n)\n",
        "        output_channels = self.reduction_a.output_channels\n",
        "        self.inception_b = self._generate_inception_module(output_channels, 1024, B, InceptionB)\n",
        "        self.reduction_b = ReductionB(1024)\n",
        "        self.inception_c = self._generate_inception_module(1536, 1536, C, InceptionC)\n",
        "        self.avgpool = nn.AvgPool2d(7)\n",
        "\n",
        "        #\"\"\"Dropout (keep 0.8)\"\"\"\n",
        "        self.dropout = nn.Dropout2d(1 - 0.8)\n",
        "        self.linear = nn.Linear(1536, class_nums)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.stem(x)\n",
        "        x = self.inception_a(x)\n",
        "        x = self.reduction_a(x)\n",
        "        x = self.inception_b(x)\n",
        "        x = self.reduction_b(x)\n",
        "        x = self.inception_c(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = self.dropout(x)\n",
        "        x = x.view(-1, 1536)\n",
        "        x = self.linear(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    @staticmethod\n",
        "    def _generate_inception_module(input_channels, output_channels, block_num, block):\n",
        "\n",
        "        layers = nn.Sequential()\n",
        "        for l in range(block_num):\n",
        "            layers.add_module(\"{}_{}\".format(block.__name__, l), block(input_channels))\n",
        "            input_channels = output_channels\n",
        "\n",
        "        return layers\n",
        "\n",
        "class InceptionResNetA(nn.Module):\n",
        "\n",
        "    def __init__(self, input_channels):\n",
        "\n",
        "        super().__init__()\n",
        "        self.branch3x3stack = nn.Sequential(\n",
        "            BasicConv2d(input_channels, 32, kernel_size=1),\n",
        "            BasicConv2d(32, 48, kernel_size=3, padding=1),\n",
        "            BasicConv2d(48, 64, kernel_size=3, padding=1)\n",
        "        )\n",
        "\n",
        "        self.branch3x3 = nn.Sequential(\n",
        "            BasicConv2d(input_channels, 32, kernel_size=1),\n",
        "            BasicConv2d(32, 32, kernel_size=3, padding=1)\n",
        "        )\n",
        "\n",
        "        self.branch1x1 = BasicConv2d(input_channels, 32, kernel_size=1)\n",
        "\n",
        "        self.reduction1x1 = nn.Conv2d(128, 384, kernel_size=1)\n",
        "        self.shortcut = nn.Conv2d(input_channels, 384, kernel_size=1)\n",
        "        self.bn = nn.BatchNorm2d(384)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        residual = [\n",
        "            self.branch1x1(x),\n",
        "            self.branch3x3(x),\n",
        "            self.branch3x3stack(x)\n",
        "        ]\n",
        "\n",
        "        residual = torch.cat(residual, 1)\n",
        "        residual = self.reduction1x1(residual)\n",
        "        shortcut = self.shortcut(x)\n",
        "\n",
        "        output = self.bn(shortcut + residual)\n",
        "        output = self.relu(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "class InceptionResNetB(nn.Module):\n",
        "\n",
        "    def __init__(self, input_channels):\n",
        "\n",
        "        super().__init__()\n",
        "        self.branch7x7 = nn.Sequential(\n",
        "            BasicConv2d(input_channels, 128, kernel_size=1),\n",
        "            BasicConv2d(128, 160, kernel_size=(1, 7), padding=(0, 3)),\n",
        "            BasicConv2d(160, 192, kernel_size=(7, 1), padding=(3, 0))\n",
        "        )\n",
        "\n",
        "        self.branch1x1 = BasicConv2d(input_channels, 192, kernel_size=1)\n",
        "\n",
        "        self.reduction1x1 = nn.Conv2d(384, 1154, kernel_size=1)\n",
        "        self.shortcut = nn.Conv2d(input_channels, 1154, kernel_size=1)\n",
        "\n",
        "        self.bn = nn.BatchNorm2d(1154)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = [\n",
        "            self.branch1x1(x),\n",
        "            self.branch7x7(x)\n",
        "        ]\n",
        "\n",
        "        residual = torch.cat(residual, 1)\n",
        "\n",
        "        #\"\"\"In general we picked some scaling factors between 0.1 and 0.3 to scale the residuals\n",
        "        #before their being added to the accumulated layer activations (cf. Figure 20).\"\"\"\n",
        "        residual = self.reduction1x1(residual) * 0.1\n",
        "\n",
        "        shortcut = self.shortcut(x)\n",
        "\n",
        "        output = self.bn(residual + shortcut)\n",
        "        output = self.relu(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "class InceptionResNetC(nn.Module):\n",
        "\n",
        "    def __init__(self, input_channels):\n",
        "\n",
        "        super().__init__()\n",
        "        self.branch3x3 = nn.Sequential(\n",
        "            BasicConv2d(input_channels, 192, kernel_size=1),\n",
        "            BasicConv2d(192, 224, kernel_size=(1, 3), padding=(0, 1)),\n",
        "            BasicConv2d(224, 256, kernel_size=(3, 1), padding=(1, 0))\n",
        "        )\n",
        "\n",
        "        self.branch1x1 = BasicConv2d(input_channels, 192, kernel_size=1)\n",
        "        self.reduction1x1 = nn.Conv2d(448, 2048, kernel_size=1)\n",
        "        self.shorcut = nn.Conv2d(input_channels, 2048, kernel_size=1)\n",
        "        self.bn = nn.BatchNorm2d(2048)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = [\n",
        "            self.branch1x1(x),\n",
        "            self.branch3x3(x)\n",
        "        ]\n",
        "\n",
        "        residual = torch.cat(residual, 1)\n",
        "        residual = self.reduction1x1(residual) * 0.1\n",
        "\n",
        "        shorcut = self.shorcut(x)\n",
        "\n",
        "        output = self.bn(shorcut + residual)\n",
        "        output = self.relu(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "class InceptionResNetReductionA(nn.Module):\n",
        "\n",
        "    def __init__(self, input_channels, k, l, m, n):\n",
        "\n",
        "        super().__init__()\n",
        "        self.branch3x3stack = nn.Sequential(\n",
        "            BasicConv2d(input_channels, k, kernel_size=1),\n",
        "            BasicConv2d(k, l, kernel_size=3, padding=1),\n",
        "            BasicConv2d(l, m, kernel_size=3, stride=2)\n",
        "        )\n",
        "\n",
        "        self.branch3x3 = BasicConv2d(input_channels, n, kernel_size=3, stride=2)\n",
        "        self.branchpool = nn.MaxPool2d(kernel_size=3, stride=2)\n",
        "        self.output_channels = input_channels + n + m\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = [\n",
        "            self.branch3x3stack(x),\n",
        "            self.branch3x3(x),\n",
        "            self.branchpool(x)\n",
        "        ]\n",
        "\n",
        "        return torch.cat(x, 1)\n",
        "\n",
        "class InceptionResNetReductionB(nn.Module):\n",
        "\n",
        "    def __init__(self, input_channels):\n",
        "\n",
        "        super().__init__()\n",
        "        self.branchpool = nn.MaxPool2d(3, stride=2)\n",
        "\n",
        "        self.branch3x3a = nn.Sequential(\n",
        "            BasicConv2d(input_channels, 256, kernel_size=1),\n",
        "            BasicConv2d(256, 384, kernel_size=3, stride=2)\n",
        "        )\n",
        "\n",
        "        self.branch3x3b = nn.Sequential(\n",
        "            BasicConv2d(input_channels, 256, kernel_size=1),\n",
        "            BasicConv2d(256, 288, kernel_size=3, stride=2)\n",
        "        )\n",
        "\n",
        "        self.branch3x3stack = nn.Sequential(\n",
        "            BasicConv2d(input_channels, 256, kernel_size=1),\n",
        "            BasicConv2d(256, 288, kernel_size=3, padding=1),\n",
        "            BasicConv2d(288, 320, kernel_size=3, stride=2)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = [\n",
        "            self.branch3x3a(x),\n",
        "            self.branch3x3b(x),\n",
        "            self.branch3x3stack(x),\n",
        "            self.branchpool(x)\n",
        "        ]\n",
        "\n",
        "        x = torch.cat(x, 1)\n",
        "        return x\n",
        "\n",
        "class InceptionResNetV2(nn.Module):\n",
        "\n",
        "    def __init__(self, A, B, C, k=256, l=256, m=384, n=384, class_nums=100):\n",
        "        super().__init__()\n",
        "        self.stem = Inception_Stem(3)\n",
        "        self.inception_resnet_a = self._generate_inception_module(384, 384, A, InceptionResNetA)\n",
        "        self.reduction_a = InceptionResNetReductionA(384, k, l, m, n)\n",
        "        output_channels = self.reduction_a.output_channels\n",
        "        self.inception_resnet_b = self._generate_inception_module(output_channels, 1154, B, InceptionResNetB)\n",
        "        self.reduction_b = InceptionResNetReductionB(1154)\n",
        "        self.inception_resnet_c = self._generate_inception_module(2146, 2048, C, InceptionResNetC)\n",
        "\n",
        "        #6x6 featuresize\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        #\"\"\"Dropout (keep 0.8)\"\"\"\n",
        "        self.dropout = nn.Dropout2d(1 - 0.8)\n",
        "        self.linear = nn.Linear(2048, class_nums)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.stem(x)\n",
        "        x = self.inception_resnet_a(x)\n",
        "        x = self.reduction_a(x)\n",
        "        x = self.inception_resnet_b(x)\n",
        "        x = self.reduction_b(x)\n",
        "        x = self.inception_resnet_c(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = self.dropout(x)\n",
        "        x = x.view(-1, 2048)\n",
        "        x = self.linear(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    @staticmethod\n",
        "    def _generate_inception_module(input_channels, output_channels, block_num, block):\n",
        "\n",
        "        layers = nn.Sequential()\n",
        "        for l in range(block_num):\n",
        "            layers.add_module(\"{}_{}\".format(block.__name__, l), block(input_channels))\n",
        "            input_channels = output_channels\n",
        "\n",
        "        return layers\n",
        "\n",
        "def inceptionv4():\n",
        "    return InceptionV4(4, 7, 3)\n",
        "\n",
        "def inception_resnet_v2():\n",
        "    return InceptionResNetV2(5, 10, 5)"
      ],
      "metadata": {
        "id": "VrXTpWSs9K6A"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "from torch import nn, optim, cuda\n",
        "from torch.utils import data\n",
        "from torchvision import datasets, transforms\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "\n",
        "batch_size = 8\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'\n",
        "\n",
        "train_dataset = datasets.CIFAR100(root='./cifal100_data/',\n",
        "                               train=True,\n",
        "                               transform=transforms.ToTensor(),\n",
        "                               download=True)\n",
        "\n",
        "test_dataset = datasets.CIFAR100(root='./cifal100_data/',\n",
        "                              train=False,\n",
        "                              transform=transforms.ToTensor())\n",
        "\n",
        "\n",
        "train_loader = data.DataLoader(dataset=train_dataset,\n",
        "                                           batch_size=batch_size,\n",
        "                                           shuffle=True)\n",
        "\n",
        "test_loader = data.DataLoader(dataset=test_dataset,\n",
        "                                          batch_size=batch_size,\n",
        "                                          shuffle=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IUQuUh--j9fQ",
        "outputId": "8240b41b-196a-4f29-ea70-c4280ac8856d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = inceptionv4()\n",
        "model.to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
        "\n",
        "def train(epoch):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % 10 == 0:\n",
        "            print('Train Epoch: {} | Batch Status: {}/{} ({:.0f}%) | Loss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))\n",
        "\n",
        "\n",
        "def test():\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    for data, target in test_loader:\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        output = model(data)\n",
        "        # sum up batch loss\n",
        "        test_loss += criterion(output, target).item()\n",
        "        # get the index of the max\n",
        "        pred = output.data.max(1, keepdim=True)[1]\n",
        "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    #print(f'===========================\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} '\n",
        "         # f'({100. * correct / len(test_loader.dataset):.0f}%)')\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    since = time.time()\n",
        "    for epoch in range(1,2):\n",
        "        epoch_start = time.time()\n",
        "        train(epoch)\n",
        "        m, s = divmod(time.time() - epoch_start, 60)\n",
        "        print(f'Training time: {m:.0f}m {s:.0f}s')\n",
        "        test()\n",
        "        m, s = divmod(time.time() - epoch_start, 60)\n",
        "        print(f'Testing time: {m:.0f}m {s:.0f}s')\n",
        "\n",
        "    m, s = divmod(time.time() - since, 60)\n",
        "    print(f'Total Time: {m:.0f}m {s:.0f}s\\nModel was trained on {device}!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bYaebLRrP8L_",
        "outputId": "89cf2b08-4c30-40a3-83d6-5e5b6a758752"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 | Batch Status: 0/50000 (0%) | Loss: 4.730550\n",
            "Train Epoch: 1 | Batch Status: 80/50000 (0%) | Loss: 5.001627\n",
            "Train Epoch: 1 | Batch Status: 160/50000 (0%) | Loss: 4.944277\n",
            "Train Epoch: 1 | Batch Status: 240/50000 (0%) | Loss: 4.666105\n",
            "Train Epoch: 1 | Batch Status: 320/50000 (1%) | Loss: 4.769226\n",
            "Train Epoch: 1 | Batch Status: 400/50000 (1%) | Loss: 4.948831\n",
            "Train Epoch: 1 | Batch Status: 480/50000 (1%) | Loss: 4.729947\n",
            "Train Epoch: 1 | Batch Status: 560/50000 (1%) | Loss: 4.507215\n",
            "Train Epoch: 1 | Batch Status: 640/50000 (1%) | Loss: 5.066483\n",
            "Train Epoch: 1 | Batch Status: 720/50000 (1%) | Loss: 5.152712\n",
            "Train Epoch: 1 | Batch Status: 800/50000 (2%) | Loss: 4.374258\n",
            "Train Epoch: 1 | Batch Status: 880/50000 (2%) | Loss: 4.610931\n",
            "Train Epoch: 1 | Batch Status: 960/50000 (2%) | Loss: 4.998093\n",
            "Train Epoch: 1 | Batch Status: 1040/50000 (2%) | Loss: 4.736474\n",
            "Train Epoch: 1 | Batch Status: 1120/50000 (2%) | Loss: 4.529099\n",
            "Train Epoch: 1 | Batch Status: 1200/50000 (2%) | Loss: 4.782944\n",
            "Train Epoch: 1 | Batch Status: 1280/50000 (3%) | Loss: 4.578569\n",
            "Train Epoch: 1 | Batch Status: 1360/50000 (3%) | Loss: 4.718657\n",
            "Train Epoch: 1 | Batch Status: 1440/50000 (3%) | Loss: 4.819585\n",
            "Train Epoch: 1 | Batch Status: 1520/50000 (3%) | Loss: 4.854411\n",
            "Train Epoch: 1 | Batch Status: 1600/50000 (3%) | Loss: 4.354178\n",
            "Train Epoch: 1 | Batch Status: 1680/50000 (3%) | Loss: 5.030419\n",
            "Train Epoch: 1 | Batch Status: 1760/50000 (4%) | Loss: 4.684882\n",
            "Train Epoch: 1 | Batch Status: 1840/50000 (4%) | Loss: 4.671773\n",
            "Train Epoch: 1 | Batch Status: 1920/50000 (4%) | Loss: 4.799766\n",
            "Train Epoch: 1 | Batch Status: 2000/50000 (4%) | Loss: 4.512540\n",
            "Train Epoch: 1 | Batch Status: 2080/50000 (4%) | Loss: 4.640408\n",
            "Train Epoch: 1 | Batch Status: 2160/50000 (4%) | Loss: 4.446907\n",
            "Train Epoch: 1 | Batch Status: 2240/50000 (4%) | Loss: 4.661402\n",
            "Train Epoch: 1 | Batch Status: 2320/50000 (5%) | Loss: 4.564963\n",
            "Train Epoch: 1 | Batch Status: 2400/50000 (5%) | Loss: 4.714348\n",
            "Train Epoch: 1 | Batch Status: 2480/50000 (5%) | Loss: 4.755919\n",
            "Train Epoch: 1 | Batch Status: 2560/50000 (5%) | Loss: 4.578572\n",
            "Train Epoch: 1 | Batch Status: 2640/50000 (5%) | Loss: 4.919063\n",
            "Train Epoch: 1 | Batch Status: 2720/50000 (5%) | Loss: 4.627973\n",
            "Train Epoch: 1 | Batch Status: 2800/50000 (6%) | Loss: 4.393236\n",
            "Train Epoch: 1 | Batch Status: 2880/50000 (6%) | Loss: 4.324815\n",
            "Train Epoch: 1 | Batch Status: 2960/50000 (6%) | Loss: 4.473845\n",
            "Train Epoch: 1 | Batch Status: 3040/50000 (6%) | Loss: 4.489312\n",
            "Train Epoch: 1 | Batch Status: 3120/50000 (6%) | Loss: 4.652788\n",
            "Train Epoch: 1 | Batch Status: 3200/50000 (6%) | Loss: 4.319745\n",
            "Train Epoch: 1 | Batch Status: 3280/50000 (7%) | Loss: 4.491216\n",
            "Train Epoch: 1 | Batch Status: 3360/50000 (7%) | Loss: 4.564513\n",
            "Train Epoch: 1 | Batch Status: 3440/50000 (7%) | Loss: 4.553245\n",
            "Train Epoch: 1 | Batch Status: 3520/50000 (7%) | Loss: 4.594849\n",
            "Train Epoch: 1 | Batch Status: 3600/50000 (7%) | Loss: 4.446451\n",
            "Train Epoch: 1 | Batch Status: 3680/50000 (7%) | Loss: 4.273703\n",
            "Train Epoch: 1 | Batch Status: 3760/50000 (8%) | Loss: 4.897776\n",
            "Train Epoch: 1 | Batch Status: 3840/50000 (8%) | Loss: 4.381965\n",
            "Train Epoch: 1 | Batch Status: 3920/50000 (8%) | Loss: 4.512963\n",
            "Train Epoch: 1 | Batch Status: 4000/50000 (8%) | Loss: 4.579970\n",
            "Train Epoch: 1 | Batch Status: 4080/50000 (8%) | Loss: 4.440471\n",
            "Train Epoch: 1 | Batch Status: 4160/50000 (8%) | Loss: 4.531599\n",
            "Train Epoch: 1 | Batch Status: 4240/50000 (8%) | Loss: 4.600441\n",
            "Train Epoch: 1 | Batch Status: 4320/50000 (9%) | Loss: 4.463114\n",
            "Train Epoch: 1 | Batch Status: 4400/50000 (9%) | Loss: 4.341337\n",
            "Train Epoch: 1 | Batch Status: 4480/50000 (9%) | Loss: 4.602799\n",
            "Train Epoch: 1 | Batch Status: 4560/50000 (9%) | Loss: 4.481599\n",
            "Train Epoch: 1 | Batch Status: 4640/50000 (9%) | Loss: 4.957204\n",
            "Train Epoch: 1 | Batch Status: 4720/50000 (9%) | Loss: 4.411707\n",
            "Train Epoch: 1 | Batch Status: 4800/50000 (10%) | Loss: 4.291055\n",
            "Train Epoch: 1 | Batch Status: 4880/50000 (10%) | Loss: 4.424109\n",
            "Train Epoch: 1 | Batch Status: 4960/50000 (10%) | Loss: 4.807829\n",
            "Train Epoch: 1 | Batch Status: 5040/50000 (10%) | Loss: 4.381344\n",
            "Train Epoch: 1 | Batch Status: 5120/50000 (10%) | Loss: 4.636557\n",
            "Train Epoch: 1 | Batch Status: 5200/50000 (10%) | Loss: 4.337597\n",
            "Train Epoch: 1 | Batch Status: 5280/50000 (11%) | Loss: 4.501480\n",
            "Train Epoch: 1 | Batch Status: 5360/50000 (11%) | Loss: 4.778144\n",
            "Train Epoch: 1 | Batch Status: 5440/50000 (11%) | Loss: 4.762389\n",
            "Train Epoch: 1 | Batch Status: 5520/50000 (11%) | Loss: 4.450020\n",
            "Train Epoch: 1 | Batch Status: 5600/50000 (11%) | Loss: 4.557208\n",
            "Train Epoch: 1 | Batch Status: 5680/50000 (11%) | Loss: 4.190983\n",
            "Train Epoch: 1 | Batch Status: 5760/50000 (12%) | Loss: 4.138090\n",
            "Train Epoch: 1 | Batch Status: 5840/50000 (12%) | Loss: 4.520306\n",
            "Train Epoch: 1 | Batch Status: 5920/50000 (12%) | Loss: 4.409628\n",
            "Train Epoch: 1 | Batch Status: 6000/50000 (12%) | Loss: 4.422823\n",
            "Train Epoch: 1 | Batch Status: 6080/50000 (12%) | Loss: 4.675726\n",
            "Train Epoch: 1 | Batch Status: 6160/50000 (12%) | Loss: 4.761135\n",
            "Train Epoch: 1 | Batch Status: 6240/50000 (12%) | Loss: 4.622731\n",
            "Train Epoch: 1 | Batch Status: 6320/50000 (13%) | Loss: 4.170060\n",
            "Train Epoch: 1 | Batch Status: 6400/50000 (13%) | Loss: 4.004498\n",
            "Train Epoch: 1 | Batch Status: 6480/50000 (13%) | Loss: 4.425407\n",
            "Train Epoch: 1 | Batch Status: 6560/50000 (13%) | Loss: 4.376713\n",
            "Train Epoch: 1 | Batch Status: 6640/50000 (13%) | Loss: 4.694907\n",
            "Train Epoch: 1 | Batch Status: 6720/50000 (13%) | Loss: 4.620999\n",
            "Train Epoch: 1 | Batch Status: 6800/50000 (14%) | Loss: 4.497023\n",
            "Train Epoch: 1 | Batch Status: 6880/50000 (14%) | Loss: 4.306412\n",
            "Train Epoch: 1 | Batch Status: 6960/50000 (14%) | Loss: 4.266018\n",
            "Train Epoch: 1 | Batch Status: 7040/50000 (14%) | Loss: 3.918435\n",
            "Train Epoch: 1 | Batch Status: 7120/50000 (14%) | Loss: 4.157136\n",
            "Train Epoch: 1 | Batch Status: 7200/50000 (14%) | Loss: 4.629704\n",
            "Train Epoch: 1 | Batch Status: 7280/50000 (15%) | Loss: 4.427152\n",
            "Train Epoch: 1 | Batch Status: 7360/50000 (15%) | Loss: 4.310099\n",
            "Train Epoch: 1 | Batch Status: 7440/50000 (15%) | Loss: 4.613461\n",
            "Train Epoch: 1 | Batch Status: 7520/50000 (15%) | Loss: 4.393228\n",
            "Train Epoch: 1 | Batch Status: 7600/50000 (15%) | Loss: 4.707542\n",
            "Train Epoch: 1 | Batch Status: 7680/50000 (15%) | Loss: 4.070920\n",
            "Train Epoch: 1 | Batch Status: 7760/50000 (16%) | Loss: 3.892870\n",
            "Train Epoch: 1 | Batch Status: 7840/50000 (16%) | Loss: 4.364645\n",
            "Train Epoch: 1 | Batch Status: 7920/50000 (16%) | Loss: 4.342328\n",
            "Train Epoch: 1 | Batch Status: 8000/50000 (16%) | Loss: 4.558124\n",
            "Train Epoch: 1 | Batch Status: 8080/50000 (16%) | Loss: 4.254476\n",
            "Train Epoch: 1 | Batch Status: 8160/50000 (16%) | Loss: 4.506735\n",
            "Train Epoch: 1 | Batch Status: 8240/50000 (16%) | Loss: 3.872867\n",
            "Train Epoch: 1 | Batch Status: 8320/50000 (17%) | Loss: 4.616541\n",
            "Train Epoch: 1 | Batch Status: 8400/50000 (17%) | Loss: 4.809216\n",
            "Train Epoch: 1 | Batch Status: 8480/50000 (17%) | Loss: 4.521798\n",
            "Train Epoch: 1 | Batch Status: 8560/50000 (17%) | Loss: 4.393048\n",
            "Train Epoch: 1 | Batch Status: 8640/50000 (17%) | Loss: 4.301518\n",
            "Train Epoch: 1 | Batch Status: 8720/50000 (17%) | Loss: 4.334245\n",
            "Train Epoch: 1 | Batch Status: 8800/50000 (18%) | Loss: 4.766725\n",
            "Train Epoch: 1 | Batch Status: 8880/50000 (18%) | Loss: 3.988830\n",
            "Train Epoch: 1 | Batch Status: 8960/50000 (18%) | Loss: 4.353303\n",
            "Train Epoch: 1 | Batch Status: 9040/50000 (18%) | Loss: 4.421229\n",
            "Train Epoch: 1 | Batch Status: 9120/50000 (18%) | Loss: 4.503261\n",
            "Train Epoch: 1 | Batch Status: 9200/50000 (18%) | Loss: 4.503359\n",
            "Train Epoch: 1 | Batch Status: 9280/50000 (19%) | Loss: 4.684808\n",
            "Train Epoch: 1 | Batch Status: 9360/50000 (19%) | Loss: 4.142029\n",
            "Train Epoch: 1 | Batch Status: 9440/50000 (19%) | Loss: 4.545508\n",
            "Train Epoch: 1 | Batch Status: 9520/50000 (19%) | Loss: 4.260627\n",
            "Train Epoch: 1 | Batch Status: 9600/50000 (19%) | Loss: 4.380509\n",
            "Train Epoch: 1 | Batch Status: 9680/50000 (19%) | Loss: 4.794332\n",
            "Train Epoch: 1 | Batch Status: 9760/50000 (20%) | Loss: 4.519514\n",
            "Train Epoch: 1 | Batch Status: 9840/50000 (20%) | Loss: 4.151289\n",
            "Train Epoch: 1 | Batch Status: 9920/50000 (20%) | Loss: 4.548825\n",
            "Train Epoch: 1 | Batch Status: 10000/50000 (20%) | Loss: 4.373215\n",
            "Train Epoch: 1 | Batch Status: 10080/50000 (20%) | Loss: 4.309985\n",
            "Train Epoch: 1 | Batch Status: 10160/50000 (20%) | Loss: 4.560887\n",
            "Train Epoch: 1 | Batch Status: 10240/50000 (20%) | Loss: 3.976274\n",
            "Train Epoch: 1 | Batch Status: 10320/50000 (21%) | Loss: 4.221990\n",
            "Train Epoch: 1 | Batch Status: 10400/50000 (21%) | Loss: 4.533177\n",
            "Train Epoch: 1 | Batch Status: 10480/50000 (21%) | Loss: 4.358453\n",
            "Train Epoch: 1 | Batch Status: 10560/50000 (21%) | Loss: 3.923055\n",
            "Train Epoch: 1 | Batch Status: 10640/50000 (21%) | Loss: 4.324455\n",
            "Train Epoch: 1 | Batch Status: 10720/50000 (21%) | Loss: 4.603774\n",
            "Train Epoch: 1 | Batch Status: 10800/50000 (22%) | Loss: 4.830662\n",
            "Train Epoch: 1 | Batch Status: 10880/50000 (22%) | Loss: 4.450802\n",
            "Train Epoch: 1 | Batch Status: 10960/50000 (22%) | Loss: 4.474483\n",
            "Train Epoch: 1 | Batch Status: 11040/50000 (22%) | Loss: 4.278698\n",
            "Train Epoch: 1 | Batch Status: 11120/50000 (22%) | Loss: 4.261488\n",
            "Train Epoch: 1 | Batch Status: 11200/50000 (22%) | Loss: 4.344861\n",
            "Train Epoch: 1 | Batch Status: 11280/50000 (23%) | Loss: 4.267264\n",
            "Train Epoch: 1 | Batch Status: 11360/50000 (23%) | Loss: 4.349304\n",
            "Train Epoch: 1 | Batch Status: 11440/50000 (23%) | Loss: 4.786694\n",
            "Train Epoch: 1 | Batch Status: 11520/50000 (23%) | Loss: 4.482167\n",
            "Train Epoch: 1 | Batch Status: 11600/50000 (23%) | Loss: 4.743159\n",
            "Train Epoch: 1 | Batch Status: 11680/50000 (23%) | Loss: 4.580834\n",
            "Train Epoch: 1 | Batch Status: 11760/50000 (24%) | Loss: 4.032984\n",
            "Train Epoch: 1 | Batch Status: 11840/50000 (24%) | Loss: 4.714214\n",
            "Train Epoch: 1 | Batch Status: 11920/50000 (24%) | Loss: 4.703865\n",
            "Train Epoch: 1 | Batch Status: 12000/50000 (24%) | Loss: 4.264238\n",
            "Train Epoch: 1 | Batch Status: 12080/50000 (24%) | Loss: 4.412361\n",
            "Train Epoch: 1 | Batch Status: 12160/50000 (24%) | Loss: 4.437894\n",
            "Train Epoch: 1 | Batch Status: 12240/50000 (24%) | Loss: 4.403368\n",
            "Train Epoch: 1 | Batch Status: 12320/50000 (25%) | Loss: 3.947181\n",
            "Train Epoch: 1 | Batch Status: 12400/50000 (25%) | Loss: 3.766569\n",
            "Train Epoch: 1 | Batch Status: 12480/50000 (25%) | Loss: 4.048159\n",
            "Train Epoch: 1 | Batch Status: 12560/50000 (25%) | Loss: 4.474574\n",
            "Train Epoch: 1 | Batch Status: 12640/50000 (25%) | Loss: 4.644236\n",
            "Train Epoch: 1 | Batch Status: 12720/50000 (25%) | Loss: 3.687033\n",
            "Train Epoch: 1 | Batch Status: 12800/50000 (26%) | Loss: 3.807551\n",
            "Train Epoch: 1 | Batch Status: 12880/50000 (26%) | Loss: 4.607367\n",
            "Train Epoch: 1 | Batch Status: 12960/50000 (26%) | Loss: 4.247415\n",
            "Train Epoch: 1 | Batch Status: 13040/50000 (26%) | Loss: 4.445981\n",
            "Train Epoch: 1 | Batch Status: 13120/50000 (26%) | Loss: 4.436279\n",
            "Train Epoch: 1 | Batch Status: 13200/50000 (26%) | Loss: 4.153992\n",
            "Train Epoch: 1 | Batch Status: 13280/50000 (27%) | Loss: 4.226228\n",
            "Train Epoch: 1 | Batch Status: 13360/50000 (27%) | Loss: 4.401523\n",
            "Train Epoch: 1 | Batch Status: 13440/50000 (27%) | Loss: 4.393443\n",
            "Train Epoch: 1 | Batch Status: 13520/50000 (27%) | Loss: 4.392184\n",
            "Train Epoch: 1 | Batch Status: 13600/50000 (27%) | Loss: 4.681828\n",
            "Train Epoch: 1 | Batch Status: 13680/50000 (27%) | Loss: 4.707388\n",
            "Train Epoch: 1 | Batch Status: 13760/50000 (28%) | Loss: 3.927930\n",
            "Train Epoch: 1 | Batch Status: 13840/50000 (28%) | Loss: 4.165133\n",
            "Train Epoch: 1 | Batch Status: 13920/50000 (28%) | Loss: 4.059692\n",
            "Train Epoch: 1 | Batch Status: 14000/50000 (28%) | Loss: 4.539571\n",
            "Train Epoch: 1 | Batch Status: 14080/50000 (28%) | Loss: 3.872473\n",
            "Train Epoch: 1 | Batch Status: 14160/50000 (28%) | Loss: 4.068617\n",
            "Train Epoch: 1 | Batch Status: 14240/50000 (28%) | Loss: 3.880584\n",
            "Train Epoch: 1 | Batch Status: 14320/50000 (29%) | Loss: 3.620603\n",
            "Train Epoch: 1 | Batch Status: 14400/50000 (29%) | Loss: 4.637008\n",
            "Train Epoch: 1 | Batch Status: 14480/50000 (29%) | Loss: 4.710424\n",
            "Train Epoch: 1 | Batch Status: 14560/50000 (29%) | Loss: 4.147394\n",
            "Train Epoch: 1 | Batch Status: 14640/50000 (29%) | Loss: 4.485975\n",
            "Train Epoch: 1 | Batch Status: 14720/50000 (29%) | Loss: 4.158669\n",
            "Train Epoch: 1 | Batch Status: 14800/50000 (30%) | Loss: 4.497880\n",
            "Train Epoch: 1 | Batch Status: 14880/50000 (30%) | Loss: 4.089721\n",
            "Train Epoch: 1 | Batch Status: 14960/50000 (30%) | Loss: 4.429214\n",
            "Train Epoch: 1 | Batch Status: 15040/50000 (30%) | Loss: 4.299471\n",
            "Train Epoch: 1 | Batch Status: 15120/50000 (30%) | Loss: 4.193367\n",
            "Train Epoch: 1 | Batch Status: 15200/50000 (30%) | Loss: 4.171967\n",
            "Train Epoch: 1 | Batch Status: 15280/50000 (31%) | Loss: 4.493594\n",
            "Train Epoch: 1 | Batch Status: 15360/50000 (31%) | Loss: 4.414390\n",
            "Train Epoch: 1 | Batch Status: 15440/50000 (31%) | Loss: 4.082008\n",
            "Train Epoch: 1 | Batch Status: 15520/50000 (31%) | Loss: 4.091739\n",
            "Train Epoch: 1 | Batch Status: 15600/50000 (31%) | Loss: 4.296279\n",
            "Train Epoch: 1 | Batch Status: 15680/50000 (31%) | Loss: 4.007026\n",
            "Train Epoch: 1 | Batch Status: 15760/50000 (32%) | Loss: 4.667646\n",
            "Train Epoch: 1 | Batch Status: 15840/50000 (32%) | Loss: 3.954038\n",
            "Train Epoch: 1 | Batch Status: 15920/50000 (32%) | Loss: 4.041891\n",
            "Train Epoch: 1 | Batch Status: 16000/50000 (32%) | Loss: 4.419523\n",
            "Train Epoch: 1 | Batch Status: 16080/50000 (32%) | Loss: 5.173680\n",
            "Train Epoch: 1 | Batch Status: 16160/50000 (32%) | Loss: 4.304249\n",
            "Train Epoch: 1 | Batch Status: 16240/50000 (32%) | Loss: 4.041499\n",
            "Train Epoch: 1 | Batch Status: 16320/50000 (33%) | Loss: 4.651665\n",
            "Train Epoch: 1 | Batch Status: 16400/50000 (33%) | Loss: 4.062407\n",
            "Train Epoch: 1 | Batch Status: 16480/50000 (33%) | Loss: 4.519451\n",
            "Train Epoch: 1 | Batch Status: 16560/50000 (33%) | Loss: 3.772479\n",
            "Train Epoch: 1 | Batch Status: 16640/50000 (33%) | Loss: 4.455379\n",
            "Train Epoch: 1 | Batch Status: 16720/50000 (33%) | Loss: 4.413632\n",
            "Train Epoch: 1 | Batch Status: 16800/50000 (34%) | Loss: 4.316442\n",
            "Train Epoch: 1 | Batch Status: 16880/50000 (34%) | Loss: 4.332650\n",
            "Train Epoch: 1 | Batch Status: 16960/50000 (34%) | Loss: 4.230714\n",
            "Train Epoch: 1 | Batch Status: 17040/50000 (34%) | Loss: 4.649114\n",
            "Train Epoch: 1 | Batch Status: 17120/50000 (34%) | Loss: 4.258110\n",
            "Train Epoch: 1 | Batch Status: 17200/50000 (34%) | Loss: 4.044914\n",
            "Train Epoch: 1 | Batch Status: 17280/50000 (35%) | Loss: 4.043472\n",
            "Train Epoch: 1 | Batch Status: 17360/50000 (35%) | Loss: 3.771355\n",
            "Train Epoch: 1 | Batch Status: 17440/50000 (35%) | Loss: 4.076666\n",
            "Train Epoch: 1 | Batch Status: 17520/50000 (35%) | Loss: 4.290359\n",
            "Train Epoch: 1 | Batch Status: 17600/50000 (35%) | Loss: 3.801641\n",
            "Train Epoch: 1 | Batch Status: 17680/50000 (35%) | Loss: 3.966522\n",
            "Train Epoch: 1 | Batch Status: 17760/50000 (36%) | Loss: 4.508323\n",
            "Train Epoch: 1 | Batch Status: 17840/50000 (36%) | Loss: 4.201904\n",
            "Train Epoch: 1 | Batch Status: 17920/50000 (36%) | Loss: 4.118618\n",
            "Train Epoch: 1 | Batch Status: 18000/50000 (36%) | Loss: 4.229012\n",
            "Train Epoch: 1 | Batch Status: 18080/50000 (36%) | Loss: 4.353161\n",
            "Train Epoch: 1 | Batch Status: 18160/50000 (36%) | Loss: 4.475954\n",
            "Train Epoch: 1 | Batch Status: 18240/50000 (36%) | Loss: 4.376420\n",
            "Train Epoch: 1 | Batch Status: 18320/50000 (37%) | Loss: 3.961842\n",
            "Train Epoch: 1 | Batch Status: 18400/50000 (37%) | Loss: 4.024664\n",
            "Train Epoch: 1 | Batch Status: 18480/50000 (37%) | Loss: 3.923230\n",
            "Train Epoch: 1 | Batch Status: 18560/50000 (37%) | Loss: 4.049187\n",
            "Train Epoch: 1 | Batch Status: 18640/50000 (37%) | Loss: 4.254352\n",
            "Train Epoch: 1 | Batch Status: 18720/50000 (37%) | Loss: 4.132007\n",
            "Train Epoch: 1 | Batch Status: 18800/50000 (38%) | Loss: 4.378110\n",
            "Train Epoch: 1 | Batch Status: 18880/50000 (38%) | Loss: 4.664469\n",
            "Train Epoch: 1 | Batch Status: 18960/50000 (38%) | Loss: 4.303185\n",
            "Train Epoch: 1 | Batch Status: 19040/50000 (38%) | Loss: 4.304168\n",
            "Train Epoch: 1 | Batch Status: 19120/50000 (38%) | Loss: 4.307515\n",
            "Train Epoch: 1 | Batch Status: 19200/50000 (38%) | Loss: 3.952684\n",
            "Train Epoch: 1 | Batch Status: 19280/50000 (39%) | Loss: 3.385767\n",
            "Train Epoch: 1 | Batch Status: 19360/50000 (39%) | Loss: 4.325633\n",
            "Train Epoch: 1 | Batch Status: 19440/50000 (39%) | Loss: 4.465869\n",
            "Train Epoch: 1 | Batch Status: 19520/50000 (39%) | Loss: 4.122466\n",
            "Train Epoch: 1 | Batch Status: 19600/50000 (39%) | Loss: 4.306094\n",
            "Train Epoch: 1 | Batch Status: 19680/50000 (39%) | Loss: 4.451820\n",
            "Train Epoch: 1 | Batch Status: 19760/50000 (40%) | Loss: 4.152069\n",
            "Train Epoch: 1 | Batch Status: 19840/50000 (40%) | Loss: 4.401942\n",
            "Train Epoch: 1 | Batch Status: 19920/50000 (40%) | Loss: 4.163563\n",
            "Train Epoch: 1 | Batch Status: 20000/50000 (40%) | Loss: 4.482758\n",
            "Train Epoch: 1 | Batch Status: 20080/50000 (40%) | Loss: 4.545300\n",
            "Train Epoch: 1 | Batch Status: 20160/50000 (40%) | Loss: 4.752272\n",
            "Train Epoch: 1 | Batch Status: 20240/50000 (40%) | Loss: 4.901297\n",
            "Train Epoch: 1 | Batch Status: 20320/50000 (41%) | Loss: 3.724462\n",
            "Train Epoch: 1 | Batch Status: 20400/50000 (41%) | Loss: 4.358571\n",
            "Train Epoch: 1 | Batch Status: 20480/50000 (41%) | Loss: 4.137563\n",
            "Train Epoch: 1 | Batch Status: 20560/50000 (41%) | Loss: 4.096972\n",
            "Train Epoch: 1 | Batch Status: 20640/50000 (41%) | Loss: 4.238951\n",
            "Train Epoch: 1 | Batch Status: 20720/50000 (41%) | Loss: 4.147901\n",
            "Train Epoch: 1 | Batch Status: 20800/50000 (42%) | Loss: 4.347131\n",
            "Train Epoch: 1 | Batch Status: 20880/50000 (42%) | Loss: 4.036309\n",
            "Train Epoch: 1 | Batch Status: 20960/50000 (42%) | Loss: 4.940740\n",
            "Train Epoch: 1 | Batch Status: 21040/50000 (42%) | Loss: 4.633445\n",
            "Train Epoch: 1 | Batch Status: 21120/50000 (42%) | Loss: 3.862683\n",
            "Train Epoch: 1 | Batch Status: 21200/50000 (42%) | Loss: 4.088172\n",
            "Train Epoch: 1 | Batch Status: 21280/50000 (43%) | Loss: 3.715222\n",
            "Train Epoch: 1 | Batch Status: 21360/50000 (43%) | Loss: 4.846138\n",
            "Train Epoch: 1 | Batch Status: 21440/50000 (43%) | Loss: 4.528409\n",
            "Train Epoch: 1 | Batch Status: 21520/50000 (43%) | Loss: 4.211288\n",
            "Train Epoch: 1 | Batch Status: 21600/50000 (43%) | Loss: 3.731433\n",
            "Train Epoch: 1 | Batch Status: 21680/50000 (43%) | Loss: 4.329206\n",
            "Train Epoch: 1 | Batch Status: 21760/50000 (44%) | Loss: 4.347192\n",
            "Train Epoch: 1 | Batch Status: 21840/50000 (44%) | Loss: 4.394801\n",
            "Train Epoch: 1 | Batch Status: 21920/50000 (44%) | Loss: 3.851593\n",
            "Train Epoch: 1 | Batch Status: 22000/50000 (44%) | Loss: 4.573357\n",
            "Train Epoch: 1 | Batch Status: 22080/50000 (44%) | Loss: 3.927770\n",
            "Train Epoch: 1 | Batch Status: 22160/50000 (44%) | Loss: 4.071573\n",
            "Train Epoch: 1 | Batch Status: 22240/50000 (44%) | Loss: 4.177347\n",
            "Train Epoch: 1 | Batch Status: 22320/50000 (45%) | Loss: 4.205055\n",
            "Train Epoch: 1 | Batch Status: 22400/50000 (45%) | Loss: 4.120555\n",
            "Train Epoch: 1 | Batch Status: 22480/50000 (45%) | Loss: 3.605178\n",
            "Train Epoch: 1 | Batch Status: 22560/50000 (45%) | Loss: 3.709464\n",
            "Train Epoch: 1 | Batch Status: 22640/50000 (45%) | Loss: 4.314353\n",
            "Train Epoch: 1 | Batch Status: 22720/50000 (45%) | Loss: 4.597957\n",
            "Train Epoch: 1 | Batch Status: 22800/50000 (46%) | Loss: 4.075359\n",
            "Train Epoch: 1 | Batch Status: 22880/50000 (46%) | Loss: 4.094525\n",
            "Train Epoch: 1 | Batch Status: 22960/50000 (46%) | Loss: 3.983145\n",
            "Train Epoch: 1 | Batch Status: 23040/50000 (46%) | Loss: 4.084105\n",
            "Train Epoch: 1 | Batch Status: 23120/50000 (46%) | Loss: 4.411250\n",
            "Train Epoch: 1 | Batch Status: 23200/50000 (46%) | Loss: 4.151880\n",
            "Train Epoch: 1 | Batch Status: 23280/50000 (47%) | Loss: 3.472395\n",
            "Train Epoch: 1 | Batch Status: 23360/50000 (47%) | Loss: 3.930116\n",
            "Train Epoch: 1 | Batch Status: 23440/50000 (47%) | Loss: 4.209458\n",
            "Train Epoch: 1 | Batch Status: 23520/50000 (47%) | Loss: 3.683088\n",
            "Train Epoch: 1 | Batch Status: 23600/50000 (47%) | Loss: 4.198102\n",
            "Train Epoch: 1 | Batch Status: 23680/50000 (47%) | Loss: 4.470163\n",
            "Train Epoch: 1 | Batch Status: 23760/50000 (48%) | Loss: 4.145142\n",
            "Train Epoch: 1 | Batch Status: 23840/50000 (48%) | Loss: 4.117842\n",
            "Train Epoch: 1 | Batch Status: 23920/50000 (48%) | Loss: 4.060517\n",
            "Train Epoch: 1 | Batch Status: 24000/50000 (48%) | Loss: 3.496870\n",
            "Train Epoch: 1 | Batch Status: 24080/50000 (48%) | Loss: 4.245092\n",
            "Train Epoch: 1 | Batch Status: 24160/50000 (48%) | Loss: 4.202826\n",
            "Train Epoch: 1 | Batch Status: 24240/50000 (48%) | Loss: 3.703863\n",
            "Train Epoch: 1 | Batch Status: 24320/50000 (49%) | Loss: 5.274710\n",
            "Train Epoch: 1 | Batch Status: 24400/50000 (49%) | Loss: 4.052529\n",
            "Train Epoch: 1 | Batch Status: 24480/50000 (49%) | Loss: 4.541168\n",
            "Train Epoch: 1 | Batch Status: 24560/50000 (49%) | Loss: 4.368592\n",
            "Train Epoch: 1 | Batch Status: 24640/50000 (49%) | Loss: 3.580389\n",
            "Train Epoch: 1 | Batch Status: 24720/50000 (49%) | Loss: 4.340180\n",
            "Train Epoch: 1 | Batch Status: 24800/50000 (50%) | Loss: 4.365684\n",
            "Train Epoch: 1 | Batch Status: 24880/50000 (50%) | Loss: 4.071454\n",
            "Train Epoch: 1 | Batch Status: 24960/50000 (50%) | Loss: 4.343433\n",
            "Train Epoch: 1 | Batch Status: 25040/50000 (50%) | Loss: 4.035605\n",
            "Train Epoch: 1 | Batch Status: 25120/50000 (50%) | Loss: 4.317361\n",
            "Train Epoch: 1 | Batch Status: 25200/50000 (50%) | Loss: 3.747964\n",
            "Train Epoch: 1 | Batch Status: 25280/50000 (51%) | Loss: 3.568280\n",
            "Train Epoch: 1 | Batch Status: 25360/50000 (51%) | Loss: 4.069281\n",
            "Train Epoch: 1 | Batch Status: 25440/50000 (51%) | Loss: 4.120655\n",
            "Train Epoch: 1 | Batch Status: 25520/50000 (51%) | Loss: 3.666730\n",
            "Train Epoch: 1 | Batch Status: 25600/50000 (51%) | Loss: 4.707720\n",
            "Train Epoch: 1 | Batch Status: 25680/50000 (51%) | Loss: 4.397761\n",
            "Train Epoch: 1 | Batch Status: 25760/50000 (52%) | Loss: 4.415033\n",
            "Train Epoch: 1 | Batch Status: 25840/50000 (52%) | Loss: 4.210801\n",
            "Train Epoch: 1 | Batch Status: 25920/50000 (52%) | Loss: 4.032162\n",
            "Train Epoch: 1 | Batch Status: 26000/50000 (52%) | Loss: 4.570158\n",
            "Train Epoch: 1 | Batch Status: 26080/50000 (52%) | Loss: 4.140325\n",
            "Train Epoch: 1 | Batch Status: 26160/50000 (52%) | Loss: 3.743996\n",
            "Train Epoch: 1 | Batch Status: 26240/50000 (52%) | Loss: 4.327486\n",
            "Train Epoch: 1 | Batch Status: 26320/50000 (53%) | Loss: 4.321573\n",
            "Train Epoch: 1 | Batch Status: 26400/50000 (53%) | Loss: 4.047343\n",
            "Train Epoch: 1 | Batch Status: 26480/50000 (53%) | Loss: 4.070940\n",
            "Train Epoch: 1 | Batch Status: 26560/50000 (53%) | Loss: 4.261487\n",
            "Train Epoch: 1 | Batch Status: 26640/50000 (53%) | Loss: 4.188135\n",
            "Train Epoch: 1 | Batch Status: 26720/50000 (53%) | Loss: 4.874476\n",
            "Train Epoch: 1 | Batch Status: 26800/50000 (54%) | Loss: 4.304426\n",
            "Train Epoch: 1 | Batch Status: 26880/50000 (54%) | Loss: 4.114748\n",
            "Train Epoch: 1 | Batch Status: 26960/50000 (54%) | Loss: 4.214851\n",
            "Train Epoch: 1 | Batch Status: 27040/50000 (54%) | Loss: 4.180897\n",
            "Train Epoch: 1 | Batch Status: 27120/50000 (54%) | Loss: 4.047603\n",
            "Train Epoch: 1 | Batch Status: 27200/50000 (54%) | Loss: 4.836525\n",
            "Train Epoch: 1 | Batch Status: 27280/50000 (55%) | Loss: 4.140676\n",
            "Train Epoch: 1 | Batch Status: 27360/50000 (55%) | Loss: 4.234847\n",
            "Train Epoch: 1 | Batch Status: 27440/50000 (55%) | Loss: 4.035199\n",
            "Train Epoch: 1 | Batch Status: 27520/50000 (55%) | Loss: 4.112837\n",
            "Train Epoch: 1 | Batch Status: 27600/50000 (55%) | Loss: 4.182967\n",
            "Train Epoch: 1 | Batch Status: 27680/50000 (55%) | Loss: 4.452494\n",
            "Train Epoch: 1 | Batch Status: 27760/50000 (56%) | Loss: 3.725520\n",
            "Train Epoch: 1 | Batch Status: 27840/50000 (56%) | Loss: 4.320632\n",
            "Train Epoch: 1 | Batch Status: 27920/50000 (56%) | Loss: 4.064891\n",
            "Train Epoch: 1 | Batch Status: 28000/50000 (56%) | Loss: 4.078552\n",
            "Train Epoch: 1 | Batch Status: 28080/50000 (56%) | Loss: 4.079131\n",
            "Train Epoch: 1 | Batch Status: 28160/50000 (56%) | Loss: 4.713368\n",
            "Train Epoch: 1 | Batch Status: 28240/50000 (56%) | Loss: 4.326510\n",
            "Train Epoch: 1 | Batch Status: 28320/50000 (57%) | Loss: 4.019457\n",
            "Train Epoch: 1 | Batch Status: 28400/50000 (57%) | Loss: 3.701337\n",
            "Train Epoch: 1 | Batch Status: 28480/50000 (57%) | Loss: 4.280540\n",
            "Train Epoch: 1 | Batch Status: 28560/50000 (57%) | Loss: 4.091629\n",
            "Train Epoch: 1 | Batch Status: 28640/50000 (57%) | Loss: 4.796596\n",
            "Train Epoch: 1 | Batch Status: 28720/50000 (57%) | Loss: 4.097288\n",
            "Train Epoch: 1 | Batch Status: 28800/50000 (58%) | Loss: 3.945446\n",
            "Train Epoch: 1 | Batch Status: 28880/50000 (58%) | Loss: 3.890440\n",
            "Train Epoch: 1 | Batch Status: 28960/50000 (58%) | Loss: 3.934622\n",
            "Train Epoch: 1 | Batch Status: 29040/50000 (58%) | Loss: 4.379630\n",
            "Train Epoch: 1 | Batch Status: 29120/50000 (58%) | Loss: 4.202943\n",
            "Train Epoch: 1 | Batch Status: 29200/50000 (58%) | Loss: 3.428764\n",
            "Train Epoch: 1 | Batch Status: 29280/50000 (59%) | Loss: 3.683502\n",
            "Train Epoch: 1 | Batch Status: 29360/50000 (59%) | Loss: 4.296329\n",
            "Train Epoch: 1 | Batch Status: 29440/50000 (59%) | Loss: 4.626921\n",
            "Train Epoch: 1 | Batch Status: 29520/50000 (59%) | Loss: 4.206090\n",
            "Train Epoch: 1 | Batch Status: 29600/50000 (59%) | Loss: 3.901246\n",
            "Train Epoch: 1 | Batch Status: 29680/50000 (59%) | Loss: 4.745040\n",
            "Train Epoch: 1 | Batch Status: 29760/50000 (60%) | Loss: 3.484141\n",
            "Train Epoch: 1 | Batch Status: 29840/50000 (60%) | Loss: 4.022792\n",
            "Train Epoch: 1 | Batch Status: 29920/50000 (60%) | Loss: 4.141190\n",
            "Train Epoch: 1 | Batch Status: 30000/50000 (60%) | Loss: 3.849090\n",
            "Train Epoch: 1 | Batch Status: 30080/50000 (60%) | Loss: 3.362663\n",
            "Train Epoch: 1 | Batch Status: 30160/50000 (60%) | Loss: 3.883234\n",
            "Train Epoch: 1 | Batch Status: 30240/50000 (60%) | Loss: 4.301806\n",
            "Train Epoch: 1 | Batch Status: 30320/50000 (61%) | Loss: 4.502473\n",
            "Train Epoch: 1 | Batch Status: 30400/50000 (61%) | Loss: 3.596116\n",
            "Train Epoch: 1 | Batch Status: 30480/50000 (61%) | Loss: 3.862541\n",
            "Train Epoch: 1 | Batch Status: 30560/50000 (61%) | Loss: 4.059442\n",
            "Train Epoch: 1 | Batch Status: 30640/50000 (61%) | Loss: 3.510355\n",
            "Train Epoch: 1 | Batch Status: 30720/50000 (61%) | Loss: 4.378791\n",
            "Train Epoch: 1 | Batch Status: 30800/50000 (62%) | Loss: 3.549015\n",
            "Train Epoch: 1 | Batch Status: 30880/50000 (62%) | Loss: 4.467788\n",
            "Train Epoch: 1 | Batch Status: 30960/50000 (62%) | Loss: 3.558306\n",
            "Train Epoch: 1 | Batch Status: 31040/50000 (62%) | Loss: 4.012876\n",
            "Train Epoch: 1 | Batch Status: 31120/50000 (62%) | Loss: 3.463067\n",
            "Train Epoch: 1 | Batch Status: 31200/50000 (62%) | Loss: 4.132507\n",
            "Train Epoch: 1 | Batch Status: 31280/50000 (63%) | Loss: 4.622848\n",
            "Train Epoch: 1 | Batch Status: 31360/50000 (63%) | Loss: 4.499778\n",
            "Train Epoch: 1 | Batch Status: 31440/50000 (63%) | Loss: 3.548670\n",
            "Train Epoch: 1 | Batch Status: 31520/50000 (63%) | Loss: 3.867347\n",
            "Train Epoch: 1 | Batch Status: 31600/50000 (63%) | Loss: 4.120874\n",
            "Train Epoch: 1 | Batch Status: 31680/50000 (63%) | Loss: 3.915077\n",
            "Train Epoch: 1 | Batch Status: 31760/50000 (64%) | Loss: 4.073001\n",
            "Train Epoch: 1 | Batch Status: 31840/50000 (64%) | Loss: 3.699010\n",
            "Train Epoch: 1 | Batch Status: 31920/50000 (64%) | Loss: 3.940106\n",
            "Train Epoch: 1 | Batch Status: 32000/50000 (64%) | Loss: 4.676530\n",
            "Train Epoch: 1 | Batch Status: 32080/50000 (64%) | Loss: 3.763473\n",
            "Train Epoch: 1 | Batch Status: 32160/50000 (64%) | Loss: 4.256693\n",
            "Train Epoch: 1 | Batch Status: 32240/50000 (64%) | Loss: 3.900819\n",
            "Train Epoch: 1 | Batch Status: 32320/50000 (65%) | Loss: 3.738218\n",
            "Train Epoch: 1 | Batch Status: 32400/50000 (65%) | Loss: 4.724709\n",
            "Train Epoch: 1 | Batch Status: 32480/50000 (65%) | Loss: 3.848326\n",
            "Train Epoch: 1 | Batch Status: 32560/50000 (65%) | Loss: 4.127504\n",
            "Train Epoch: 1 | Batch Status: 32640/50000 (65%) | Loss: 4.354757\n",
            "Train Epoch: 1 | Batch Status: 32720/50000 (65%) | Loss: 4.682338\n",
            "Train Epoch: 1 | Batch Status: 32800/50000 (66%) | Loss: 3.777226\n",
            "Train Epoch: 1 | Batch Status: 32880/50000 (66%) | Loss: 3.953361\n",
            "Train Epoch: 1 | Batch Status: 32960/50000 (66%) | Loss: 4.058797\n",
            "Train Epoch: 1 | Batch Status: 33040/50000 (66%) | Loss: 3.809297\n",
            "Train Epoch: 1 | Batch Status: 33120/50000 (66%) | Loss: 3.687369\n",
            "Train Epoch: 1 | Batch Status: 33200/50000 (66%) | Loss: 3.888715\n",
            "Train Epoch: 1 | Batch Status: 33280/50000 (67%) | Loss: 4.787370\n",
            "Train Epoch: 1 | Batch Status: 33360/50000 (67%) | Loss: 3.874857\n",
            "Train Epoch: 1 | Batch Status: 33440/50000 (67%) | Loss: 3.780574\n",
            "Train Epoch: 1 | Batch Status: 33520/50000 (67%) | Loss: 3.738454\n",
            "Train Epoch: 1 | Batch Status: 33600/50000 (67%) | Loss: 4.480851\n",
            "Train Epoch: 1 | Batch Status: 33680/50000 (67%) | Loss: 4.009128\n",
            "Train Epoch: 1 | Batch Status: 33760/50000 (68%) | Loss: 3.776646\n",
            "Train Epoch: 1 | Batch Status: 33840/50000 (68%) | Loss: 3.985324\n",
            "Train Epoch: 1 | Batch Status: 33920/50000 (68%) | Loss: 4.091982\n",
            "Train Epoch: 1 | Batch Status: 34000/50000 (68%) | Loss: 3.848540\n",
            "Train Epoch: 1 | Batch Status: 34080/50000 (68%) | Loss: 4.166327\n",
            "Train Epoch: 1 | Batch Status: 34160/50000 (68%) | Loss: 4.136412\n",
            "Train Epoch: 1 | Batch Status: 34240/50000 (68%) | Loss: 4.265390\n",
            "Train Epoch: 1 | Batch Status: 34320/50000 (69%) | Loss: 3.517250\n",
            "Train Epoch: 1 | Batch Status: 34400/50000 (69%) | Loss: 4.083099\n",
            "Train Epoch: 1 | Batch Status: 34480/50000 (69%) | Loss: 3.713346\n",
            "Train Epoch: 1 | Batch Status: 34560/50000 (69%) | Loss: 4.305967\n",
            "Train Epoch: 1 | Batch Status: 34640/50000 (69%) | Loss: 3.631582\n",
            "Train Epoch: 1 | Batch Status: 34720/50000 (69%) | Loss: 3.894703\n",
            "Train Epoch: 1 | Batch Status: 34800/50000 (70%) | Loss: 4.100981\n",
            "Train Epoch: 1 | Batch Status: 34880/50000 (70%) | Loss: 4.685959\n",
            "Train Epoch: 1 | Batch Status: 34960/50000 (70%) | Loss: 3.943597\n",
            "Train Epoch: 1 | Batch Status: 35040/50000 (70%) | Loss: 3.702615\n",
            "Train Epoch: 1 | Batch Status: 35120/50000 (70%) | Loss: 4.127490\n",
            "Train Epoch: 1 | Batch Status: 35200/50000 (70%) | Loss: 4.357294\n",
            "Train Epoch: 1 | Batch Status: 35280/50000 (71%) | Loss: 4.534686\n",
            "Train Epoch: 1 | Batch Status: 35360/50000 (71%) | Loss: 4.199235\n",
            "Train Epoch: 1 | Batch Status: 35440/50000 (71%) | Loss: 3.914994\n",
            "Train Epoch: 1 | Batch Status: 35520/50000 (71%) | Loss: 3.688704\n",
            "Train Epoch: 1 | Batch Status: 35600/50000 (71%) | Loss: 3.973310\n",
            "Train Epoch: 1 | Batch Status: 35680/50000 (71%) | Loss: 3.838592\n",
            "Train Epoch: 1 | Batch Status: 35760/50000 (72%) | Loss: 3.960845\n",
            "Train Epoch: 1 | Batch Status: 35840/50000 (72%) | Loss: 3.960336\n",
            "Train Epoch: 1 | Batch Status: 35920/50000 (72%) | Loss: 3.724226\n",
            "Train Epoch: 1 | Batch Status: 36000/50000 (72%) | Loss: 4.197046\n",
            "Train Epoch: 1 | Batch Status: 36080/50000 (72%) | Loss: 3.970383\n",
            "Train Epoch: 1 | Batch Status: 36160/50000 (72%) | Loss: 3.814113\n",
            "Train Epoch: 1 | Batch Status: 36240/50000 (72%) | Loss: 3.590648\n",
            "Train Epoch: 1 | Batch Status: 36320/50000 (73%) | Loss: 3.232542\n",
            "Train Epoch: 1 | Batch Status: 36400/50000 (73%) | Loss: 3.624818\n",
            "Train Epoch: 1 | Batch Status: 36480/50000 (73%) | Loss: 4.213232\n",
            "Train Epoch: 1 | Batch Status: 36560/50000 (73%) | Loss: 4.392506\n",
            "Train Epoch: 1 | Batch Status: 36640/50000 (73%) | Loss: 4.802135\n",
            "Train Epoch: 1 | Batch Status: 36720/50000 (73%) | Loss: 3.514621\n",
            "Train Epoch: 1 | Batch Status: 36800/50000 (74%) | Loss: 3.608388\n",
            "Train Epoch: 1 | Batch Status: 36880/50000 (74%) | Loss: 3.587944\n",
            "Train Epoch: 1 | Batch Status: 36960/50000 (74%) | Loss: 3.801599\n",
            "Train Epoch: 1 | Batch Status: 37040/50000 (74%) | Loss: 3.857506\n",
            "Train Epoch: 1 | Batch Status: 37120/50000 (74%) | Loss: 3.290861\n",
            "Train Epoch: 1 | Batch Status: 37200/50000 (74%) | Loss: 4.236234\n",
            "Train Epoch: 1 | Batch Status: 37280/50000 (75%) | Loss: 4.498427\n",
            "Train Epoch: 1 | Batch Status: 37360/50000 (75%) | Loss: 4.403754\n",
            "Train Epoch: 1 | Batch Status: 37440/50000 (75%) | Loss: 4.092487\n",
            "Train Epoch: 1 | Batch Status: 37520/50000 (75%) | Loss: 3.549885\n",
            "Train Epoch: 1 | Batch Status: 37600/50000 (75%) | Loss: 4.035645\n",
            "Train Epoch: 1 | Batch Status: 37680/50000 (75%) | Loss: 3.536653\n",
            "Train Epoch: 1 | Batch Status: 37760/50000 (76%) | Loss: 4.155837\n",
            "Train Epoch: 1 | Batch Status: 37840/50000 (76%) | Loss: 4.261336\n",
            "Train Epoch: 1 | Batch Status: 37920/50000 (76%) | Loss: 4.230612\n",
            "Train Epoch: 1 | Batch Status: 38000/50000 (76%) | Loss: 4.405929\n",
            "Train Epoch: 1 | Batch Status: 38080/50000 (76%) | Loss: 4.048547\n",
            "Train Epoch: 1 | Batch Status: 38160/50000 (76%) | Loss: 3.687022\n",
            "Train Epoch: 1 | Batch Status: 38240/50000 (76%) | Loss: 3.742127\n",
            "Train Epoch: 1 | Batch Status: 38320/50000 (77%) | Loss: 3.783172\n",
            "Train Epoch: 1 | Batch Status: 38400/50000 (77%) | Loss: 4.362874\n",
            "Train Epoch: 1 | Batch Status: 38480/50000 (77%) | Loss: 3.574380\n",
            "Train Epoch: 1 | Batch Status: 38560/50000 (77%) | Loss: 4.124374\n",
            "Train Epoch: 1 | Batch Status: 38640/50000 (77%) | Loss: 3.817467\n",
            "Train Epoch: 1 | Batch Status: 38720/50000 (77%) | Loss: 4.138119\n",
            "Train Epoch: 1 | Batch Status: 38800/50000 (78%) | Loss: 4.018133\n",
            "Train Epoch: 1 | Batch Status: 38880/50000 (78%) | Loss: 3.442930\n",
            "Train Epoch: 1 | Batch Status: 38960/50000 (78%) | Loss: 3.492779\n",
            "Train Epoch: 1 | Batch Status: 39040/50000 (78%) | Loss: 3.861734\n",
            "Train Epoch: 1 | Batch Status: 39120/50000 (78%) | Loss: 4.458717\n",
            "Train Epoch: 1 | Batch Status: 39200/50000 (78%) | Loss: 3.874151\n",
            "Train Epoch: 1 | Batch Status: 39280/50000 (79%) | Loss: 3.822485\n",
            "Train Epoch: 1 | Batch Status: 39360/50000 (79%) | Loss: 4.113553\n",
            "Train Epoch: 1 | Batch Status: 39440/50000 (79%) | Loss: 4.125545\n",
            "Train Epoch: 1 | Batch Status: 39520/50000 (79%) | Loss: 4.505338\n",
            "Train Epoch: 1 | Batch Status: 39600/50000 (79%) | Loss: 4.110549\n",
            "Train Epoch: 1 | Batch Status: 39680/50000 (79%) | Loss: 3.782140\n",
            "Train Epoch: 1 | Batch Status: 39760/50000 (80%) | Loss: 4.633217\n",
            "Train Epoch: 1 | Batch Status: 39840/50000 (80%) | Loss: 3.387146\n",
            "Train Epoch: 1 | Batch Status: 39920/50000 (80%) | Loss: 3.864059\n",
            "Train Epoch: 1 | Batch Status: 40000/50000 (80%) | Loss: 3.869047\n",
            "Train Epoch: 1 | Batch Status: 40080/50000 (80%) | Loss: 4.631638\n",
            "Train Epoch: 1 | Batch Status: 40160/50000 (80%) | Loss: 4.196899\n",
            "Train Epoch: 1 | Batch Status: 40240/50000 (80%) | Loss: 3.291883\n",
            "Train Epoch: 1 | Batch Status: 40320/50000 (81%) | Loss: 4.219761\n",
            "Train Epoch: 1 | Batch Status: 40400/50000 (81%) | Loss: 4.097373\n",
            "Train Epoch: 1 | Batch Status: 40480/50000 (81%) | Loss: 4.213831\n",
            "Train Epoch: 1 | Batch Status: 40560/50000 (81%) | Loss: 3.775834\n",
            "Train Epoch: 1 | Batch Status: 40640/50000 (81%) | Loss: 3.924412\n",
            "Train Epoch: 1 | Batch Status: 40720/50000 (81%) | Loss: 3.865131\n",
            "Train Epoch: 1 | Batch Status: 40800/50000 (82%) | Loss: 3.967812\n",
            "Train Epoch: 1 | Batch Status: 40880/50000 (82%) | Loss: 3.797836\n",
            "Train Epoch: 1 | Batch Status: 40960/50000 (82%) | Loss: 3.692713\n",
            "Train Epoch: 1 | Batch Status: 41040/50000 (82%) | Loss: 3.909889\n",
            "Train Epoch: 1 | Batch Status: 41120/50000 (82%) | Loss: 3.862822\n",
            "Train Epoch: 1 | Batch Status: 41200/50000 (82%) | Loss: 3.761112\n",
            "Train Epoch: 1 | Batch Status: 41280/50000 (83%) | Loss: 4.137334\n",
            "Train Epoch: 1 | Batch Status: 41360/50000 (83%) | Loss: 4.168437\n",
            "Train Epoch: 1 | Batch Status: 41440/50000 (83%) | Loss: 4.121254\n",
            "Train Epoch: 1 | Batch Status: 41520/50000 (83%) | Loss: 3.326264\n",
            "Train Epoch: 1 | Batch Status: 41600/50000 (83%) | Loss: 4.183359\n",
            "Train Epoch: 1 | Batch Status: 41680/50000 (83%) | Loss: 4.410469\n",
            "Train Epoch: 1 | Batch Status: 41760/50000 (84%) | Loss: 4.082989\n",
            "Train Epoch: 1 | Batch Status: 41840/50000 (84%) | Loss: 4.552786\n",
            "Train Epoch: 1 | Batch Status: 41920/50000 (84%) | Loss: 3.902675\n",
            "Train Epoch: 1 | Batch Status: 42000/50000 (84%) | Loss: 4.330314\n",
            "Train Epoch: 1 | Batch Status: 42080/50000 (84%) | Loss: 3.580002\n",
            "Train Epoch: 1 | Batch Status: 42160/50000 (84%) | Loss: 4.645187\n",
            "Train Epoch: 1 | Batch Status: 42240/50000 (84%) | Loss: 4.697977\n",
            "Train Epoch: 1 | Batch Status: 42320/50000 (85%) | Loss: 4.526745\n",
            "Train Epoch: 1 | Batch Status: 42400/50000 (85%) | Loss: 3.291668\n",
            "Train Epoch: 1 | Batch Status: 42480/50000 (85%) | Loss: 3.771779\n",
            "Train Epoch: 1 | Batch Status: 42560/50000 (85%) | Loss: 3.339927\n",
            "Train Epoch: 1 | Batch Status: 42640/50000 (85%) | Loss: 3.784681\n",
            "Train Epoch: 1 | Batch Status: 42720/50000 (85%) | Loss: 4.406358\n",
            "Train Epoch: 1 | Batch Status: 42800/50000 (86%) | Loss: 3.921104\n",
            "Train Epoch: 1 | Batch Status: 42880/50000 (86%) | Loss: 3.839010\n",
            "Train Epoch: 1 | Batch Status: 42960/50000 (86%) | Loss: 3.026136\n",
            "Train Epoch: 1 | Batch Status: 43040/50000 (86%) | Loss: 3.741091\n",
            "Train Epoch: 1 | Batch Status: 43120/50000 (86%) | Loss: 4.079205\n",
            "Train Epoch: 1 | Batch Status: 43200/50000 (86%) | Loss: 3.767244\n",
            "Train Epoch: 1 | Batch Status: 43280/50000 (87%) | Loss: 4.151707\n",
            "Train Epoch: 1 | Batch Status: 43360/50000 (87%) | Loss: 4.225069\n",
            "Train Epoch: 1 | Batch Status: 43440/50000 (87%) | Loss: 3.310466\n",
            "Train Epoch: 1 | Batch Status: 43520/50000 (87%) | Loss: 4.517922\n",
            "Train Epoch: 1 | Batch Status: 43600/50000 (87%) | Loss: 4.103270\n",
            "Train Epoch: 1 | Batch Status: 43680/50000 (87%) | Loss: 4.592070\n",
            "Train Epoch: 1 | Batch Status: 43760/50000 (88%) | Loss: 4.765255\n",
            "Train Epoch: 1 | Batch Status: 43840/50000 (88%) | Loss: 3.833310\n",
            "Train Epoch: 1 | Batch Status: 43920/50000 (88%) | Loss: 4.519243\n",
            "Train Epoch: 1 | Batch Status: 44000/50000 (88%) | Loss: 4.244715\n",
            "Train Epoch: 1 | Batch Status: 44080/50000 (88%) | Loss: 4.048754\n",
            "Train Epoch: 1 | Batch Status: 44160/50000 (88%) | Loss: 3.986319\n",
            "Train Epoch: 1 | Batch Status: 44240/50000 (88%) | Loss: 3.663095\n",
            "Train Epoch: 1 | Batch Status: 44320/50000 (89%) | Loss: 3.951874\n",
            "Train Epoch: 1 | Batch Status: 44400/50000 (89%) | Loss: 3.897919\n",
            "Train Epoch: 1 | Batch Status: 44480/50000 (89%) | Loss: 3.541227\n",
            "Train Epoch: 1 | Batch Status: 44560/50000 (89%) | Loss: 4.360494\n",
            "Train Epoch: 1 | Batch Status: 44640/50000 (89%) | Loss: 3.840531\n",
            "Train Epoch: 1 | Batch Status: 44720/50000 (89%) | Loss: 3.675040\n",
            "Train Epoch: 1 | Batch Status: 44800/50000 (90%) | Loss: 3.770386\n",
            "Train Epoch: 1 | Batch Status: 44880/50000 (90%) | Loss: 3.781455\n",
            "Train Epoch: 1 | Batch Status: 44960/50000 (90%) | Loss: 4.403551\n",
            "Train Epoch: 1 | Batch Status: 45040/50000 (90%) | Loss: 3.568001\n",
            "Train Epoch: 1 | Batch Status: 45120/50000 (90%) | Loss: 3.412020\n",
            "Train Epoch: 1 | Batch Status: 45200/50000 (90%) | Loss: 4.796935\n",
            "Train Epoch: 1 | Batch Status: 45280/50000 (91%) | Loss: 4.174907\n",
            "Train Epoch: 1 | Batch Status: 45360/50000 (91%) | Loss: 3.974944\n",
            "Train Epoch: 1 | Batch Status: 45440/50000 (91%) | Loss: 4.653754\n",
            "Train Epoch: 1 | Batch Status: 45520/50000 (91%) | Loss: 4.244863\n",
            "Train Epoch: 1 | Batch Status: 45600/50000 (91%) | Loss: 3.910775\n",
            "Train Epoch: 1 | Batch Status: 45680/50000 (91%) | Loss: 4.633042\n",
            "Train Epoch: 1 | Batch Status: 45760/50000 (92%) | Loss: 4.449152\n",
            "Train Epoch: 1 | Batch Status: 45840/50000 (92%) | Loss: 4.339781\n",
            "Train Epoch: 1 | Batch Status: 45920/50000 (92%) | Loss: 3.299538\n",
            "Train Epoch: 1 | Batch Status: 46000/50000 (92%) | Loss: 3.950302\n",
            "Train Epoch: 1 | Batch Status: 46080/50000 (92%) | Loss: 3.952359\n",
            "Train Epoch: 1 | Batch Status: 46160/50000 (92%) | Loss: 4.502436\n",
            "Train Epoch: 1 | Batch Status: 46240/50000 (92%) | Loss: 4.183892\n",
            "Train Epoch: 1 | Batch Status: 46320/50000 (93%) | Loss: 3.751795\n",
            "Train Epoch: 1 | Batch Status: 46400/50000 (93%) | Loss: 3.313827\n",
            "Train Epoch: 1 | Batch Status: 46480/50000 (93%) | Loss: 4.405162\n",
            "Train Epoch: 1 | Batch Status: 46560/50000 (93%) | Loss: 3.840487\n",
            "Train Epoch: 1 | Batch Status: 46640/50000 (93%) | Loss: 4.452207\n",
            "Train Epoch: 1 | Batch Status: 46720/50000 (93%) | Loss: 3.982342\n",
            "Train Epoch: 1 | Batch Status: 46800/50000 (94%) | Loss: 3.931809\n",
            "Train Epoch: 1 | Batch Status: 46880/50000 (94%) | Loss: 4.265542\n",
            "Train Epoch: 1 | Batch Status: 46960/50000 (94%) | Loss: 3.688946\n",
            "Train Epoch: 1 | Batch Status: 47040/50000 (94%) | Loss: 3.394637\n",
            "Train Epoch: 1 | Batch Status: 47120/50000 (94%) | Loss: 3.814487\n",
            "Train Epoch: 1 | Batch Status: 47200/50000 (94%) | Loss: 3.681997\n",
            "Train Epoch: 1 | Batch Status: 47280/50000 (95%) | Loss: 4.093396\n",
            "Train Epoch: 1 | Batch Status: 47360/50000 (95%) | Loss: 4.545360\n",
            "Train Epoch: 1 | Batch Status: 47440/50000 (95%) | Loss: 3.485237\n",
            "Train Epoch: 1 | Batch Status: 47520/50000 (95%) | Loss: 3.186043\n",
            "Train Epoch: 1 | Batch Status: 47600/50000 (95%) | Loss: 3.727571\n",
            "Train Epoch: 1 | Batch Status: 47680/50000 (95%) | Loss: 3.990960\n",
            "Train Epoch: 1 | Batch Status: 47760/50000 (96%) | Loss: 3.539423\n",
            "Train Epoch: 1 | Batch Status: 47840/50000 (96%) | Loss: 4.430447\n",
            "Train Epoch: 1 | Batch Status: 47920/50000 (96%) | Loss: 3.996608\n",
            "Train Epoch: 1 | Batch Status: 48000/50000 (96%) | Loss: 3.252500\n",
            "Train Epoch: 1 | Batch Status: 48080/50000 (96%) | Loss: 4.413767\n",
            "Train Epoch: 1 | Batch Status: 48160/50000 (96%) | Loss: 3.706016\n",
            "Train Epoch: 1 | Batch Status: 48240/50000 (96%) | Loss: 3.577757\n",
            "Train Epoch: 1 | Batch Status: 48320/50000 (97%) | Loss: 4.179732\n",
            "Train Epoch: 1 | Batch Status: 48400/50000 (97%) | Loss: 4.349965\n",
            "Train Epoch: 1 | Batch Status: 48480/50000 (97%) | Loss: 4.544755\n",
            "Train Epoch: 1 | Batch Status: 48560/50000 (97%) | Loss: 3.388700\n",
            "Train Epoch: 1 | Batch Status: 48640/50000 (97%) | Loss: 3.667378\n",
            "Train Epoch: 1 | Batch Status: 48720/50000 (97%) | Loss: 4.165848\n",
            "Train Epoch: 1 | Batch Status: 48800/50000 (98%) | Loss: 3.535324\n",
            "Train Epoch: 1 | Batch Status: 48880/50000 (98%) | Loss: 4.059276\n",
            "Train Epoch: 1 | Batch Status: 48960/50000 (98%) | Loss: 3.347666\n",
            "Train Epoch: 1 | Batch Status: 49040/50000 (98%) | Loss: 3.821647\n",
            "Train Epoch: 1 | Batch Status: 49120/50000 (98%) | Loss: 3.024515\n",
            "Train Epoch: 1 | Batch Status: 49200/50000 (98%) | Loss: 4.635336\n",
            "Train Epoch: 1 | Batch Status: 49280/50000 (99%) | Loss: 4.212307\n",
            "Train Epoch: 1 | Batch Status: 49360/50000 (99%) | Loss: 4.128840\n",
            "Train Epoch: 1 | Batch Status: 49440/50000 (99%) | Loss: 4.329609\n",
            "Train Epoch: 1 | Batch Status: 49520/50000 (99%) | Loss: 3.759994\n",
            "Train Epoch: 1 | Batch Status: 49600/50000 (99%) | Loss: 3.305351\n",
            "Train Epoch: 1 | Batch Status: 49680/50000 (99%) | Loss: 3.998686\n",
            "Train Epoch: 1 | Batch Status: 49760/50000 (100%) | Loss: 3.855879\n",
            "Train Epoch: 1 | Batch Status: 49840/50000 (100%) | Loss: 3.133072\n",
            "Train Epoch: 1 | Batch Status: 49920/50000 (100%) | Loss: 3.035719\n",
            "Training time: 46m 14s\n",
            "Testing time: 49m 5s\n",
            "Total Time: 49m 5s\n",
            "Model was trained on cuda!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. [Exercise 11-2] 아래 논문을 읽고, ResNet18을 구현한 다음 CIFAR-100에 대한 학습을 수행하시\n",
        "오. (소스코드와 결과를 첨부할 것) (링크: https://arxiv.org/abs/1512.03385)"
      ],
      "metadata": {
        "id": "k3eHSgvoHZen"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.residual_function = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels * BasicBlock.expansion, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels * BasicBlock.expansion)\n",
        "        )\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "\n",
        "        if stride != 1 or in_channels != BasicBlock.expansion * out_channels:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels * BasicBlock.expansion, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(out_channels * BasicBlock.expansion)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return nn.ReLU(inplace=True)(self.residual_function(x) + self.shortcut(x))\n",
        "\n",
        "class BottleNeck(nn.Module):\n",
        "    expansion = 4\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super().__init__()\n",
        "        self.residual_function = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, stride=stride, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels * BottleNeck.expansion, kernel_size=1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels * BottleNeck.expansion),\n",
        "        )\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "\n",
        "        if stride != 1 or in_channels != out_channels * BottleNeck.expansion:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels * BottleNeck.expansion, stride=stride, kernel_size=1, bias=False),\n",
        "                nn.BatchNorm2d(out_channels * BottleNeck.expansion)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return nn.ReLU(inplace=True)(self.residual_function(x) + self.shortcut(x))\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "\n",
        "    def __init__(self, block, num_block, num_classes=100):\n",
        "        super().__init__()\n",
        "\n",
        "        self.in_channels = 64\n",
        "\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True))\n",
        "        #we use a different inputsize than the original paper\n",
        "        #so conv2_x's stride is 1\n",
        "        self.conv2_x = self._make_layer(block, 64, num_block[0], 1)\n",
        "        self.conv3_x = self._make_layer(block, 128, num_block[1], 2)\n",
        "        self.conv4_x = self._make_layer(block, 256, num_block[2], 2)\n",
        "        self.conv5_x = self._make_layer(block, 512, num_block[3], 2)\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, out_channels, num_blocks, stride):\n",
        "        strides = [stride] + [1] * (num_blocks - 1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_channels, out_channels, stride))\n",
        "            self.in_channels = out_channels * block.expansion\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = self.conv1(x)\n",
        "        output = self.conv2_x(output)\n",
        "        output = self.conv3_x(output)\n",
        "        output = self.conv4_x(output)\n",
        "        output = self.conv5_x(output)\n",
        "        output = self.avg_pool(output)\n",
        "        output = output.view(output.size(0), -1)\n",
        "        output = self.fc(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "def resnet18():\n",
        "    return ResNet(BasicBlock, [2, 2, 2, 2])"
      ],
      "metadata": {
        "id": "Yy0hXQnfSjYW"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'\n",
        "\n",
        "train_dataset = datasets.CIFAR100(root='./cifal100_data/',\n",
        "                               train=True,\n",
        "                               transform=transforms.ToTensor(),\n",
        "                               download=True)\n",
        "\n",
        "test_dataset = datasets.CIFAR100(root='./cifal100_data/',\n",
        "                              train=False,\n",
        "                              transform=transforms.ToTensor())\n",
        "\n",
        "\n",
        "train_loader = data.DataLoader(dataset=train_dataset,\n",
        "                                           batch_size=batch_size,\n",
        "                                           shuffle=True)\n",
        "\n",
        "test_loader = data.DataLoader(dataset=test_dataset,\n",
        "                                          batch_size=batch_size,\n",
        "                                          shuffle=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v1yjiS-ZxrkY",
        "outputId": "697c8e73-58df-4bf2-ea66-a6e1d420f21a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = resnet18()\n",
        "model.to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    since = time.time()\n",
        "    for epoch in range(1, 5):\n",
        "        epoch_start = time.time()\n",
        "        train(epoch)\n",
        "        m, s = divmod(time.time() - epoch_start, 60)\n",
        "        print(f'Training time: {m:.0f}m {s:.0f}s')\n",
        "        test()\n",
        "        m, s = divmod(time.time() - epoch_start, 60)\n",
        "        print(f'Testing time: {m:.0f}m {s:.0f}s')\n",
        "\n",
        "    m, s = divmod(time.time() - since, 60)\n",
        "    print(f'Total Time: {m:.0f}m {s:.0f}s\\nModel was trained on {device}!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gE8-tnzFWDKM",
        "outputId": "40260fa1-867d-4344-c450-51f4a0607ced"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 | Batch Status: 0/50000 (0%) | Loss: 4.861624\n",
            "Train Epoch: 1 | Batch Status: 320/50000 (1%) | Loss: 4.720034\n",
            "Train Epoch: 1 | Batch Status: 640/50000 (1%) | Loss: 4.492246\n",
            "Train Epoch: 1 | Batch Status: 960/50000 (2%) | Loss: 4.546706\n",
            "Train Epoch: 1 | Batch Status: 1280/50000 (3%) | Loss: 4.194552\n",
            "Train Epoch: 1 | Batch Status: 1600/50000 (3%) | Loss: 4.202145\n",
            "Train Epoch: 1 | Batch Status: 1920/50000 (4%) | Loss: 4.260677\n",
            "Train Epoch: 1 | Batch Status: 2240/50000 (4%) | Loss: 4.505493\n",
            "Train Epoch: 1 | Batch Status: 2560/50000 (5%) | Loss: 4.406214\n",
            "Train Epoch: 1 | Batch Status: 2880/50000 (6%) | Loss: 3.994364\n",
            "Train Epoch: 1 | Batch Status: 3200/50000 (6%) | Loss: 4.244756\n",
            "Train Epoch: 1 | Batch Status: 3520/50000 (7%) | Loss: 3.973985\n",
            "Train Epoch: 1 | Batch Status: 3840/50000 (8%) | Loss: 3.792170\n",
            "Train Epoch: 1 | Batch Status: 4160/50000 (8%) | Loss: 3.875527\n",
            "Train Epoch: 1 | Batch Status: 4480/50000 (9%) | Loss: 3.786405\n",
            "Train Epoch: 1 | Batch Status: 4800/50000 (10%) | Loss: 4.239514\n",
            "Train Epoch: 1 | Batch Status: 5120/50000 (10%) | Loss: 4.076033\n",
            "Train Epoch: 1 | Batch Status: 5440/50000 (11%) | Loss: 4.303668\n",
            "Train Epoch: 1 | Batch Status: 5760/50000 (12%) | Loss: 3.606593\n",
            "Train Epoch: 1 | Batch Status: 6080/50000 (12%) | Loss: 4.065771\n",
            "Train Epoch: 1 | Batch Status: 6400/50000 (13%) | Loss: 3.637812\n",
            "Train Epoch: 1 | Batch Status: 6720/50000 (13%) | Loss: 4.157724\n",
            "Train Epoch: 1 | Batch Status: 7040/50000 (14%) | Loss: 3.833326\n",
            "Train Epoch: 1 | Batch Status: 7360/50000 (15%) | Loss: 3.926021\n",
            "Train Epoch: 1 | Batch Status: 7680/50000 (15%) | Loss: 4.015110\n",
            "Train Epoch: 1 | Batch Status: 8000/50000 (16%) | Loss: 3.850874\n",
            "Train Epoch: 1 | Batch Status: 8320/50000 (17%) | Loss: 3.567175\n",
            "Train Epoch: 1 | Batch Status: 8640/50000 (17%) | Loss: 4.276240\n",
            "Train Epoch: 1 | Batch Status: 8960/50000 (18%) | Loss: 3.414375\n",
            "Train Epoch: 1 | Batch Status: 9280/50000 (19%) | Loss: 4.239754\n",
            "Train Epoch: 1 | Batch Status: 9600/50000 (19%) | Loss: 3.858021\n",
            "Train Epoch: 1 | Batch Status: 9920/50000 (20%) | Loss: 3.612435\n",
            "Train Epoch: 1 | Batch Status: 10240/50000 (20%) | Loss: 3.689767\n",
            "Train Epoch: 1 | Batch Status: 10560/50000 (21%) | Loss: 3.998828\n",
            "Train Epoch: 1 | Batch Status: 10880/50000 (22%) | Loss: 3.562507\n",
            "Train Epoch: 1 | Batch Status: 11200/50000 (22%) | Loss: 3.850878\n",
            "Train Epoch: 1 | Batch Status: 11520/50000 (23%) | Loss: 3.287294\n",
            "Train Epoch: 1 | Batch Status: 11840/50000 (24%) | Loss: 3.804845\n",
            "Train Epoch: 1 | Batch Status: 12160/50000 (24%) | Loss: 3.799014\n",
            "Train Epoch: 1 | Batch Status: 12480/50000 (25%) | Loss: 3.608171\n",
            "Train Epoch: 1 | Batch Status: 12800/50000 (26%) | Loss: 3.780547\n",
            "Train Epoch: 1 | Batch Status: 13120/50000 (26%) | Loss: 3.592119\n",
            "Train Epoch: 1 | Batch Status: 13440/50000 (27%) | Loss: 3.790366\n",
            "Train Epoch: 1 | Batch Status: 13760/50000 (28%) | Loss: 3.327590\n",
            "Train Epoch: 1 | Batch Status: 14080/50000 (28%) | Loss: 3.988420\n",
            "Train Epoch: 1 | Batch Status: 14400/50000 (29%) | Loss: 3.391677\n",
            "Train Epoch: 1 | Batch Status: 14720/50000 (29%) | Loss: 3.529398\n",
            "Train Epoch: 1 | Batch Status: 15040/50000 (30%) | Loss: 3.605071\n",
            "Train Epoch: 1 | Batch Status: 15360/50000 (31%) | Loss: 3.811984\n",
            "Train Epoch: 1 | Batch Status: 15680/50000 (31%) | Loss: 3.354019\n",
            "Train Epoch: 1 | Batch Status: 16000/50000 (32%) | Loss: 3.246167\n",
            "Train Epoch: 1 | Batch Status: 16320/50000 (33%) | Loss: 3.755527\n",
            "Train Epoch: 1 | Batch Status: 16640/50000 (33%) | Loss: 3.991439\n",
            "Train Epoch: 1 | Batch Status: 16960/50000 (34%) | Loss: 3.542258\n",
            "Train Epoch: 1 | Batch Status: 17280/50000 (35%) | Loss: 3.611542\n",
            "Train Epoch: 1 | Batch Status: 17600/50000 (35%) | Loss: 3.470458\n",
            "Train Epoch: 1 | Batch Status: 17920/50000 (36%) | Loss: 3.854253\n",
            "Train Epoch: 1 | Batch Status: 18240/50000 (36%) | Loss: 3.469520\n",
            "Train Epoch: 1 | Batch Status: 18560/50000 (37%) | Loss: 3.395423\n",
            "Train Epoch: 1 | Batch Status: 18880/50000 (38%) | Loss: 3.424721\n",
            "Train Epoch: 1 | Batch Status: 19200/50000 (38%) | Loss: 3.456613\n",
            "Train Epoch: 1 | Batch Status: 19520/50000 (39%) | Loss: 3.022452\n",
            "Train Epoch: 1 | Batch Status: 19840/50000 (40%) | Loss: 3.112373\n",
            "Train Epoch: 1 | Batch Status: 20160/50000 (40%) | Loss: 3.714027\n",
            "Train Epoch: 1 | Batch Status: 20480/50000 (41%) | Loss: 3.496146\n",
            "Train Epoch: 1 | Batch Status: 20800/50000 (42%) | Loss: 3.759318\n",
            "Train Epoch: 1 | Batch Status: 21120/50000 (42%) | Loss: 3.051234\n",
            "Train Epoch: 1 | Batch Status: 21440/50000 (43%) | Loss: 3.161061\n",
            "Train Epoch: 1 | Batch Status: 21760/50000 (44%) | Loss: 3.158412\n",
            "Train Epoch: 1 | Batch Status: 22080/50000 (44%) | Loss: 2.998575\n",
            "Train Epoch: 1 | Batch Status: 22400/50000 (45%) | Loss: 3.479258\n",
            "Train Epoch: 1 | Batch Status: 22720/50000 (45%) | Loss: 3.341928\n",
            "Train Epoch: 1 | Batch Status: 23040/50000 (46%) | Loss: 3.565723\n",
            "Train Epoch: 1 | Batch Status: 23360/50000 (47%) | Loss: 3.590160\n",
            "Train Epoch: 1 | Batch Status: 23680/50000 (47%) | Loss: 3.042766\n",
            "Train Epoch: 1 | Batch Status: 24000/50000 (48%) | Loss: 3.778757\n",
            "Train Epoch: 1 | Batch Status: 24320/50000 (49%) | Loss: 2.853198\n",
            "Train Epoch: 1 | Batch Status: 24640/50000 (49%) | Loss: 3.313819\n",
            "Train Epoch: 1 | Batch Status: 24960/50000 (50%) | Loss: 3.400456\n",
            "Train Epoch: 1 | Batch Status: 25280/50000 (51%) | Loss: 3.190599\n",
            "Train Epoch: 1 | Batch Status: 25600/50000 (51%) | Loss: 3.394720\n",
            "Train Epoch: 1 | Batch Status: 25920/50000 (52%) | Loss: 3.035834\n",
            "Train Epoch: 1 | Batch Status: 26240/50000 (52%) | Loss: 3.555447\n",
            "Train Epoch: 1 | Batch Status: 26560/50000 (53%) | Loss: 3.433244\n",
            "Train Epoch: 1 | Batch Status: 26880/50000 (54%) | Loss: 3.528951\n",
            "Train Epoch: 1 | Batch Status: 27200/50000 (54%) | Loss: 3.341569\n",
            "Train Epoch: 1 | Batch Status: 27520/50000 (55%) | Loss: 3.100723\n",
            "Train Epoch: 1 | Batch Status: 27840/50000 (56%) | Loss: 3.153254\n",
            "Train Epoch: 1 | Batch Status: 28160/50000 (56%) | Loss: 3.148771\n",
            "Train Epoch: 1 | Batch Status: 28480/50000 (57%) | Loss: 3.026805\n",
            "Train Epoch: 1 | Batch Status: 28800/50000 (58%) | Loss: 3.050348\n",
            "Train Epoch: 1 | Batch Status: 29120/50000 (58%) | Loss: 3.302551\n",
            "Train Epoch: 1 | Batch Status: 29440/50000 (59%) | Loss: 3.415921\n",
            "Train Epoch: 1 | Batch Status: 29760/50000 (60%) | Loss: 3.439700\n",
            "Train Epoch: 1 | Batch Status: 30080/50000 (60%) | Loss: 3.344549\n",
            "Train Epoch: 1 | Batch Status: 30400/50000 (61%) | Loss: 3.434793\n",
            "Train Epoch: 1 | Batch Status: 30720/50000 (61%) | Loss: 3.439165\n",
            "Train Epoch: 1 | Batch Status: 31040/50000 (62%) | Loss: 3.457921\n",
            "Train Epoch: 1 | Batch Status: 31360/50000 (63%) | Loss: 3.200796\n",
            "Train Epoch: 1 | Batch Status: 31680/50000 (63%) | Loss: 3.313319\n",
            "Train Epoch: 1 | Batch Status: 32000/50000 (64%) | Loss: 2.771787\n",
            "Train Epoch: 1 | Batch Status: 32320/50000 (65%) | Loss: 3.329746\n",
            "Train Epoch: 1 | Batch Status: 32640/50000 (65%) | Loss: 3.163185\n",
            "Train Epoch: 1 | Batch Status: 32960/50000 (66%) | Loss: 3.230318\n",
            "Train Epoch: 1 | Batch Status: 33280/50000 (67%) | Loss: 3.586034\n",
            "Train Epoch: 1 | Batch Status: 33600/50000 (67%) | Loss: 2.956653\n",
            "Train Epoch: 1 | Batch Status: 33920/50000 (68%) | Loss: 3.100517\n",
            "Train Epoch: 1 | Batch Status: 34240/50000 (68%) | Loss: 2.675680\n",
            "Train Epoch: 1 | Batch Status: 34560/50000 (69%) | Loss: 3.616161\n",
            "Train Epoch: 1 | Batch Status: 34880/50000 (70%) | Loss: 3.063087\n",
            "Train Epoch: 1 | Batch Status: 35200/50000 (70%) | Loss: 2.734989\n",
            "Train Epoch: 1 | Batch Status: 35520/50000 (71%) | Loss: 3.038407\n",
            "Train Epoch: 1 | Batch Status: 35840/50000 (72%) | Loss: 2.875515\n",
            "Train Epoch: 1 | Batch Status: 36160/50000 (72%) | Loss: 3.151248\n",
            "Train Epoch: 1 | Batch Status: 36480/50000 (73%) | Loss: 3.080575\n",
            "Train Epoch: 1 | Batch Status: 36800/50000 (74%) | Loss: 3.453171\n",
            "Train Epoch: 1 | Batch Status: 37120/50000 (74%) | Loss: 2.826993\n",
            "Train Epoch: 1 | Batch Status: 37440/50000 (75%) | Loss: 2.694628\n",
            "Train Epoch: 1 | Batch Status: 37760/50000 (75%) | Loss: 3.046744\n",
            "Train Epoch: 1 | Batch Status: 38080/50000 (76%) | Loss: 3.124177\n",
            "Train Epoch: 1 | Batch Status: 38400/50000 (77%) | Loss: 3.057516\n",
            "Train Epoch: 1 | Batch Status: 38720/50000 (77%) | Loss: 3.330792\n",
            "Train Epoch: 1 | Batch Status: 39040/50000 (78%) | Loss: 3.243991\n",
            "Train Epoch: 1 | Batch Status: 39360/50000 (79%) | Loss: 2.850742\n",
            "Train Epoch: 1 | Batch Status: 39680/50000 (79%) | Loss: 3.251368\n",
            "Train Epoch: 1 | Batch Status: 40000/50000 (80%) | Loss: 2.945290\n",
            "Train Epoch: 1 | Batch Status: 40320/50000 (81%) | Loss: 2.913240\n",
            "Train Epoch: 1 | Batch Status: 40640/50000 (81%) | Loss: 2.977684\n",
            "Train Epoch: 1 | Batch Status: 40960/50000 (82%) | Loss: 2.870521\n",
            "Train Epoch: 1 | Batch Status: 41280/50000 (83%) | Loss: 3.448703\n",
            "Train Epoch: 1 | Batch Status: 41600/50000 (83%) | Loss: 3.421783\n",
            "Train Epoch: 1 | Batch Status: 41920/50000 (84%) | Loss: 2.892674\n",
            "Train Epoch: 1 | Batch Status: 42240/50000 (84%) | Loss: 3.362463\n",
            "Train Epoch: 1 | Batch Status: 42560/50000 (85%) | Loss: 3.269094\n",
            "Train Epoch: 1 | Batch Status: 42880/50000 (86%) | Loss: 3.339932\n",
            "Train Epoch: 1 | Batch Status: 43200/50000 (86%) | Loss: 2.993613\n",
            "Train Epoch: 1 | Batch Status: 43520/50000 (87%) | Loss: 2.901301\n",
            "Train Epoch: 1 | Batch Status: 43840/50000 (88%) | Loss: 2.911736\n",
            "Train Epoch: 1 | Batch Status: 44160/50000 (88%) | Loss: 2.858338\n",
            "Train Epoch: 1 | Batch Status: 44480/50000 (89%) | Loss: 3.202857\n",
            "Train Epoch: 1 | Batch Status: 44800/50000 (90%) | Loss: 3.197204\n",
            "Train Epoch: 1 | Batch Status: 45120/50000 (90%) | Loss: 3.244578\n",
            "Train Epoch: 1 | Batch Status: 45440/50000 (91%) | Loss: 3.135288\n",
            "Train Epoch: 1 | Batch Status: 45760/50000 (91%) | Loss: 3.394263\n",
            "Train Epoch: 1 | Batch Status: 46080/50000 (92%) | Loss: 2.867568\n",
            "Train Epoch: 1 | Batch Status: 46400/50000 (93%) | Loss: 3.056846\n",
            "Train Epoch: 1 | Batch Status: 46720/50000 (93%) | Loss: 2.265291\n",
            "Train Epoch: 1 | Batch Status: 47040/50000 (94%) | Loss: 2.720554\n",
            "Train Epoch: 1 | Batch Status: 47360/50000 (95%) | Loss: 2.257156\n",
            "Train Epoch: 1 | Batch Status: 47680/50000 (95%) | Loss: 3.046371\n",
            "Train Epoch: 1 | Batch Status: 48000/50000 (96%) | Loss: 2.433591\n",
            "Train Epoch: 1 | Batch Status: 48320/50000 (97%) | Loss: 3.095041\n",
            "Train Epoch: 1 | Batch Status: 48640/50000 (97%) | Loss: 3.112222\n",
            "Train Epoch: 1 | Batch Status: 48960/50000 (98%) | Loss: 2.827187\n",
            "Train Epoch: 1 | Batch Status: 49280/50000 (99%) | Loss: 2.869955\n",
            "Train Epoch: 1 | Batch Status: 49600/50000 (99%) | Loss: 2.621519\n",
            "Train Epoch: 1 | Batch Status: 49920/50000 (100%) | Loss: 2.652912\n",
            "Training time: 2m 39s\n",
            "Testing time: 2m 50s\n",
            "Train Epoch: 2 | Batch Status: 0/50000 (0%) | Loss: 2.484235\n",
            "Train Epoch: 2 | Batch Status: 320/50000 (1%) | Loss: 3.038207\n",
            "Train Epoch: 2 | Batch Status: 640/50000 (1%) | Loss: 2.655412\n",
            "Train Epoch: 2 | Batch Status: 960/50000 (2%) | Loss: 2.481042\n",
            "Train Epoch: 2 | Batch Status: 1280/50000 (3%) | Loss: 2.592360\n",
            "Train Epoch: 2 | Batch Status: 1600/50000 (3%) | Loss: 2.759681\n",
            "Train Epoch: 2 | Batch Status: 1920/50000 (4%) | Loss: 2.287930\n",
            "Train Epoch: 2 | Batch Status: 2240/50000 (4%) | Loss: 2.692736\n",
            "Train Epoch: 2 | Batch Status: 2560/50000 (5%) | Loss: 2.949262\n",
            "Train Epoch: 2 | Batch Status: 2880/50000 (6%) | Loss: 2.574190\n",
            "Train Epoch: 2 | Batch Status: 3200/50000 (6%) | Loss: 3.166357\n",
            "Train Epoch: 2 | Batch Status: 3520/50000 (7%) | Loss: 2.604472\n",
            "Train Epoch: 2 | Batch Status: 3840/50000 (8%) | Loss: 2.396977\n",
            "Train Epoch: 2 | Batch Status: 4160/50000 (8%) | Loss: 2.536208\n",
            "Train Epoch: 2 | Batch Status: 4480/50000 (9%) | Loss: 2.343421\n",
            "Train Epoch: 2 | Batch Status: 4800/50000 (10%) | Loss: 3.039389\n",
            "Train Epoch: 2 | Batch Status: 5120/50000 (10%) | Loss: 2.755625\n",
            "Train Epoch: 2 | Batch Status: 5440/50000 (11%) | Loss: 3.275706\n",
            "Train Epoch: 2 | Batch Status: 5760/50000 (12%) | Loss: 2.638832\n",
            "Train Epoch: 2 | Batch Status: 6080/50000 (12%) | Loss: 2.612617\n",
            "Train Epoch: 2 | Batch Status: 6400/50000 (13%) | Loss: 2.453464\n",
            "Train Epoch: 2 | Batch Status: 6720/50000 (13%) | Loss: 3.033784\n",
            "Train Epoch: 2 | Batch Status: 7040/50000 (14%) | Loss: 2.667859\n",
            "Train Epoch: 2 | Batch Status: 7360/50000 (15%) | Loss: 3.119632\n",
            "Train Epoch: 2 | Batch Status: 7680/50000 (15%) | Loss: 2.349122\n",
            "Train Epoch: 2 | Batch Status: 8000/50000 (16%) | Loss: 2.528306\n",
            "Train Epoch: 2 | Batch Status: 8320/50000 (17%) | Loss: 3.210613\n",
            "Train Epoch: 2 | Batch Status: 8640/50000 (17%) | Loss: 2.563056\n",
            "Train Epoch: 2 | Batch Status: 8960/50000 (18%) | Loss: 2.540246\n",
            "Train Epoch: 2 | Batch Status: 9280/50000 (19%) | Loss: 2.538248\n",
            "Train Epoch: 2 | Batch Status: 9600/50000 (19%) | Loss: 2.577792\n",
            "Train Epoch: 2 | Batch Status: 9920/50000 (20%) | Loss: 2.670591\n",
            "Train Epoch: 2 | Batch Status: 10240/50000 (20%) | Loss: 2.602577\n",
            "Train Epoch: 2 | Batch Status: 10560/50000 (21%) | Loss: 2.550773\n",
            "Train Epoch: 2 | Batch Status: 10880/50000 (22%) | Loss: 2.541593\n",
            "Train Epoch: 2 | Batch Status: 11200/50000 (22%) | Loss: 2.734427\n",
            "Train Epoch: 2 | Batch Status: 11520/50000 (23%) | Loss: 2.177898\n",
            "Train Epoch: 2 | Batch Status: 11840/50000 (24%) | Loss: 2.382746\n",
            "Train Epoch: 2 | Batch Status: 12160/50000 (24%) | Loss: 2.948184\n",
            "Train Epoch: 2 | Batch Status: 12480/50000 (25%) | Loss: 2.108394\n",
            "Train Epoch: 2 | Batch Status: 12800/50000 (26%) | Loss: 2.402640\n",
            "Train Epoch: 2 | Batch Status: 13120/50000 (26%) | Loss: 2.421456\n",
            "Train Epoch: 2 | Batch Status: 13440/50000 (27%) | Loss: 2.704403\n",
            "Train Epoch: 2 | Batch Status: 13760/50000 (28%) | Loss: 2.524031\n",
            "Train Epoch: 2 | Batch Status: 14080/50000 (28%) | Loss: 3.175576\n",
            "Train Epoch: 2 | Batch Status: 14400/50000 (29%) | Loss: 2.324474\n",
            "Train Epoch: 2 | Batch Status: 14720/50000 (29%) | Loss: 3.090468\n",
            "Train Epoch: 2 | Batch Status: 15040/50000 (30%) | Loss: 2.605753\n",
            "Train Epoch: 2 | Batch Status: 15360/50000 (31%) | Loss: 2.305802\n",
            "Train Epoch: 2 | Batch Status: 15680/50000 (31%) | Loss: 2.519146\n",
            "Train Epoch: 2 | Batch Status: 16000/50000 (32%) | Loss: 2.371840\n",
            "Train Epoch: 2 | Batch Status: 16320/50000 (33%) | Loss: 2.225164\n",
            "Train Epoch: 2 | Batch Status: 16640/50000 (33%) | Loss: 2.239218\n",
            "Train Epoch: 2 | Batch Status: 16960/50000 (34%) | Loss: 2.328825\n",
            "Train Epoch: 2 | Batch Status: 17280/50000 (35%) | Loss: 2.224910\n",
            "Train Epoch: 2 | Batch Status: 17600/50000 (35%) | Loss: 2.732185\n",
            "Train Epoch: 2 | Batch Status: 17920/50000 (36%) | Loss: 3.015189\n",
            "Train Epoch: 2 | Batch Status: 18240/50000 (36%) | Loss: 2.956518\n",
            "Train Epoch: 2 | Batch Status: 18560/50000 (37%) | Loss: 2.522471\n",
            "Train Epoch: 2 | Batch Status: 18880/50000 (38%) | Loss: 2.325507\n",
            "Train Epoch: 2 | Batch Status: 19200/50000 (38%) | Loss: 2.779196\n",
            "Train Epoch: 2 | Batch Status: 19520/50000 (39%) | Loss: 2.829185\n",
            "Train Epoch: 2 | Batch Status: 19840/50000 (40%) | Loss: 2.131270\n",
            "Train Epoch: 2 | Batch Status: 20160/50000 (40%) | Loss: 2.971972\n",
            "Train Epoch: 2 | Batch Status: 20480/50000 (41%) | Loss: 2.523236\n",
            "Train Epoch: 2 | Batch Status: 20800/50000 (42%) | Loss: 2.424872\n",
            "Train Epoch: 2 | Batch Status: 21120/50000 (42%) | Loss: 2.440245\n",
            "Train Epoch: 2 | Batch Status: 21440/50000 (43%) | Loss: 2.326042\n",
            "Train Epoch: 2 | Batch Status: 21760/50000 (44%) | Loss: 2.323954\n",
            "Train Epoch: 2 | Batch Status: 22080/50000 (44%) | Loss: 2.192979\n",
            "Train Epoch: 2 | Batch Status: 22400/50000 (45%) | Loss: 2.122229\n",
            "Train Epoch: 2 | Batch Status: 22720/50000 (45%) | Loss: 2.383368\n",
            "Train Epoch: 2 | Batch Status: 23040/50000 (46%) | Loss: 2.565350\n",
            "Train Epoch: 2 | Batch Status: 23360/50000 (47%) | Loss: 2.581049\n",
            "Train Epoch: 2 | Batch Status: 23680/50000 (47%) | Loss: 2.502161\n",
            "Train Epoch: 2 | Batch Status: 24000/50000 (48%) | Loss: 2.615580\n",
            "Train Epoch: 2 | Batch Status: 24320/50000 (49%) | Loss: 2.992659\n",
            "Train Epoch: 2 | Batch Status: 24640/50000 (49%) | Loss: 2.449270\n",
            "Train Epoch: 2 | Batch Status: 24960/50000 (50%) | Loss: 2.327957\n",
            "Train Epoch: 2 | Batch Status: 25280/50000 (51%) | Loss: 1.886147\n",
            "Train Epoch: 2 | Batch Status: 25600/50000 (51%) | Loss: 2.716353\n",
            "Train Epoch: 2 | Batch Status: 25920/50000 (52%) | Loss: 2.444243\n",
            "Train Epoch: 2 | Batch Status: 26240/50000 (52%) | Loss: 2.689448\n",
            "Train Epoch: 2 | Batch Status: 26560/50000 (53%) | Loss: 2.786388\n",
            "Train Epoch: 2 | Batch Status: 26880/50000 (54%) | Loss: 2.334238\n",
            "Train Epoch: 2 | Batch Status: 27200/50000 (54%) | Loss: 2.350975\n",
            "Train Epoch: 2 | Batch Status: 27520/50000 (55%) | Loss: 2.398614\n",
            "Train Epoch: 2 | Batch Status: 27840/50000 (56%) | Loss: 2.609804\n",
            "Train Epoch: 2 | Batch Status: 28160/50000 (56%) | Loss: 2.961101\n",
            "Train Epoch: 2 | Batch Status: 28480/50000 (57%) | Loss: 2.423265\n",
            "Train Epoch: 2 | Batch Status: 28800/50000 (58%) | Loss: 2.602455\n",
            "Train Epoch: 2 | Batch Status: 29120/50000 (58%) | Loss: 2.325077\n",
            "Train Epoch: 2 | Batch Status: 29440/50000 (59%) | Loss: 2.585945\n",
            "Train Epoch: 2 | Batch Status: 29760/50000 (60%) | Loss: 2.283833\n",
            "Train Epoch: 2 | Batch Status: 30080/50000 (60%) | Loss: 2.616873\n",
            "Train Epoch: 2 | Batch Status: 30400/50000 (61%) | Loss: 1.969489\n",
            "Train Epoch: 2 | Batch Status: 30720/50000 (61%) | Loss: 2.223868\n",
            "Train Epoch: 2 | Batch Status: 31040/50000 (62%) | Loss: 2.459848\n",
            "Train Epoch: 2 | Batch Status: 31360/50000 (63%) | Loss: 2.033224\n",
            "Train Epoch: 2 | Batch Status: 31680/50000 (63%) | Loss: 2.732033\n",
            "Train Epoch: 2 | Batch Status: 32000/50000 (64%) | Loss: 2.804552\n",
            "Train Epoch: 2 | Batch Status: 32320/50000 (65%) | Loss: 1.842987\n",
            "Train Epoch: 2 | Batch Status: 32640/50000 (65%) | Loss: 1.982531\n",
            "Train Epoch: 2 | Batch Status: 32960/50000 (66%) | Loss: 2.284255\n",
            "Train Epoch: 2 | Batch Status: 33280/50000 (67%) | Loss: 3.098113\n",
            "Train Epoch: 2 | Batch Status: 33600/50000 (67%) | Loss: 2.029850\n",
            "Train Epoch: 2 | Batch Status: 33920/50000 (68%) | Loss: 2.318876\n",
            "Train Epoch: 2 | Batch Status: 34240/50000 (68%) | Loss: 2.578674\n",
            "Train Epoch: 2 | Batch Status: 34560/50000 (69%) | Loss: 2.549311\n",
            "Train Epoch: 2 | Batch Status: 34880/50000 (70%) | Loss: 2.075849\n",
            "Train Epoch: 2 | Batch Status: 35200/50000 (70%) | Loss: 2.452127\n",
            "Train Epoch: 2 | Batch Status: 35520/50000 (71%) | Loss: 2.157357\n",
            "Train Epoch: 2 | Batch Status: 35840/50000 (72%) | Loss: 2.297014\n",
            "Train Epoch: 2 | Batch Status: 36160/50000 (72%) | Loss: 2.575976\n",
            "Train Epoch: 2 | Batch Status: 36480/50000 (73%) | Loss: 2.392450\n",
            "Train Epoch: 2 | Batch Status: 36800/50000 (74%) | Loss: 2.566482\n",
            "Train Epoch: 2 | Batch Status: 37120/50000 (74%) | Loss: 2.753955\n",
            "Train Epoch: 2 | Batch Status: 37440/50000 (75%) | Loss: 1.608784\n",
            "Train Epoch: 2 | Batch Status: 37760/50000 (75%) | Loss: 2.337302\n",
            "Train Epoch: 2 | Batch Status: 38080/50000 (76%) | Loss: 2.723417\n",
            "Train Epoch: 2 | Batch Status: 38400/50000 (77%) | Loss: 1.966079\n",
            "Train Epoch: 2 | Batch Status: 38720/50000 (77%) | Loss: 2.493692\n",
            "Train Epoch: 2 | Batch Status: 39040/50000 (78%) | Loss: 2.266899\n",
            "Train Epoch: 2 | Batch Status: 39360/50000 (79%) | Loss: 2.067675\n",
            "Train Epoch: 2 | Batch Status: 39680/50000 (79%) | Loss: 2.207339\n",
            "Train Epoch: 2 | Batch Status: 40000/50000 (80%) | Loss: 2.211524\n",
            "Train Epoch: 2 | Batch Status: 40320/50000 (81%) | Loss: 2.112694\n",
            "Train Epoch: 2 | Batch Status: 40640/50000 (81%) | Loss: 2.368341\n",
            "Train Epoch: 2 | Batch Status: 40960/50000 (82%) | Loss: 2.280551\n",
            "Train Epoch: 2 | Batch Status: 41280/50000 (83%) | Loss: 2.226502\n",
            "Train Epoch: 2 | Batch Status: 41600/50000 (83%) | Loss: 2.522447\n",
            "Train Epoch: 2 | Batch Status: 41920/50000 (84%) | Loss: 2.664718\n",
            "Train Epoch: 2 | Batch Status: 42240/50000 (84%) | Loss: 2.309429\n",
            "Train Epoch: 2 | Batch Status: 42560/50000 (85%) | Loss: 2.373365\n",
            "Train Epoch: 2 | Batch Status: 42880/50000 (86%) | Loss: 2.091763\n",
            "Train Epoch: 2 | Batch Status: 43200/50000 (86%) | Loss: 2.234123\n",
            "Train Epoch: 2 | Batch Status: 43520/50000 (87%) | Loss: 2.170828\n",
            "Train Epoch: 2 | Batch Status: 43840/50000 (88%) | Loss: 2.090514\n",
            "Train Epoch: 2 | Batch Status: 44160/50000 (88%) | Loss: 2.987347\n",
            "Train Epoch: 2 | Batch Status: 44480/50000 (89%) | Loss: 2.694216\n",
            "Train Epoch: 2 | Batch Status: 44800/50000 (90%) | Loss: 2.268213\n",
            "Train Epoch: 2 | Batch Status: 45120/50000 (90%) | Loss: 2.295454\n",
            "Train Epoch: 2 | Batch Status: 45440/50000 (91%) | Loss: 2.199228\n",
            "Train Epoch: 2 | Batch Status: 45760/50000 (91%) | Loss: 1.845356\n",
            "Train Epoch: 2 | Batch Status: 46080/50000 (92%) | Loss: 1.952038\n",
            "Train Epoch: 2 | Batch Status: 46400/50000 (93%) | Loss: 2.562577\n",
            "Train Epoch: 2 | Batch Status: 46720/50000 (93%) | Loss: 2.564243\n",
            "Train Epoch: 2 | Batch Status: 47040/50000 (94%) | Loss: 1.880921\n",
            "Train Epoch: 2 | Batch Status: 47360/50000 (95%) | Loss: 2.258710\n",
            "Train Epoch: 2 | Batch Status: 47680/50000 (95%) | Loss: 2.352235\n",
            "Train Epoch: 2 | Batch Status: 48000/50000 (96%) | Loss: 2.210815\n",
            "Train Epoch: 2 | Batch Status: 48320/50000 (97%) | Loss: 2.314203\n",
            "Train Epoch: 2 | Batch Status: 48640/50000 (97%) | Loss: 2.297556\n",
            "Train Epoch: 2 | Batch Status: 48960/50000 (98%) | Loss: 2.283577\n",
            "Train Epoch: 2 | Batch Status: 49280/50000 (99%) | Loss: 2.586709\n",
            "Train Epoch: 2 | Batch Status: 49600/50000 (99%) | Loss: 2.082058\n",
            "Train Epoch: 2 | Batch Status: 49920/50000 (100%) | Loss: 1.832272\n",
            "Training time: 2m 38s\n",
            "Testing time: 2m 50s\n",
            "Train Epoch: 3 | Batch Status: 0/50000 (0%) | Loss: 1.685257\n",
            "Train Epoch: 3 | Batch Status: 320/50000 (1%) | Loss: 2.197012\n",
            "Train Epoch: 3 | Batch Status: 640/50000 (1%) | Loss: 1.310120\n",
            "Train Epoch: 3 | Batch Status: 960/50000 (2%) | Loss: 2.091842\n",
            "Train Epoch: 3 | Batch Status: 1280/50000 (3%) | Loss: 1.891030\n",
            "Train Epoch: 3 | Batch Status: 1600/50000 (3%) | Loss: 1.584846\n",
            "Train Epoch: 3 | Batch Status: 1920/50000 (4%) | Loss: 2.355278\n",
            "Train Epoch: 3 | Batch Status: 2240/50000 (4%) | Loss: 2.259982\n",
            "Train Epoch: 3 | Batch Status: 2560/50000 (5%) | Loss: 1.858836\n",
            "Train Epoch: 3 | Batch Status: 2880/50000 (6%) | Loss: 2.059206\n",
            "Train Epoch: 3 | Batch Status: 3200/50000 (6%) | Loss: 1.938991\n",
            "Train Epoch: 3 | Batch Status: 3520/50000 (7%) | Loss: 2.151703\n",
            "Train Epoch: 3 | Batch Status: 3840/50000 (8%) | Loss: 2.533009\n",
            "Train Epoch: 3 | Batch Status: 4160/50000 (8%) | Loss: 2.250674\n",
            "Train Epoch: 3 | Batch Status: 4480/50000 (9%) | Loss: 1.770371\n",
            "Train Epoch: 3 | Batch Status: 4800/50000 (10%) | Loss: 1.820269\n",
            "Train Epoch: 3 | Batch Status: 5120/50000 (10%) | Loss: 1.792021\n",
            "Train Epoch: 3 | Batch Status: 5440/50000 (11%) | Loss: 2.309307\n",
            "Train Epoch: 3 | Batch Status: 5760/50000 (12%) | Loss: 1.532439\n",
            "Train Epoch: 3 | Batch Status: 6080/50000 (12%) | Loss: 1.722873\n",
            "Train Epoch: 3 | Batch Status: 6400/50000 (13%) | Loss: 1.637082\n",
            "Train Epoch: 3 | Batch Status: 6720/50000 (13%) | Loss: 1.959746\n",
            "Train Epoch: 3 | Batch Status: 7040/50000 (14%) | Loss: 1.637621\n",
            "Train Epoch: 3 | Batch Status: 7360/50000 (15%) | Loss: 2.226492\n",
            "Train Epoch: 3 | Batch Status: 7680/50000 (15%) | Loss: 1.969279\n",
            "Train Epoch: 3 | Batch Status: 8000/50000 (16%) | Loss: 2.000569\n",
            "Train Epoch: 3 | Batch Status: 8320/50000 (17%) | Loss: 2.016047\n",
            "Train Epoch: 3 | Batch Status: 8640/50000 (17%) | Loss: 1.924571\n",
            "Train Epoch: 3 | Batch Status: 8960/50000 (18%) | Loss: 2.076754\n",
            "Train Epoch: 3 | Batch Status: 9280/50000 (19%) | Loss: 1.830224\n",
            "Train Epoch: 3 | Batch Status: 9600/50000 (19%) | Loss: 1.934518\n",
            "Train Epoch: 3 | Batch Status: 9920/50000 (20%) | Loss: 2.196665\n",
            "Train Epoch: 3 | Batch Status: 10240/50000 (20%) | Loss: 1.987594\n",
            "Train Epoch: 3 | Batch Status: 10560/50000 (21%) | Loss: 1.736026\n",
            "Train Epoch: 3 | Batch Status: 10880/50000 (22%) | Loss: 1.651607\n",
            "Train Epoch: 3 | Batch Status: 11200/50000 (22%) | Loss: 1.698113\n",
            "Train Epoch: 3 | Batch Status: 11520/50000 (23%) | Loss: 1.850450\n",
            "Train Epoch: 3 | Batch Status: 11840/50000 (24%) | Loss: 1.784674\n",
            "Train Epoch: 3 | Batch Status: 12160/50000 (24%) | Loss: 1.484881\n",
            "Train Epoch: 3 | Batch Status: 12480/50000 (25%) | Loss: 2.017467\n",
            "Train Epoch: 3 | Batch Status: 12800/50000 (26%) | Loss: 1.973758\n",
            "Train Epoch: 3 | Batch Status: 13120/50000 (26%) | Loss: 2.170476\n",
            "Train Epoch: 3 | Batch Status: 13440/50000 (27%) | Loss: 2.621844\n",
            "Train Epoch: 3 | Batch Status: 13760/50000 (28%) | Loss: 1.980052\n",
            "Train Epoch: 3 | Batch Status: 14080/50000 (28%) | Loss: 2.208303\n",
            "Train Epoch: 3 | Batch Status: 14400/50000 (29%) | Loss: 2.064783\n",
            "Train Epoch: 3 | Batch Status: 14720/50000 (29%) | Loss: 1.583256\n",
            "Train Epoch: 3 | Batch Status: 15040/50000 (30%) | Loss: 1.577511\n",
            "Train Epoch: 3 | Batch Status: 15360/50000 (31%) | Loss: 2.055762\n",
            "Train Epoch: 3 | Batch Status: 15680/50000 (31%) | Loss: 2.076783\n",
            "Train Epoch: 3 | Batch Status: 16000/50000 (32%) | Loss: 1.970172\n",
            "Train Epoch: 3 | Batch Status: 16320/50000 (33%) | Loss: 2.213237\n",
            "Train Epoch: 3 | Batch Status: 16640/50000 (33%) | Loss: 1.850701\n",
            "Train Epoch: 3 | Batch Status: 16960/50000 (34%) | Loss: 2.069885\n",
            "Train Epoch: 3 | Batch Status: 17280/50000 (35%) | Loss: 1.559146\n",
            "Train Epoch: 3 | Batch Status: 17600/50000 (35%) | Loss: 1.758759\n",
            "Train Epoch: 3 | Batch Status: 17920/50000 (36%) | Loss: 2.148268\n",
            "Train Epoch: 3 | Batch Status: 18240/50000 (36%) | Loss: 2.150154\n",
            "Train Epoch: 3 | Batch Status: 18560/50000 (37%) | Loss: 1.884526\n",
            "Train Epoch: 3 | Batch Status: 18880/50000 (38%) | Loss: 2.248794\n",
            "Train Epoch: 3 | Batch Status: 19200/50000 (38%) | Loss: 2.244271\n",
            "Train Epoch: 3 | Batch Status: 19520/50000 (39%) | Loss: 1.960769\n",
            "Train Epoch: 3 | Batch Status: 19840/50000 (40%) | Loss: 2.068612\n",
            "Train Epoch: 3 | Batch Status: 20160/50000 (40%) | Loss: 1.711003\n",
            "Train Epoch: 3 | Batch Status: 20480/50000 (41%) | Loss: 1.744555\n",
            "Train Epoch: 3 | Batch Status: 20800/50000 (42%) | Loss: 1.995646\n",
            "Train Epoch: 3 | Batch Status: 21120/50000 (42%) | Loss: 1.945289\n",
            "Train Epoch: 3 | Batch Status: 21440/50000 (43%) | Loss: 2.219251\n",
            "Train Epoch: 3 | Batch Status: 21760/50000 (44%) | Loss: 2.129929\n",
            "Train Epoch: 3 | Batch Status: 22080/50000 (44%) | Loss: 1.588807\n",
            "Train Epoch: 3 | Batch Status: 22400/50000 (45%) | Loss: 1.860676\n",
            "Train Epoch: 3 | Batch Status: 22720/50000 (45%) | Loss: 1.864379\n",
            "Train Epoch: 3 | Batch Status: 23040/50000 (46%) | Loss: 1.694860\n",
            "Train Epoch: 3 | Batch Status: 23360/50000 (47%) | Loss: 2.054729\n",
            "Train Epoch: 3 | Batch Status: 23680/50000 (47%) | Loss: 2.167275\n",
            "Train Epoch: 3 | Batch Status: 24000/50000 (48%) | Loss: 2.417429\n",
            "Train Epoch: 3 | Batch Status: 24320/50000 (49%) | Loss: 1.732179\n",
            "Train Epoch: 3 | Batch Status: 24640/50000 (49%) | Loss: 1.632953\n",
            "Train Epoch: 3 | Batch Status: 24960/50000 (50%) | Loss: 1.299535\n",
            "Train Epoch: 3 | Batch Status: 25280/50000 (51%) | Loss: 2.278016\n",
            "Train Epoch: 3 | Batch Status: 25600/50000 (51%) | Loss: 1.936350\n",
            "Train Epoch: 3 | Batch Status: 25920/50000 (52%) | Loss: 1.740853\n",
            "Train Epoch: 3 | Batch Status: 26240/50000 (52%) | Loss: 2.297546\n",
            "Train Epoch: 3 | Batch Status: 26560/50000 (53%) | Loss: 2.291695\n",
            "Train Epoch: 3 | Batch Status: 26880/50000 (54%) | Loss: 1.803423\n",
            "Train Epoch: 3 | Batch Status: 27200/50000 (54%) | Loss: 2.094838\n",
            "Train Epoch: 3 | Batch Status: 27520/50000 (55%) | Loss: 2.070590\n",
            "Train Epoch: 3 | Batch Status: 27840/50000 (56%) | Loss: 2.020155\n",
            "Train Epoch: 3 | Batch Status: 28160/50000 (56%) | Loss: 1.605145\n",
            "Train Epoch: 3 | Batch Status: 28480/50000 (57%) | Loss: 2.477319\n",
            "Train Epoch: 3 | Batch Status: 28800/50000 (58%) | Loss: 2.102568\n",
            "Train Epoch: 3 | Batch Status: 29120/50000 (58%) | Loss: 2.450050\n",
            "Train Epoch: 3 | Batch Status: 29440/50000 (59%) | Loss: 1.981970\n",
            "Train Epoch: 3 | Batch Status: 29760/50000 (60%) | Loss: 1.831197\n",
            "Train Epoch: 3 | Batch Status: 30080/50000 (60%) | Loss: 1.348300\n",
            "Train Epoch: 3 | Batch Status: 30400/50000 (61%) | Loss: 2.048632\n",
            "Train Epoch: 3 | Batch Status: 30720/50000 (61%) | Loss: 1.788527\n",
            "Train Epoch: 3 | Batch Status: 31040/50000 (62%) | Loss: 2.106616\n",
            "Train Epoch: 3 | Batch Status: 31360/50000 (63%) | Loss: 1.564572\n",
            "Train Epoch: 3 | Batch Status: 31680/50000 (63%) | Loss: 2.190512\n",
            "Train Epoch: 3 | Batch Status: 32000/50000 (64%) | Loss: 1.681115\n",
            "Train Epoch: 3 | Batch Status: 32320/50000 (65%) | Loss: 2.296255\n",
            "Train Epoch: 3 | Batch Status: 32640/50000 (65%) | Loss: 1.818116\n",
            "Train Epoch: 3 | Batch Status: 32960/50000 (66%) | Loss: 1.580911\n",
            "Train Epoch: 3 | Batch Status: 33280/50000 (67%) | Loss: 2.067338\n",
            "Train Epoch: 3 | Batch Status: 33600/50000 (67%) | Loss: 1.981613\n",
            "Train Epoch: 3 | Batch Status: 33920/50000 (68%) | Loss: 1.845390\n",
            "Train Epoch: 3 | Batch Status: 34240/50000 (68%) | Loss: 1.750856\n",
            "Train Epoch: 3 | Batch Status: 34560/50000 (69%) | Loss: 2.039174\n",
            "Train Epoch: 3 | Batch Status: 34880/50000 (70%) | Loss: 2.233610\n",
            "Train Epoch: 3 | Batch Status: 35200/50000 (70%) | Loss: 2.217250\n",
            "Train Epoch: 3 | Batch Status: 35520/50000 (71%) | Loss: 2.207829\n",
            "Train Epoch: 3 | Batch Status: 35840/50000 (72%) | Loss: 2.122324\n",
            "Train Epoch: 3 | Batch Status: 36160/50000 (72%) | Loss: 1.929589\n",
            "Train Epoch: 3 | Batch Status: 36480/50000 (73%) | Loss: 1.487218\n",
            "Train Epoch: 3 | Batch Status: 36800/50000 (74%) | Loss: 2.501313\n",
            "Train Epoch: 3 | Batch Status: 37120/50000 (74%) | Loss: 2.013118\n",
            "Train Epoch: 3 | Batch Status: 37440/50000 (75%) | Loss: 1.999188\n",
            "Train Epoch: 3 | Batch Status: 37760/50000 (75%) | Loss: 2.224615\n",
            "Train Epoch: 3 | Batch Status: 38080/50000 (76%) | Loss: 1.678479\n",
            "Train Epoch: 3 | Batch Status: 38400/50000 (77%) | Loss: 1.627296\n",
            "Train Epoch: 3 | Batch Status: 38720/50000 (77%) | Loss: 1.575985\n",
            "Train Epoch: 3 | Batch Status: 39040/50000 (78%) | Loss: 1.519169\n",
            "Train Epoch: 3 | Batch Status: 39360/50000 (79%) | Loss: 1.658276\n",
            "Train Epoch: 3 | Batch Status: 39680/50000 (79%) | Loss: 1.964763\n",
            "Train Epoch: 3 | Batch Status: 40000/50000 (80%) | Loss: 1.889211\n",
            "Train Epoch: 3 | Batch Status: 40320/50000 (81%) | Loss: 1.868346\n",
            "Train Epoch: 3 | Batch Status: 40640/50000 (81%) | Loss: 1.919171\n",
            "Train Epoch: 3 | Batch Status: 40960/50000 (82%) | Loss: 1.985610\n",
            "Train Epoch: 3 | Batch Status: 41280/50000 (83%) | Loss: 2.160725\n",
            "Train Epoch: 3 | Batch Status: 41600/50000 (83%) | Loss: 2.227606\n",
            "Train Epoch: 3 | Batch Status: 41920/50000 (84%) | Loss: 1.655142\n",
            "Train Epoch: 3 | Batch Status: 42240/50000 (84%) | Loss: 2.093151\n",
            "Train Epoch: 3 | Batch Status: 42560/50000 (85%) | Loss: 1.955719\n",
            "Train Epoch: 3 | Batch Status: 42880/50000 (86%) | Loss: 1.670404\n",
            "Train Epoch: 3 | Batch Status: 43200/50000 (86%) | Loss: 2.129882\n",
            "Train Epoch: 3 | Batch Status: 43520/50000 (87%) | Loss: 1.471993\n",
            "Train Epoch: 3 | Batch Status: 43840/50000 (88%) | Loss: 1.670574\n",
            "Train Epoch: 3 | Batch Status: 44160/50000 (88%) | Loss: 2.327506\n",
            "Train Epoch: 3 | Batch Status: 44480/50000 (89%) | Loss: 1.424466\n",
            "Train Epoch: 3 | Batch Status: 44800/50000 (90%) | Loss: 2.207716\n",
            "Train Epoch: 3 | Batch Status: 45120/50000 (90%) | Loss: 2.256929\n",
            "Train Epoch: 3 | Batch Status: 45440/50000 (91%) | Loss: 1.285399\n",
            "Train Epoch: 3 | Batch Status: 45760/50000 (91%) | Loss: 1.900237\n",
            "Train Epoch: 3 | Batch Status: 46080/50000 (92%) | Loss: 2.314570\n",
            "Train Epoch: 3 | Batch Status: 46400/50000 (93%) | Loss: 2.086833\n",
            "Train Epoch: 3 | Batch Status: 46720/50000 (93%) | Loss: 1.518202\n",
            "Train Epoch: 3 | Batch Status: 47040/50000 (94%) | Loss: 1.979530\n",
            "Train Epoch: 3 | Batch Status: 47360/50000 (95%) | Loss: 1.453034\n",
            "Train Epoch: 3 | Batch Status: 47680/50000 (95%) | Loss: 1.917745\n",
            "Train Epoch: 3 | Batch Status: 48000/50000 (96%) | Loss: 2.189517\n",
            "Train Epoch: 3 | Batch Status: 48320/50000 (97%) | Loss: 1.952102\n",
            "Train Epoch: 3 | Batch Status: 48640/50000 (97%) | Loss: 1.594804\n",
            "Train Epoch: 3 | Batch Status: 48960/50000 (98%) | Loss: 2.005183\n",
            "Train Epoch: 3 | Batch Status: 49280/50000 (99%) | Loss: 2.347447\n",
            "Train Epoch: 3 | Batch Status: 49600/50000 (99%) | Loss: 1.774575\n",
            "Train Epoch: 3 | Batch Status: 49920/50000 (100%) | Loss: 1.733553\n",
            "Training time: 2m 38s\n",
            "Testing time: 2m 50s\n",
            "Train Epoch: 4 | Batch Status: 0/50000 (0%) | Loss: 1.598081\n",
            "Train Epoch: 4 | Batch Status: 320/50000 (1%) | Loss: 1.626931\n",
            "Train Epoch: 4 | Batch Status: 640/50000 (1%) | Loss: 1.794193\n",
            "Train Epoch: 4 | Batch Status: 960/50000 (2%) | Loss: 1.344066\n",
            "Train Epoch: 4 | Batch Status: 1280/50000 (3%) | Loss: 1.561550\n",
            "Train Epoch: 4 | Batch Status: 1600/50000 (3%) | Loss: 1.357443\n",
            "Train Epoch: 4 | Batch Status: 1920/50000 (4%) | Loss: 1.779773\n",
            "Train Epoch: 4 | Batch Status: 2240/50000 (4%) | Loss: 1.267775\n",
            "Train Epoch: 4 | Batch Status: 2560/50000 (5%) | Loss: 1.437402\n",
            "Train Epoch: 4 | Batch Status: 2880/50000 (6%) | Loss: 1.621213\n",
            "Train Epoch: 4 | Batch Status: 3200/50000 (6%) | Loss: 1.233093\n",
            "Train Epoch: 4 | Batch Status: 3520/50000 (7%) | Loss: 1.767869\n",
            "Train Epoch: 4 | Batch Status: 3840/50000 (8%) | Loss: 1.189186\n",
            "Train Epoch: 4 | Batch Status: 4160/50000 (8%) | Loss: 1.491792\n",
            "Train Epoch: 4 | Batch Status: 4480/50000 (9%) | Loss: 1.696954\n",
            "Train Epoch: 4 | Batch Status: 4800/50000 (10%) | Loss: 1.467209\n",
            "Train Epoch: 4 | Batch Status: 5120/50000 (10%) | Loss: 1.539163\n",
            "Train Epoch: 4 | Batch Status: 5440/50000 (11%) | Loss: 1.350205\n",
            "Train Epoch: 4 | Batch Status: 5760/50000 (12%) | Loss: 1.378999\n",
            "Train Epoch: 4 | Batch Status: 6080/50000 (12%) | Loss: 1.054231\n",
            "Train Epoch: 4 | Batch Status: 6400/50000 (13%) | Loss: 1.302264\n",
            "Train Epoch: 4 | Batch Status: 6720/50000 (13%) | Loss: 1.424631\n",
            "Train Epoch: 4 | Batch Status: 7040/50000 (14%) | Loss: 2.311498\n",
            "Train Epoch: 4 | Batch Status: 7360/50000 (15%) | Loss: 2.004704\n",
            "Train Epoch: 4 | Batch Status: 7680/50000 (15%) | Loss: 1.779231\n",
            "Train Epoch: 4 | Batch Status: 8000/50000 (16%) | Loss: 1.078750\n",
            "Train Epoch: 4 | Batch Status: 8320/50000 (17%) | Loss: 1.636325\n",
            "Train Epoch: 4 | Batch Status: 8640/50000 (17%) | Loss: 1.815835\n",
            "Train Epoch: 4 | Batch Status: 8960/50000 (18%) | Loss: 1.590793\n",
            "Train Epoch: 4 | Batch Status: 9280/50000 (19%) | Loss: 1.194065\n",
            "Train Epoch: 4 | Batch Status: 9600/50000 (19%) | Loss: 1.552640\n",
            "Train Epoch: 4 | Batch Status: 9920/50000 (20%) | Loss: 1.264764\n",
            "Train Epoch: 4 | Batch Status: 10240/50000 (20%) | Loss: 1.331401\n",
            "Train Epoch: 4 | Batch Status: 10560/50000 (21%) | Loss: 1.445468\n",
            "Train Epoch: 4 | Batch Status: 10880/50000 (22%) | Loss: 1.728412\n",
            "Train Epoch: 4 | Batch Status: 11200/50000 (22%) | Loss: 1.730591\n",
            "Train Epoch: 4 | Batch Status: 11520/50000 (23%) | Loss: 1.656915\n",
            "Train Epoch: 4 | Batch Status: 11840/50000 (24%) | Loss: 1.693483\n",
            "Train Epoch: 4 | Batch Status: 12160/50000 (24%) | Loss: 1.385138\n",
            "Train Epoch: 4 | Batch Status: 12480/50000 (25%) | Loss: 1.259689\n",
            "Train Epoch: 4 | Batch Status: 12800/50000 (26%) | Loss: 1.600437\n",
            "Train Epoch: 4 | Batch Status: 13120/50000 (26%) | Loss: 1.352222\n",
            "Train Epoch: 4 | Batch Status: 13440/50000 (27%) | Loss: 1.636736\n",
            "Train Epoch: 4 | Batch Status: 13760/50000 (28%) | Loss: 1.649773\n",
            "Train Epoch: 4 | Batch Status: 14080/50000 (28%) | Loss: 0.933609\n",
            "Train Epoch: 4 | Batch Status: 14400/50000 (29%) | Loss: 1.340193\n",
            "Train Epoch: 4 | Batch Status: 14720/50000 (29%) | Loss: 1.552462\n",
            "Train Epoch: 4 | Batch Status: 15040/50000 (30%) | Loss: 1.542558\n",
            "Train Epoch: 4 | Batch Status: 15360/50000 (31%) | Loss: 1.404041\n",
            "Train Epoch: 4 | Batch Status: 15680/50000 (31%) | Loss: 1.529231\n",
            "Train Epoch: 4 | Batch Status: 16000/50000 (32%) | Loss: 1.134587\n",
            "Train Epoch: 4 | Batch Status: 16320/50000 (33%) | Loss: 1.179863\n",
            "Train Epoch: 4 | Batch Status: 16640/50000 (33%) | Loss: 1.371732\n",
            "Train Epoch: 4 | Batch Status: 16960/50000 (34%) | Loss: 1.487759\n",
            "Train Epoch: 4 | Batch Status: 17280/50000 (35%) | Loss: 1.569923\n",
            "Train Epoch: 4 | Batch Status: 17600/50000 (35%) | Loss: 1.733359\n",
            "Train Epoch: 4 | Batch Status: 17920/50000 (36%) | Loss: 1.239683\n",
            "Train Epoch: 4 | Batch Status: 18240/50000 (36%) | Loss: 1.530545\n",
            "Train Epoch: 4 | Batch Status: 18560/50000 (37%) | Loss: 1.825032\n",
            "Train Epoch: 4 | Batch Status: 18880/50000 (38%) | Loss: 1.195433\n",
            "Train Epoch: 4 | Batch Status: 19200/50000 (38%) | Loss: 1.298582\n",
            "Train Epoch: 4 | Batch Status: 19520/50000 (39%) | Loss: 1.715102\n",
            "Train Epoch: 4 | Batch Status: 19840/50000 (40%) | Loss: 1.975688\n",
            "Train Epoch: 4 | Batch Status: 20160/50000 (40%) | Loss: 1.574301\n",
            "Train Epoch: 4 | Batch Status: 20480/50000 (41%) | Loss: 1.495499\n",
            "Train Epoch: 4 | Batch Status: 20800/50000 (42%) | Loss: 1.212114\n",
            "Train Epoch: 4 | Batch Status: 21120/50000 (42%) | Loss: 1.757835\n",
            "Train Epoch: 4 | Batch Status: 21440/50000 (43%) | Loss: 1.229692\n",
            "Train Epoch: 4 | Batch Status: 21760/50000 (44%) | Loss: 1.292182\n",
            "Train Epoch: 4 | Batch Status: 22080/50000 (44%) | Loss: 1.296485\n",
            "Train Epoch: 4 | Batch Status: 22400/50000 (45%) | Loss: 1.435709\n",
            "Train Epoch: 4 | Batch Status: 22720/50000 (45%) | Loss: 1.326907\n",
            "Train Epoch: 4 | Batch Status: 23040/50000 (46%) | Loss: 1.276848\n",
            "Train Epoch: 4 | Batch Status: 23360/50000 (47%) | Loss: 1.911320\n",
            "Train Epoch: 4 | Batch Status: 23680/50000 (47%) | Loss: 1.742081\n",
            "Train Epoch: 4 | Batch Status: 24000/50000 (48%) | Loss: 1.194389\n",
            "Train Epoch: 4 | Batch Status: 24320/50000 (49%) | Loss: 1.347669\n",
            "Train Epoch: 4 | Batch Status: 24640/50000 (49%) | Loss: 1.711100\n",
            "Train Epoch: 4 | Batch Status: 24960/50000 (50%) | Loss: 1.380226\n",
            "Train Epoch: 4 | Batch Status: 25280/50000 (51%) | Loss: 1.374518\n",
            "Train Epoch: 4 | Batch Status: 25600/50000 (51%) | Loss: 2.005381\n",
            "Train Epoch: 4 | Batch Status: 25920/50000 (52%) | Loss: 1.537681\n",
            "Train Epoch: 4 | Batch Status: 26240/50000 (52%) | Loss: 1.969402\n",
            "Train Epoch: 4 | Batch Status: 26560/50000 (53%) | Loss: 1.394009\n",
            "Train Epoch: 4 | Batch Status: 26880/50000 (54%) | Loss: 1.172271\n",
            "Train Epoch: 4 | Batch Status: 27200/50000 (54%) | Loss: 1.383272\n",
            "Train Epoch: 4 | Batch Status: 27520/50000 (55%) | Loss: 1.248262\n",
            "Train Epoch: 4 | Batch Status: 27840/50000 (56%) | Loss: 1.702956\n",
            "Train Epoch: 4 | Batch Status: 28160/50000 (56%) | Loss: 1.467119\n",
            "Train Epoch: 4 | Batch Status: 28480/50000 (57%) | Loss: 1.520741\n",
            "Train Epoch: 4 | Batch Status: 28800/50000 (58%) | Loss: 1.673046\n",
            "Train Epoch: 4 | Batch Status: 29120/50000 (58%) | Loss: 1.307325\n",
            "Train Epoch: 4 | Batch Status: 29440/50000 (59%) | Loss: 1.508085\n",
            "Train Epoch: 4 | Batch Status: 29760/50000 (60%) | Loss: 1.437799\n",
            "Train Epoch: 4 | Batch Status: 30080/50000 (60%) | Loss: 1.546263\n",
            "Train Epoch: 4 | Batch Status: 30400/50000 (61%) | Loss: 1.332846\n",
            "Train Epoch: 4 | Batch Status: 30720/50000 (61%) | Loss: 1.955366\n",
            "Train Epoch: 4 | Batch Status: 31040/50000 (62%) | Loss: 1.763389\n",
            "Train Epoch: 4 | Batch Status: 31360/50000 (63%) | Loss: 1.480077\n",
            "Train Epoch: 4 | Batch Status: 31680/50000 (63%) | Loss: 1.429249\n",
            "Train Epoch: 4 | Batch Status: 32000/50000 (64%) | Loss: 1.784223\n",
            "Train Epoch: 4 | Batch Status: 32320/50000 (65%) | Loss: 1.725682\n",
            "Train Epoch: 4 | Batch Status: 32640/50000 (65%) | Loss: 1.536737\n",
            "Train Epoch: 4 | Batch Status: 32960/50000 (66%) | Loss: 1.687192\n",
            "Train Epoch: 4 | Batch Status: 33280/50000 (67%) | Loss: 1.077533\n",
            "Train Epoch: 4 | Batch Status: 33600/50000 (67%) | Loss: 1.770142\n",
            "Train Epoch: 4 | Batch Status: 33920/50000 (68%) | Loss: 1.581558\n",
            "Train Epoch: 4 | Batch Status: 34240/50000 (68%) | Loss: 1.496762\n",
            "Train Epoch: 4 | Batch Status: 34560/50000 (69%) | Loss: 1.612397\n",
            "Train Epoch: 4 | Batch Status: 34880/50000 (70%) | Loss: 1.499884\n",
            "Train Epoch: 4 | Batch Status: 35200/50000 (70%) | Loss: 1.683510\n",
            "Train Epoch: 4 | Batch Status: 35520/50000 (71%) | Loss: 1.676528\n",
            "Train Epoch: 4 | Batch Status: 35840/50000 (72%) | Loss: 1.536597\n",
            "Train Epoch: 4 | Batch Status: 36160/50000 (72%) | Loss: 1.847806\n",
            "Train Epoch: 4 | Batch Status: 36480/50000 (73%) | Loss: 1.411711\n",
            "Train Epoch: 4 | Batch Status: 36800/50000 (74%) | Loss: 1.449371\n",
            "Train Epoch: 4 | Batch Status: 37120/50000 (74%) | Loss: 1.204532\n",
            "Train Epoch: 4 | Batch Status: 37440/50000 (75%) | Loss: 1.626387\n",
            "Train Epoch: 4 | Batch Status: 37760/50000 (75%) | Loss: 1.954101\n",
            "Train Epoch: 4 | Batch Status: 38080/50000 (76%) | Loss: 1.628412\n",
            "Train Epoch: 4 | Batch Status: 38400/50000 (77%) | Loss: 1.620584\n",
            "Train Epoch: 4 | Batch Status: 38720/50000 (77%) | Loss: 1.601733\n",
            "Train Epoch: 4 | Batch Status: 39040/50000 (78%) | Loss: 1.128617\n",
            "Train Epoch: 4 | Batch Status: 39360/50000 (79%) | Loss: 1.081871\n",
            "Train Epoch: 4 | Batch Status: 39680/50000 (79%) | Loss: 1.839052\n",
            "Train Epoch: 4 | Batch Status: 40000/50000 (80%) | Loss: 1.450841\n",
            "Train Epoch: 4 | Batch Status: 40320/50000 (81%) | Loss: 1.301058\n",
            "Train Epoch: 4 | Batch Status: 40640/50000 (81%) | Loss: 1.770712\n",
            "Train Epoch: 4 | Batch Status: 40960/50000 (82%) | Loss: 1.742134\n",
            "Train Epoch: 4 | Batch Status: 41280/50000 (83%) | Loss: 1.467111\n",
            "Train Epoch: 4 | Batch Status: 41600/50000 (83%) | Loss: 1.102698\n",
            "Train Epoch: 4 | Batch Status: 41920/50000 (84%) | Loss: 1.592169\n",
            "Train Epoch: 4 | Batch Status: 42240/50000 (84%) | Loss: 1.481735\n",
            "Train Epoch: 4 | Batch Status: 42560/50000 (85%) | Loss: 1.737294\n",
            "Train Epoch: 4 | Batch Status: 42880/50000 (86%) | Loss: 1.569889\n",
            "Train Epoch: 4 | Batch Status: 43200/50000 (86%) | Loss: 1.836671\n",
            "Train Epoch: 4 | Batch Status: 43520/50000 (87%) | Loss: 1.304231\n",
            "Train Epoch: 4 | Batch Status: 43840/50000 (88%) | Loss: 1.173211\n",
            "Train Epoch: 4 | Batch Status: 44160/50000 (88%) | Loss: 1.174023\n",
            "Train Epoch: 4 | Batch Status: 44480/50000 (89%) | Loss: 1.435909\n",
            "Train Epoch: 4 | Batch Status: 44800/50000 (90%) | Loss: 1.393635\n",
            "Train Epoch: 4 | Batch Status: 45120/50000 (90%) | Loss: 1.711828\n",
            "Train Epoch: 4 | Batch Status: 45440/50000 (91%) | Loss: 1.814596\n",
            "Train Epoch: 4 | Batch Status: 45760/50000 (91%) | Loss: 1.982412\n",
            "Train Epoch: 4 | Batch Status: 46080/50000 (92%) | Loss: 1.349558\n",
            "Train Epoch: 4 | Batch Status: 46400/50000 (93%) | Loss: 1.613207\n",
            "Train Epoch: 4 | Batch Status: 46720/50000 (93%) | Loss: 2.246968\n",
            "Train Epoch: 4 | Batch Status: 47040/50000 (94%) | Loss: 1.704135\n",
            "Train Epoch: 4 | Batch Status: 47360/50000 (95%) | Loss: 1.261464\n",
            "Train Epoch: 4 | Batch Status: 47680/50000 (95%) | Loss: 1.155320\n",
            "Train Epoch: 4 | Batch Status: 48000/50000 (96%) | Loss: 1.431943\n",
            "Train Epoch: 4 | Batch Status: 48320/50000 (97%) | Loss: 1.583773\n",
            "Train Epoch: 4 | Batch Status: 48640/50000 (97%) | Loss: 1.660020\n",
            "Train Epoch: 4 | Batch Status: 48960/50000 (98%) | Loss: 0.920149\n",
            "Train Epoch: 4 | Batch Status: 49280/50000 (99%) | Loss: 1.225629\n",
            "Train Epoch: 4 | Batch Status: 49600/50000 (99%) | Loss: 1.290794\n",
            "Train Epoch: 4 | Batch Status: 49920/50000 (100%) | Loss: 1.528386\n",
            "Training time: 2m 38s\n",
            "Testing time: 2m 49s\n",
            "Total Time: 11m 19s\n",
            "Model was trained on cuda!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. [Exercise 11-3] 아래 논문을 읽고, DenseNet을 구현한 다음 CIFAR-100에 대한 학습을 수행하시\n",
        "오. (layer의 개수는 본인이 적절히 설정할 것, 소스코드와 결과를 첨부할 것)"
      ],
      "metadata": {
        "id": "xhjhANyQHbGk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Bottleneck(nn.Module):\n",
        "    def __init__(self, in_channels, growth_rate):\n",
        "        super().__init__()\n",
        "        inner_channel = 4 * growth_rate\n",
        "        self.bottle_neck = nn.Sequential(\n",
        "            nn.BatchNorm2d(in_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(in_channels, inner_channel, kernel_size=1, bias=False),\n",
        "            nn.BatchNorm2d(inner_channel),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(inner_channel, growth_rate, kernel_size=3, padding=1, bias=False)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.cat([x, self.bottle_neck(x)], 1)\n",
        "\n",
        "class Transition(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.down_sample = nn.Sequential(\n",
        "            nn.BatchNorm2d(in_channels),\n",
        "            nn.Conv2d(in_channels, out_channels, 1, bias=False),\n",
        "            nn.AvgPool2d(2, stride=2)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.down_sample(x)\n",
        "\n",
        "class DenseNet(nn.Module):\n",
        "    def __init__(self, block, nblocks, growth_rate=12, reduction=0.5, num_class=100):\n",
        "        super().__init__()\n",
        "        self.growth_rate = growth_rate\n",
        "        inner_channels = 2 * growth_rate\n",
        "        self.conv1 = nn.Conv2d(3, inner_channels, kernel_size=3, padding=1, bias=False)\n",
        "\n",
        "        self.features = nn.Sequential()\n",
        "\n",
        "        for index in range(len(nblocks) - 1):\n",
        "            self.features.add_module(\"dense_block_layer_{}\".format(index), self._make_dense_layers(block, inner_channels, nblocks[index]))\n",
        "            inner_channels += growth_rate * nblocks[index]\n",
        "            out_channels = int(reduction * inner_channels) # int() will automatic floor the value\n",
        "            self.features.add_module(\"transition_layer_{}\".format(index), Transition(inner_channels, out_channels))\n",
        "            inner_channels = out_channels\n",
        "\n",
        "        self.features.add_module(\"dense_block{}\".format(len(nblocks) - 1), self._make_dense_layers(block, inner_channels, nblocks[len(nblocks)-1]))\n",
        "        inner_channels += growth_rate * nblocks[len(nblocks) - 1]\n",
        "        self.features.add_module('bn', nn.BatchNorm2d(inner_channels))\n",
        "        self.features.add_module('relu', nn.ReLU(inplace=True))\n",
        "\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "\n",
        "        self.linear = nn.Linear(inner_channels, num_class)\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = self.conv1(x)\n",
        "        output = self.features(output)\n",
        "        output = self.avgpool(output)\n",
        "        output = output.view(output.size()[0], -1)\n",
        "        output = self.linear(output)\n",
        "        return output\n",
        "\n",
        "    def _make_dense_layers(self, block, in_channels, nblocks):\n",
        "        dense_block = nn.Sequential()\n",
        "        for index in range(nblocks):\n",
        "            dense_block.add_module('bottle_neck_layer_{}'.format(index), block(in_channels, self.growth_rate))\n",
        "            in_channels += self.growth_rate\n",
        "        return dense_block\n",
        "\n",
        "def densenet121():\n",
        "    return DenseNet(Bottleneck, [6,12,24,16], growth_rate=32)"
      ],
      "metadata": {
        "id": "h_bzOGILSj-v"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = densenet121()\n",
        "model.to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    since = time.time()\n",
        "    for epoch in range(1, 2):\n",
        "        epoch_start = time.time()\n",
        "        train(epoch)\n",
        "        m, s = divmod(time.time() - epoch_start, 60)\n",
        "        print(f'Training time: {m:.0f}m {s:.0f}s')\n",
        "        test()\n",
        "        m, s = divmod(time.time() - epoch_start, 60)\n",
        "        print(f'Testing time: {m:.0f}m {s:.0f}s')\n",
        "\n",
        "    m, s = divmod(time.time() - since, 60)\n",
        "    print(f'Total Time: {m:.0f}m {s:.0f}s\\nModel was trained on {device}!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AJeaugagWffQ",
        "outputId": "d8088bb5-ebd1-4de5-d4e2-e6a3c2f93941"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 | Batch Status: 0/50000 (0%) | Loss: 4.575579\n",
            "Train Epoch: 1 | Batch Status: 320/50000 (1%) | Loss: 4.557178\n",
            "Train Epoch: 1 | Batch Status: 640/50000 (1%) | Loss: 4.466639\n",
            "Train Epoch: 1 | Batch Status: 960/50000 (2%) | Loss: 4.405077\n",
            "Train Epoch: 1 | Batch Status: 1280/50000 (3%) | Loss: 4.400238\n",
            "Train Epoch: 1 | Batch Status: 1600/50000 (3%) | Loss: 4.443605\n",
            "Train Epoch: 1 | Batch Status: 1920/50000 (4%) | Loss: 4.530312\n",
            "Train Epoch: 1 | Batch Status: 2240/50000 (4%) | Loss: 4.417354\n",
            "Train Epoch: 1 | Batch Status: 2560/50000 (5%) | Loss: 4.411758\n",
            "Train Epoch: 1 | Batch Status: 2880/50000 (6%) | Loss: 4.252985\n",
            "Train Epoch: 1 | Batch Status: 3200/50000 (6%) | Loss: 3.997777\n",
            "Train Epoch: 1 | Batch Status: 3520/50000 (7%) | Loss: 4.126915\n",
            "Train Epoch: 1 | Batch Status: 3840/50000 (8%) | Loss: 4.169085\n",
            "Train Epoch: 1 | Batch Status: 4160/50000 (8%) | Loss: 4.163388\n",
            "Train Epoch: 1 | Batch Status: 4480/50000 (9%) | Loss: 4.144103\n",
            "Train Epoch: 1 | Batch Status: 4800/50000 (10%) | Loss: 4.310805\n",
            "Train Epoch: 1 | Batch Status: 5120/50000 (10%) | Loss: 4.237829\n",
            "Train Epoch: 1 | Batch Status: 5440/50000 (11%) | Loss: 4.212861\n",
            "Train Epoch: 1 | Batch Status: 5760/50000 (12%) | Loss: 4.179975\n",
            "Train Epoch: 1 | Batch Status: 6080/50000 (12%) | Loss: 4.063601\n",
            "Train Epoch: 1 | Batch Status: 6400/50000 (13%) | Loss: 3.953849\n",
            "Train Epoch: 1 | Batch Status: 6720/50000 (13%) | Loss: 3.769649\n",
            "Train Epoch: 1 | Batch Status: 7040/50000 (14%) | Loss: 3.982198\n",
            "Train Epoch: 1 | Batch Status: 7360/50000 (15%) | Loss: 4.007496\n",
            "Train Epoch: 1 | Batch Status: 7680/50000 (15%) | Loss: 4.168310\n",
            "Train Epoch: 1 | Batch Status: 8000/50000 (16%) | Loss: 3.944314\n",
            "Train Epoch: 1 | Batch Status: 8320/50000 (17%) | Loss: 3.989554\n",
            "Train Epoch: 1 | Batch Status: 8640/50000 (17%) | Loss: 3.752780\n",
            "Train Epoch: 1 | Batch Status: 8960/50000 (18%) | Loss: 3.831817\n",
            "Train Epoch: 1 | Batch Status: 9280/50000 (19%) | Loss: 3.843051\n",
            "Train Epoch: 1 | Batch Status: 9600/50000 (19%) | Loss: 3.761128\n",
            "Train Epoch: 1 | Batch Status: 9920/50000 (20%) | Loss: 3.806088\n",
            "Train Epoch: 1 | Batch Status: 10240/50000 (20%) | Loss: 4.038305\n",
            "Train Epoch: 1 | Batch Status: 10560/50000 (21%) | Loss: 4.102328\n",
            "Train Epoch: 1 | Batch Status: 10880/50000 (22%) | Loss: 3.792462\n",
            "Train Epoch: 1 | Batch Status: 11200/50000 (22%) | Loss: 3.605781\n",
            "Train Epoch: 1 | Batch Status: 11520/50000 (23%) | Loss: 3.829673\n",
            "Train Epoch: 1 | Batch Status: 11840/50000 (24%) | Loss: 3.965844\n",
            "Train Epoch: 1 | Batch Status: 12160/50000 (24%) | Loss: 3.812989\n",
            "Train Epoch: 1 | Batch Status: 12480/50000 (25%) | Loss: 3.631968\n",
            "Train Epoch: 1 | Batch Status: 12800/50000 (26%) | Loss: 3.752491\n",
            "Train Epoch: 1 | Batch Status: 13120/50000 (26%) | Loss: 3.743741\n",
            "Train Epoch: 1 | Batch Status: 13440/50000 (27%) | Loss: 3.569004\n",
            "Train Epoch: 1 | Batch Status: 13760/50000 (28%) | Loss: 3.968697\n",
            "Train Epoch: 1 | Batch Status: 14080/50000 (28%) | Loss: 3.993759\n",
            "Train Epoch: 1 | Batch Status: 14400/50000 (29%) | Loss: 3.487487\n",
            "Train Epoch: 1 | Batch Status: 14720/50000 (29%) | Loss: 3.846303\n",
            "Train Epoch: 1 | Batch Status: 15040/50000 (30%) | Loss: 3.687333\n",
            "Train Epoch: 1 | Batch Status: 15360/50000 (31%) | Loss: 3.804522\n",
            "Train Epoch: 1 | Batch Status: 15680/50000 (31%) | Loss: 3.791647\n",
            "Train Epoch: 1 | Batch Status: 16000/50000 (32%) | Loss: 3.786106\n",
            "Train Epoch: 1 | Batch Status: 16320/50000 (33%) | Loss: 3.811877\n",
            "Train Epoch: 1 | Batch Status: 16640/50000 (33%) | Loss: 3.876288\n",
            "Train Epoch: 1 | Batch Status: 16960/50000 (34%) | Loss: 3.743346\n",
            "Train Epoch: 1 | Batch Status: 17280/50000 (35%) | Loss: 3.638698\n",
            "Train Epoch: 1 | Batch Status: 17600/50000 (35%) | Loss: 3.866574\n",
            "Train Epoch: 1 | Batch Status: 17920/50000 (36%) | Loss: 3.642554\n",
            "Train Epoch: 1 | Batch Status: 18240/50000 (36%) | Loss: 3.642051\n",
            "Train Epoch: 1 | Batch Status: 18560/50000 (37%) | Loss: 4.104464\n",
            "Train Epoch: 1 | Batch Status: 18880/50000 (38%) | Loss: 3.492473\n",
            "Train Epoch: 1 | Batch Status: 19200/50000 (38%) | Loss: 3.942001\n",
            "Train Epoch: 1 | Batch Status: 19520/50000 (39%) | Loss: 3.596681\n",
            "Train Epoch: 1 | Batch Status: 19840/50000 (40%) | Loss: 3.725393\n",
            "Train Epoch: 1 | Batch Status: 20160/50000 (40%) | Loss: 3.410453\n",
            "Train Epoch: 1 | Batch Status: 20480/50000 (41%) | Loss: 4.021368\n",
            "Train Epoch: 1 | Batch Status: 20800/50000 (42%) | Loss: 3.661779\n",
            "Train Epoch: 1 | Batch Status: 21120/50000 (42%) | Loss: 3.630450\n",
            "Train Epoch: 1 | Batch Status: 21440/50000 (43%) | Loss: 3.444860\n",
            "Train Epoch: 1 | Batch Status: 21760/50000 (44%) | Loss: 3.647501\n",
            "Train Epoch: 1 | Batch Status: 22080/50000 (44%) | Loss: 3.693602\n",
            "Train Epoch: 1 | Batch Status: 22400/50000 (45%) | Loss: 3.237993\n",
            "Train Epoch: 1 | Batch Status: 22720/50000 (45%) | Loss: 2.933327\n",
            "Train Epoch: 1 | Batch Status: 23040/50000 (46%) | Loss: 3.906145\n",
            "Train Epoch: 1 | Batch Status: 23360/50000 (47%) | Loss: 3.862149\n",
            "Train Epoch: 1 | Batch Status: 23680/50000 (47%) | Loss: 3.568505\n",
            "Train Epoch: 1 | Batch Status: 24000/50000 (48%) | Loss: 3.673761\n",
            "Train Epoch: 1 | Batch Status: 24320/50000 (49%) | Loss: 3.489343\n",
            "Train Epoch: 1 | Batch Status: 24640/50000 (49%) | Loss: 3.645993\n",
            "Train Epoch: 1 | Batch Status: 24960/50000 (50%) | Loss: 3.344666\n",
            "Train Epoch: 1 | Batch Status: 25280/50000 (51%) | Loss: 3.595937\n",
            "Train Epoch: 1 | Batch Status: 25600/50000 (51%) | Loss: 3.910771\n",
            "Train Epoch: 1 | Batch Status: 25920/50000 (52%) | Loss: 4.054445\n",
            "Train Epoch: 1 | Batch Status: 26240/50000 (52%) | Loss: 3.648076\n",
            "Train Epoch: 1 | Batch Status: 26560/50000 (53%) | Loss: 3.501998\n",
            "Train Epoch: 1 | Batch Status: 26880/50000 (54%) | Loss: 3.558618\n",
            "Train Epoch: 1 | Batch Status: 27200/50000 (54%) | Loss: 3.415783\n",
            "Train Epoch: 1 | Batch Status: 27520/50000 (55%) | Loss: 3.392381\n",
            "Train Epoch: 1 | Batch Status: 27840/50000 (56%) | Loss: 3.075090\n",
            "Train Epoch: 1 | Batch Status: 28160/50000 (56%) | Loss: 3.560885\n",
            "Train Epoch: 1 | Batch Status: 28480/50000 (57%) | Loss: 3.557356\n",
            "Train Epoch: 1 | Batch Status: 28800/50000 (58%) | Loss: 3.899769\n",
            "Train Epoch: 1 | Batch Status: 29120/50000 (58%) | Loss: 3.259213\n",
            "Train Epoch: 1 | Batch Status: 29440/50000 (59%) | Loss: 3.471905\n",
            "Train Epoch: 1 | Batch Status: 29760/50000 (60%) | Loss: 3.656953\n",
            "Train Epoch: 1 | Batch Status: 30080/50000 (60%) | Loss: 3.156235\n",
            "Train Epoch: 1 | Batch Status: 30400/50000 (61%) | Loss: 3.585463\n",
            "Train Epoch: 1 | Batch Status: 30720/50000 (61%) | Loss: 3.283839\n",
            "Train Epoch: 1 | Batch Status: 31040/50000 (62%) | Loss: 3.144330\n",
            "Train Epoch: 1 | Batch Status: 31360/50000 (63%) | Loss: 3.679283\n",
            "Train Epoch: 1 | Batch Status: 31680/50000 (63%) | Loss: 3.481514\n",
            "Train Epoch: 1 | Batch Status: 32000/50000 (64%) | Loss: 3.467204\n",
            "Train Epoch: 1 | Batch Status: 32320/50000 (65%) | Loss: 3.377292\n",
            "Train Epoch: 1 | Batch Status: 32640/50000 (65%) | Loss: 3.365369\n",
            "Train Epoch: 1 | Batch Status: 32960/50000 (66%) | Loss: 3.072158\n",
            "Train Epoch: 1 | Batch Status: 33280/50000 (67%) | Loss: 3.467296\n",
            "Train Epoch: 1 | Batch Status: 33600/50000 (67%) | Loss: 3.392561\n",
            "Train Epoch: 1 | Batch Status: 33920/50000 (68%) | Loss: 3.219417\n",
            "Train Epoch: 1 | Batch Status: 34240/50000 (68%) | Loss: 3.227919\n",
            "Train Epoch: 1 | Batch Status: 34560/50000 (69%) | Loss: 3.580344\n",
            "Train Epoch: 1 | Batch Status: 34880/50000 (70%) | Loss: 3.450218\n",
            "Train Epoch: 1 | Batch Status: 35200/50000 (70%) | Loss: 3.824146\n",
            "Train Epoch: 1 | Batch Status: 35520/50000 (71%) | Loss: 3.475439\n",
            "Train Epoch: 1 | Batch Status: 35840/50000 (72%) | Loss: 3.312019\n",
            "Train Epoch: 1 | Batch Status: 36160/50000 (72%) | Loss: 2.896503\n",
            "Train Epoch: 1 | Batch Status: 36480/50000 (73%) | Loss: 3.255677\n",
            "Train Epoch: 1 | Batch Status: 36800/50000 (74%) | Loss: 2.985200\n",
            "Train Epoch: 1 | Batch Status: 37120/50000 (74%) | Loss: 3.105373\n",
            "Train Epoch: 1 | Batch Status: 37440/50000 (75%) | Loss: 3.177993\n",
            "Train Epoch: 1 | Batch Status: 37760/50000 (75%) | Loss: 3.418523\n",
            "Train Epoch: 1 | Batch Status: 38080/50000 (76%) | Loss: 3.392547\n",
            "Train Epoch: 1 | Batch Status: 38400/50000 (77%) | Loss: 3.226078\n",
            "Train Epoch: 1 | Batch Status: 38720/50000 (77%) | Loss: 3.091238\n",
            "Train Epoch: 1 | Batch Status: 39040/50000 (78%) | Loss: 3.081208\n",
            "Train Epoch: 1 | Batch Status: 39360/50000 (79%) | Loss: 3.042591\n",
            "Train Epoch: 1 | Batch Status: 39680/50000 (79%) | Loss: 3.035282\n",
            "Train Epoch: 1 | Batch Status: 40000/50000 (80%) | Loss: 2.798631\n",
            "Train Epoch: 1 | Batch Status: 40320/50000 (81%) | Loss: 3.361902\n",
            "Train Epoch: 1 | Batch Status: 40640/50000 (81%) | Loss: 3.215977\n",
            "Train Epoch: 1 | Batch Status: 40960/50000 (82%) | Loss: 3.126891\n",
            "Train Epoch: 1 | Batch Status: 41280/50000 (83%) | Loss: 3.154207\n",
            "Train Epoch: 1 | Batch Status: 41600/50000 (83%) | Loss: 3.323853\n",
            "Train Epoch: 1 | Batch Status: 41920/50000 (84%) | Loss: 3.483289\n",
            "Train Epoch: 1 | Batch Status: 42240/50000 (84%) | Loss: 2.797786\n",
            "Train Epoch: 1 | Batch Status: 42560/50000 (85%) | Loss: 2.983958\n",
            "Train Epoch: 1 | Batch Status: 42880/50000 (86%) | Loss: 3.095746\n",
            "Train Epoch: 1 | Batch Status: 43200/50000 (86%) | Loss: 3.284219\n",
            "Train Epoch: 1 | Batch Status: 43520/50000 (87%) | Loss: 3.336416\n",
            "Train Epoch: 1 | Batch Status: 43840/50000 (88%) | Loss: 3.604653\n",
            "Train Epoch: 1 | Batch Status: 44160/50000 (88%) | Loss: 3.242362\n",
            "Train Epoch: 1 | Batch Status: 44480/50000 (89%) | Loss: 3.172029\n",
            "Train Epoch: 1 | Batch Status: 44800/50000 (90%) | Loss: 3.198457\n",
            "Train Epoch: 1 | Batch Status: 45120/50000 (90%) | Loss: 3.533172\n",
            "Train Epoch: 1 | Batch Status: 45440/50000 (91%) | Loss: 3.204352\n",
            "Train Epoch: 1 | Batch Status: 45760/50000 (91%) | Loss: 3.010252\n",
            "Train Epoch: 1 | Batch Status: 46080/50000 (92%) | Loss: 3.258005\n",
            "Train Epoch: 1 | Batch Status: 46400/50000 (93%) | Loss: 3.031841\n",
            "Train Epoch: 1 | Batch Status: 46720/50000 (93%) | Loss: 3.193831\n",
            "Train Epoch: 1 | Batch Status: 47040/50000 (94%) | Loss: 2.889231\n",
            "Train Epoch: 1 | Batch Status: 47360/50000 (95%) | Loss: 2.839703\n",
            "Train Epoch: 1 | Batch Status: 47680/50000 (95%) | Loss: 2.841780\n",
            "Train Epoch: 1 | Batch Status: 48000/50000 (96%) | Loss: 3.092159\n",
            "Train Epoch: 1 | Batch Status: 48320/50000 (97%) | Loss: 2.818257\n",
            "Train Epoch: 1 | Batch Status: 48640/50000 (97%) | Loss: 3.112246\n",
            "Train Epoch: 1 | Batch Status: 48960/50000 (98%) | Loss: 3.187976\n",
            "Train Epoch: 1 | Batch Status: 49280/50000 (99%) | Loss: 2.980742\n",
            "Train Epoch: 1 | Batch Status: 49600/50000 (99%) | Loss: 2.769616\n",
            "Train Epoch: 1 | Batch Status: 49920/50000 (100%) | Loss: 2.894292\n",
            "Training time: 9m 2s\n",
            "Testing time: 9m 39s\n",
            "Train Epoch: 2 | Batch Status: 0/50000 (0%) | Loss: 2.868852\n",
            "Train Epoch: 2 | Batch Status: 320/50000 (1%) | Loss: 3.094990\n",
            "Train Epoch: 2 | Batch Status: 640/50000 (1%) | Loss: 3.054615\n",
            "Train Epoch: 2 | Batch Status: 960/50000 (2%) | Loss: 3.144956\n",
            "Train Epoch: 2 | Batch Status: 1280/50000 (3%) | Loss: 2.977323\n",
            "Train Epoch: 2 | Batch Status: 1600/50000 (3%) | Loss: 3.200318\n",
            "Train Epoch: 2 | Batch Status: 1920/50000 (4%) | Loss: 3.145166\n",
            "Train Epoch: 2 | Batch Status: 2240/50000 (4%) | Loss: 2.795390\n",
            "Train Epoch: 2 | Batch Status: 2560/50000 (5%) | Loss: 3.292945\n",
            "Train Epoch: 2 | Batch Status: 2880/50000 (6%) | Loss: 3.191007\n",
            "Train Epoch: 2 | Batch Status: 3200/50000 (6%) | Loss: 3.317759\n",
            "Train Epoch: 2 | Batch Status: 3520/50000 (7%) | Loss: 2.945512\n",
            "Train Epoch: 2 | Batch Status: 3840/50000 (8%) | Loss: 2.695650\n",
            "Train Epoch: 2 | Batch Status: 4160/50000 (8%) | Loss: 3.051835\n",
            "Train Epoch: 2 | Batch Status: 4480/50000 (9%) | Loss: 2.981411\n",
            "Train Epoch: 2 | Batch Status: 4800/50000 (10%) | Loss: 2.747020\n",
            "Train Epoch: 2 | Batch Status: 5120/50000 (10%) | Loss: 2.900573\n",
            "Train Epoch: 2 | Batch Status: 5440/50000 (11%) | Loss: 3.160951\n",
            "Train Epoch: 2 | Batch Status: 5760/50000 (12%) | Loss: 2.539120\n",
            "Train Epoch: 2 | Batch Status: 6080/50000 (12%) | Loss: 2.620890\n",
            "Train Epoch: 2 | Batch Status: 6400/50000 (13%) | Loss: 2.860475\n",
            "Train Epoch: 2 | Batch Status: 6720/50000 (13%) | Loss: 2.768701\n",
            "Train Epoch: 2 | Batch Status: 7040/50000 (14%) | Loss: 2.772018\n",
            "Train Epoch: 2 | Batch Status: 7360/50000 (15%) | Loss: 3.017450\n",
            "Train Epoch: 2 | Batch Status: 7680/50000 (15%) | Loss: 2.622971\n",
            "Train Epoch: 2 | Batch Status: 8000/50000 (16%) | Loss: 3.279595\n",
            "Train Epoch: 2 | Batch Status: 8320/50000 (17%) | Loss: 3.345648\n",
            "Train Epoch: 2 | Batch Status: 8640/50000 (17%) | Loss: 2.790918\n",
            "Train Epoch: 2 | Batch Status: 8960/50000 (18%) | Loss: 2.793091\n",
            "Train Epoch: 2 | Batch Status: 9280/50000 (19%) | Loss: 2.848419\n",
            "Train Epoch: 2 | Batch Status: 9600/50000 (19%) | Loss: 2.859427\n",
            "Train Epoch: 2 | Batch Status: 9920/50000 (20%) | Loss: 2.882232\n",
            "Train Epoch: 2 | Batch Status: 10240/50000 (20%) | Loss: 2.446792\n",
            "Train Epoch: 2 | Batch Status: 10560/50000 (21%) | Loss: 2.720190\n",
            "Train Epoch: 2 | Batch Status: 10880/50000 (22%) | Loss: 3.035578\n",
            "Train Epoch: 2 | Batch Status: 11200/50000 (22%) | Loss: 2.795642\n",
            "Train Epoch: 2 | Batch Status: 11520/50000 (23%) | Loss: 2.967607\n",
            "Train Epoch: 2 | Batch Status: 11840/50000 (24%) | Loss: 3.069392\n",
            "Train Epoch: 2 | Batch Status: 12160/50000 (24%) | Loss: 2.894320\n",
            "Train Epoch: 2 | Batch Status: 12480/50000 (25%) | Loss: 2.381683\n",
            "Train Epoch: 2 | Batch Status: 12800/50000 (26%) | Loss: 2.660970\n",
            "Train Epoch: 2 | Batch Status: 13120/50000 (26%) | Loss: 2.459158\n",
            "Train Epoch: 2 | Batch Status: 13440/50000 (27%) | Loss: 2.768759\n",
            "Train Epoch: 2 | Batch Status: 13760/50000 (28%) | Loss: 2.727304\n",
            "Train Epoch: 2 | Batch Status: 14080/50000 (28%) | Loss: 3.052835\n",
            "Train Epoch: 2 | Batch Status: 14400/50000 (29%) | Loss: 3.168808\n",
            "Train Epoch: 2 | Batch Status: 14720/50000 (29%) | Loss: 2.887134\n",
            "Train Epoch: 2 | Batch Status: 15040/50000 (30%) | Loss: 2.482160\n",
            "Train Epoch: 2 | Batch Status: 15360/50000 (31%) | Loss: 2.694089\n",
            "Train Epoch: 2 | Batch Status: 15680/50000 (31%) | Loss: 2.619498\n",
            "Train Epoch: 2 | Batch Status: 16000/50000 (32%) | Loss: 2.279721\n",
            "Train Epoch: 2 | Batch Status: 16320/50000 (33%) | Loss: 2.712021\n",
            "Train Epoch: 2 | Batch Status: 16640/50000 (33%) | Loss: 2.889436\n",
            "Train Epoch: 2 | Batch Status: 16960/50000 (34%) | Loss: 2.916396\n",
            "Train Epoch: 2 | Batch Status: 17280/50000 (35%) | Loss: 2.363157\n",
            "Train Epoch: 2 | Batch Status: 17600/50000 (35%) | Loss: 2.993125\n",
            "Train Epoch: 2 | Batch Status: 17920/50000 (36%) | Loss: 2.753510\n",
            "Train Epoch: 2 | Batch Status: 18240/50000 (36%) | Loss: 2.922691\n",
            "Train Epoch: 2 | Batch Status: 18560/50000 (37%) | Loss: 2.741508\n",
            "Train Epoch: 2 | Batch Status: 18880/50000 (38%) | Loss: 2.811273\n",
            "Train Epoch: 2 | Batch Status: 19200/50000 (38%) | Loss: 3.116574\n",
            "Train Epoch: 2 | Batch Status: 19520/50000 (39%) | Loss: 3.016903\n",
            "Train Epoch: 2 | Batch Status: 19840/50000 (40%) | Loss: 2.290805\n",
            "Train Epoch: 2 | Batch Status: 20160/50000 (40%) | Loss: 2.803775\n",
            "Train Epoch: 2 | Batch Status: 20480/50000 (41%) | Loss: 3.091873\n",
            "Train Epoch: 2 | Batch Status: 20800/50000 (42%) | Loss: 2.160796\n",
            "Train Epoch: 2 | Batch Status: 21120/50000 (42%) | Loss: 2.809398\n",
            "Train Epoch: 2 | Batch Status: 21440/50000 (43%) | Loss: 2.294567\n",
            "Train Epoch: 2 | Batch Status: 21760/50000 (44%) | Loss: 2.611476\n",
            "Train Epoch: 2 | Batch Status: 22080/50000 (44%) | Loss: 2.775439\n",
            "Train Epoch: 2 | Batch Status: 22400/50000 (45%) | Loss: 3.179652\n",
            "Train Epoch: 2 | Batch Status: 22720/50000 (45%) | Loss: 2.829778\n",
            "Train Epoch: 2 | Batch Status: 23040/50000 (46%) | Loss: 2.550828\n",
            "Train Epoch: 2 | Batch Status: 23360/50000 (47%) | Loss: 3.020719\n",
            "Train Epoch: 2 | Batch Status: 23680/50000 (47%) | Loss: 2.935008\n",
            "Train Epoch: 2 | Batch Status: 24000/50000 (48%) | Loss: 2.418544\n",
            "Train Epoch: 2 | Batch Status: 24320/50000 (49%) | Loss: 2.366957\n",
            "Train Epoch: 2 | Batch Status: 24640/50000 (49%) | Loss: 2.244791\n",
            "Train Epoch: 2 | Batch Status: 24960/50000 (50%) | Loss: 2.476493\n",
            "Train Epoch: 2 | Batch Status: 25280/50000 (51%) | Loss: 2.670708\n",
            "Train Epoch: 2 | Batch Status: 25600/50000 (51%) | Loss: 2.483291\n",
            "Train Epoch: 2 | Batch Status: 25920/50000 (52%) | Loss: 2.788681\n",
            "Train Epoch: 2 | Batch Status: 26240/50000 (52%) | Loss: 2.532414\n",
            "Train Epoch: 2 | Batch Status: 26560/50000 (53%) | Loss: 2.676752\n",
            "Train Epoch: 2 | Batch Status: 26880/50000 (54%) | Loss: 2.322197\n",
            "Train Epoch: 2 | Batch Status: 27200/50000 (54%) | Loss: 2.142743\n",
            "Train Epoch: 2 | Batch Status: 27520/50000 (55%) | Loss: 2.486197\n",
            "Train Epoch: 2 | Batch Status: 27840/50000 (56%) | Loss: 2.395800\n",
            "Train Epoch: 2 | Batch Status: 28160/50000 (56%) | Loss: 2.883330\n",
            "Train Epoch: 2 | Batch Status: 28480/50000 (57%) | Loss: 2.712064\n",
            "Train Epoch: 2 | Batch Status: 28800/50000 (58%) | Loss: 2.793075\n",
            "Train Epoch: 2 | Batch Status: 29120/50000 (58%) | Loss: 2.443339\n",
            "Train Epoch: 2 | Batch Status: 29440/50000 (59%) | Loss: 2.891725\n",
            "Train Epoch: 2 | Batch Status: 29760/50000 (60%) | Loss: 2.377345\n",
            "Train Epoch: 2 | Batch Status: 30080/50000 (60%) | Loss: 2.345323\n",
            "Train Epoch: 2 | Batch Status: 30400/50000 (61%) | Loss: 2.362651\n",
            "Train Epoch: 2 | Batch Status: 30720/50000 (61%) | Loss: 2.588184\n",
            "Train Epoch: 2 | Batch Status: 31040/50000 (62%) | Loss: 2.193342\n",
            "Train Epoch: 2 | Batch Status: 31360/50000 (63%) | Loss: 2.858925\n",
            "Train Epoch: 2 | Batch Status: 31680/50000 (63%) | Loss: 2.853122\n",
            "Train Epoch: 2 | Batch Status: 32000/50000 (64%) | Loss: 2.656605\n",
            "Train Epoch: 2 | Batch Status: 32320/50000 (65%) | Loss: 3.055861\n",
            "Train Epoch: 2 | Batch Status: 32640/50000 (65%) | Loss: 2.397249\n",
            "Train Epoch: 2 | Batch Status: 32960/50000 (66%) | Loss: 2.495963\n",
            "Train Epoch: 2 | Batch Status: 33280/50000 (67%) | Loss: 2.379231\n",
            "Train Epoch: 2 | Batch Status: 33600/50000 (67%) | Loss: 2.701171\n",
            "Train Epoch: 2 | Batch Status: 33920/50000 (68%) | Loss: 2.177228\n",
            "Train Epoch: 2 | Batch Status: 34240/50000 (68%) | Loss: 2.735035\n",
            "Train Epoch: 2 | Batch Status: 34560/50000 (69%) | Loss: 2.208880\n",
            "Train Epoch: 2 | Batch Status: 34880/50000 (70%) | Loss: 2.532999\n",
            "Train Epoch: 2 | Batch Status: 35200/50000 (70%) | Loss: 2.227079\n",
            "Train Epoch: 2 | Batch Status: 35520/50000 (71%) | Loss: 2.215561\n",
            "Train Epoch: 2 | Batch Status: 35840/50000 (72%) | Loss: 2.314913\n",
            "Train Epoch: 2 | Batch Status: 36160/50000 (72%) | Loss: 1.994301\n",
            "Train Epoch: 2 | Batch Status: 36480/50000 (73%) | Loss: 3.091975\n",
            "Train Epoch: 2 | Batch Status: 36800/50000 (74%) | Loss: 2.501725\n",
            "Train Epoch: 2 | Batch Status: 37120/50000 (74%) | Loss: 2.271414\n",
            "Train Epoch: 2 | Batch Status: 37440/50000 (75%) | Loss: 2.799572\n",
            "Train Epoch: 2 | Batch Status: 37760/50000 (75%) | Loss: 2.425058\n",
            "Train Epoch: 2 | Batch Status: 38080/50000 (76%) | Loss: 2.611820\n",
            "Train Epoch: 2 | Batch Status: 38400/50000 (77%) | Loss: 2.375048\n",
            "Train Epoch: 2 | Batch Status: 38720/50000 (77%) | Loss: 2.316743\n",
            "Train Epoch: 2 | Batch Status: 39040/50000 (78%) | Loss: 2.220243\n",
            "Train Epoch: 2 | Batch Status: 39360/50000 (79%) | Loss: 2.714280\n",
            "Train Epoch: 2 | Batch Status: 39680/50000 (79%) | Loss: 2.656360\n",
            "Train Epoch: 2 | Batch Status: 40000/50000 (80%) | Loss: 2.433552\n",
            "Train Epoch: 2 | Batch Status: 40320/50000 (81%) | Loss: 2.936429\n",
            "Train Epoch: 2 | Batch Status: 40640/50000 (81%) | Loss: 2.971137\n",
            "Train Epoch: 2 | Batch Status: 40960/50000 (82%) | Loss: 2.806041\n",
            "Train Epoch: 2 | Batch Status: 41280/50000 (83%) | Loss: 2.816661\n",
            "Train Epoch: 2 | Batch Status: 41600/50000 (83%) | Loss: 2.769286\n",
            "Train Epoch: 2 | Batch Status: 41920/50000 (84%) | Loss: 1.970198\n",
            "Train Epoch: 2 | Batch Status: 42240/50000 (84%) | Loss: 2.388776\n",
            "Train Epoch: 2 | Batch Status: 42560/50000 (85%) | Loss: 2.311823\n",
            "Train Epoch: 2 | Batch Status: 42880/50000 (86%) | Loss: 2.408362\n",
            "Train Epoch: 2 | Batch Status: 43200/50000 (86%) | Loss: 2.389850\n",
            "Train Epoch: 2 | Batch Status: 43520/50000 (87%) | Loss: 2.092382\n",
            "Train Epoch: 2 | Batch Status: 43840/50000 (88%) | Loss: 2.602560\n",
            "Train Epoch: 2 | Batch Status: 44160/50000 (88%) | Loss: 2.486574\n",
            "Train Epoch: 2 | Batch Status: 44480/50000 (89%) | Loss: 2.387814\n",
            "Train Epoch: 2 | Batch Status: 44800/50000 (90%) | Loss: 2.406969\n",
            "Train Epoch: 2 | Batch Status: 45120/50000 (90%) | Loss: 2.422072\n",
            "Train Epoch: 2 | Batch Status: 45440/50000 (91%) | Loss: 2.563011\n",
            "Train Epoch: 2 | Batch Status: 45760/50000 (91%) | Loss: 2.624189\n",
            "Train Epoch: 2 | Batch Status: 46080/50000 (92%) | Loss: 2.812509\n",
            "Train Epoch: 2 | Batch Status: 46400/50000 (93%) | Loss: 2.765429\n",
            "Train Epoch: 2 | Batch Status: 46720/50000 (93%) | Loss: 2.377820\n",
            "Train Epoch: 2 | Batch Status: 47040/50000 (94%) | Loss: 2.762710\n",
            "Train Epoch: 2 | Batch Status: 47360/50000 (95%) | Loss: 2.415507\n",
            "Train Epoch: 2 | Batch Status: 47680/50000 (95%) | Loss: 2.596379\n",
            "Train Epoch: 2 | Batch Status: 48000/50000 (96%) | Loss: 2.993234\n",
            "Train Epoch: 2 | Batch Status: 48320/50000 (97%) | Loss: 2.622609\n",
            "Train Epoch: 2 | Batch Status: 48640/50000 (97%) | Loss: 2.910048\n",
            "Train Epoch: 2 | Batch Status: 48960/50000 (98%) | Loss: 2.545727\n",
            "Train Epoch: 2 | Batch Status: 49280/50000 (99%) | Loss: 2.329823\n",
            "Train Epoch: 2 | Batch Status: 49600/50000 (99%) | Loss: 1.964271\n",
            "Train Epoch: 2 | Batch Status: 49920/50000 (100%) | Loss: 2.469196\n",
            "Training time: 9m 2s\n",
            "Testing time: 9m 39s\n",
            "Train Epoch: 3 | Batch Status: 0/50000 (0%) | Loss: 2.284562\n",
            "Train Epoch: 3 | Batch Status: 320/50000 (1%) | Loss: 2.722852\n",
            "Train Epoch: 3 | Batch Status: 640/50000 (1%) | Loss: 2.366327\n",
            "Train Epoch: 3 | Batch Status: 960/50000 (2%) | Loss: 2.452012\n",
            "Train Epoch: 3 | Batch Status: 1280/50000 (3%) | Loss: 2.351380\n",
            "Train Epoch: 3 | Batch Status: 1600/50000 (3%) | Loss: 1.954312\n",
            "Train Epoch: 3 | Batch Status: 1920/50000 (4%) | Loss: 2.477054\n",
            "Train Epoch: 3 | Batch Status: 2240/50000 (4%) | Loss: 1.735241\n",
            "Train Epoch: 3 | Batch Status: 2560/50000 (5%) | Loss: 2.601107\n",
            "Train Epoch: 3 | Batch Status: 2880/50000 (6%) | Loss: 2.054792\n",
            "Train Epoch: 3 | Batch Status: 3200/50000 (6%) | Loss: 2.407072\n",
            "Train Epoch: 3 | Batch Status: 3520/50000 (7%) | Loss: 2.241608\n",
            "Train Epoch: 3 | Batch Status: 3840/50000 (8%) | Loss: 1.889151\n",
            "Train Epoch: 3 | Batch Status: 4160/50000 (8%) | Loss: 2.704251\n",
            "Train Epoch: 3 | Batch Status: 4480/50000 (9%) | Loss: 1.811055\n",
            "Train Epoch: 3 | Batch Status: 4800/50000 (10%) | Loss: 1.943116\n",
            "Train Epoch: 3 | Batch Status: 5120/50000 (10%) | Loss: 2.370352\n",
            "Train Epoch: 3 | Batch Status: 5440/50000 (11%) | Loss: 2.118739\n",
            "Train Epoch: 3 | Batch Status: 5760/50000 (12%) | Loss: 1.952604\n",
            "Train Epoch: 3 | Batch Status: 6080/50000 (12%) | Loss: 2.108876\n",
            "Train Epoch: 3 | Batch Status: 6400/50000 (13%) | Loss: 2.477270\n",
            "Train Epoch: 3 | Batch Status: 6720/50000 (13%) | Loss: 2.434850\n",
            "Train Epoch: 3 | Batch Status: 7040/50000 (14%) | Loss: 2.012645\n",
            "Train Epoch: 3 | Batch Status: 7360/50000 (15%) | Loss: 2.358719\n",
            "Train Epoch: 3 | Batch Status: 7680/50000 (15%) | Loss: 2.278556\n",
            "Train Epoch: 3 | Batch Status: 8000/50000 (16%) | Loss: 1.990483\n",
            "Train Epoch: 3 | Batch Status: 8320/50000 (17%) | Loss: 2.363096\n",
            "Train Epoch: 3 | Batch Status: 8640/50000 (17%) | Loss: 2.193590\n",
            "Train Epoch: 3 | Batch Status: 8960/50000 (18%) | Loss: 2.236158\n",
            "Train Epoch: 3 | Batch Status: 9280/50000 (19%) | Loss: 2.055912\n",
            "Train Epoch: 3 | Batch Status: 9600/50000 (19%) | Loss: 2.102767\n",
            "Train Epoch: 3 | Batch Status: 9920/50000 (20%) | Loss: 2.340090\n",
            "Train Epoch: 3 | Batch Status: 10240/50000 (20%) | Loss: 2.457964\n",
            "Train Epoch: 3 | Batch Status: 10560/50000 (21%) | Loss: 2.396096\n",
            "Train Epoch: 3 | Batch Status: 10880/50000 (22%) | Loss: 1.930278\n",
            "Train Epoch: 3 | Batch Status: 11200/50000 (22%) | Loss: 2.284065\n",
            "Train Epoch: 3 | Batch Status: 11520/50000 (23%) | Loss: 2.480828\n",
            "Train Epoch: 3 | Batch Status: 11840/50000 (24%) | Loss: 2.384086\n",
            "Train Epoch: 3 | Batch Status: 12160/50000 (24%) | Loss: 2.453698\n",
            "Train Epoch: 3 | Batch Status: 12480/50000 (25%) | Loss: 2.438013\n",
            "Train Epoch: 3 | Batch Status: 12800/50000 (26%) | Loss: 2.290338\n",
            "Train Epoch: 3 | Batch Status: 13120/50000 (26%) | Loss: 2.362818\n",
            "Train Epoch: 3 | Batch Status: 13440/50000 (27%) | Loss: 2.004418\n",
            "Train Epoch: 3 | Batch Status: 13760/50000 (28%) | Loss: 2.070218\n",
            "Train Epoch: 3 | Batch Status: 14080/50000 (28%) | Loss: 2.038326\n",
            "Train Epoch: 3 | Batch Status: 14400/50000 (29%) | Loss: 2.396358\n",
            "Train Epoch: 3 | Batch Status: 14720/50000 (29%) | Loss: 2.175053\n",
            "Train Epoch: 3 | Batch Status: 15040/50000 (30%) | Loss: 2.527189\n",
            "Train Epoch: 3 | Batch Status: 15360/50000 (31%) | Loss: 1.893846\n",
            "Train Epoch: 3 | Batch Status: 15680/50000 (31%) | Loss: 2.587079\n",
            "Train Epoch: 3 | Batch Status: 16000/50000 (32%) | Loss: 1.954215\n",
            "Train Epoch: 3 | Batch Status: 16320/50000 (33%) | Loss: 2.472078\n",
            "Train Epoch: 3 | Batch Status: 16640/50000 (33%) | Loss: 1.747824\n",
            "Train Epoch: 3 | Batch Status: 16960/50000 (34%) | Loss: 2.069809\n",
            "Train Epoch: 3 | Batch Status: 17280/50000 (35%) | Loss: 1.833635\n",
            "Train Epoch: 3 | Batch Status: 17600/50000 (35%) | Loss: 2.201025\n",
            "Train Epoch: 3 | Batch Status: 17920/50000 (36%) | Loss: 1.667026\n",
            "Train Epoch: 3 | Batch Status: 18240/50000 (36%) | Loss: 2.022811\n",
            "Train Epoch: 3 | Batch Status: 18560/50000 (37%) | Loss: 2.303408\n",
            "Train Epoch: 3 | Batch Status: 18880/50000 (38%) | Loss: 2.082203\n",
            "Train Epoch: 3 | Batch Status: 19200/50000 (38%) | Loss: 2.642283\n",
            "Train Epoch: 3 | Batch Status: 19520/50000 (39%) | Loss: 2.337252\n",
            "Train Epoch: 3 | Batch Status: 19840/50000 (40%) | Loss: 2.461296\n",
            "Train Epoch: 3 | Batch Status: 20160/50000 (40%) | Loss: 2.660547\n",
            "Train Epoch: 3 | Batch Status: 20480/50000 (41%) | Loss: 2.027168\n",
            "Train Epoch: 3 | Batch Status: 20800/50000 (42%) | Loss: 2.468511\n",
            "Train Epoch: 3 | Batch Status: 21120/50000 (42%) | Loss: 2.332410\n",
            "Train Epoch: 3 | Batch Status: 21440/50000 (43%) | Loss: 1.844684\n",
            "Train Epoch: 3 | Batch Status: 21760/50000 (44%) | Loss: 1.856418\n",
            "Train Epoch: 3 | Batch Status: 22080/50000 (44%) | Loss: 2.492532\n",
            "Train Epoch: 3 | Batch Status: 22400/50000 (45%) | Loss: 2.364986\n",
            "Train Epoch: 3 | Batch Status: 22720/50000 (45%) | Loss: 2.097288\n",
            "Train Epoch: 3 | Batch Status: 23040/50000 (46%) | Loss: 1.919871\n",
            "Train Epoch: 3 | Batch Status: 23360/50000 (47%) | Loss: 2.227561\n",
            "Train Epoch: 3 | Batch Status: 23680/50000 (47%) | Loss: 1.569548\n",
            "Train Epoch: 3 | Batch Status: 24000/50000 (48%) | Loss: 2.329729\n",
            "Train Epoch: 3 | Batch Status: 24320/50000 (49%) | Loss: 2.421911\n",
            "Train Epoch: 3 | Batch Status: 24640/50000 (49%) | Loss: 2.402369\n",
            "Train Epoch: 3 | Batch Status: 24960/50000 (50%) | Loss: 1.613337\n",
            "Train Epoch: 3 | Batch Status: 25280/50000 (51%) | Loss: 1.939407\n",
            "Train Epoch: 3 | Batch Status: 25600/50000 (51%) | Loss: 2.300673\n",
            "Train Epoch: 3 | Batch Status: 25920/50000 (52%) | Loss: 2.721649\n",
            "Train Epoch: 3 | Batch Status: 26240/50000 (52%) | Loss: 1.996522\n",
            "Train Epoch: 3 | Batch Status: 26560/50000 (53%) | Loss: 2.094903\n",
            "Train Epoch: 3 | Batch Status: 26880/50000 (54%) | Loss: 1.683442\n",
            "Train Epoch: 3 | Batch Status: 27200/50000 (54%) | Loss: 2.143964\n",
            "Train Epoch: 3 | Batch Status: 27520/50000 (55%) | Loss: 1.984205\n",
            "Train Epoch: 3 | Batch Status: 27840/50000 (56%) | Loss: 1.967741\n",
            "Train Epoch: 3 | Batch Status: 28160/50000 (56%) | Loss: 2.518753\n",
            "Train Epoch: 3 | Batch Status: 28480/50000 (57%) | Loss: 1.940227\n",
            "Train Epoch: 3 | Batch Status: 28800/50000 (58%) | Loss: 2.265911\n",
            "Train Epoch: 3 | Batch Status: 29120/50000 (58%) | Loss: 1.920694\n",
            "Train Epoch: 3 | Batch Status: 29440/50000 (59%) | Loss: 2.184441\n",
            "Train Epoch: 3 | Batch Status: 29760/50000 (60%) | Loss: 2.038970\n",
            "Train Epoch: 3 | Batch Status: 30080/50000 (60%) | Loss: 1.996630\n",
            "Train Epoch: 3 | Batch Status: 30400/50000 (61%) | Loss: 2.039783\n",
            "Train Epoch: 3 | Batch Status: 30720/50000 (61%) | Loss: 1.914692\n",
            "Train Epoch: 3 | Batch Status: 31040/50000 (62%) | Loss: 2.441863\n",
            "Train Epoch: 3 | Batch Status: 31360/50000 (63%) | Loss: 2.334201\n",
            "Train Epoch: 3 | Batch Status: 31680/50000 (63%) | Loss: 2.359528\n",
            "Train Epoch: 3 | Batch Status: 32000/50000 (64%) | Loss: 2.498884\n",
            "Train Epoch: 3 | Batch Status: 32320/50000 (65%) | Loss: 2.408520\n",
            "Train Epoch: 3 | Batch Status: 32640/50000 (65%) | Loss: 2.023467\n",
            "Train Epoch: 3 | Batch Status: 32960/50000 (66%) | Loss: 2.463572\n",
            "Train Epoch: 3 | Batch Status: 33280/50000 (67%) | Loss: 2.341518\n",
            "Train Epoch: 3 | Batch Status: 33600/50000 (67%) | Loss: 1.820050\n",
            "Train Epoch: 3 | Batch Status: 33920/50000 (68%) | Loss: 1.865680\n",
            "Train Epoch: 3 | Batch Status: 34240/50000 (68%) | Loss: 1.713943\n",
            "Train Epoch: 3 | Batch Status: 34560/50000 (69%) | Loss: 1.723825\n",
            "Train Epoch: 3 | Batch Status: 34880/50000 (70%) | Loss: 2.465254\n",
            "Train Epoch: 3 | Batch Status: 35200/50000 (70%) | Loss: 2.243335\n",
            "Train Epoch: 3 | Batch Status: 35520/50000 (71%) | Loss: 2.182408\n",
            "Train Epoch: 3 | Batch Status: 35840/50000 (72%) | Loss: 2.523382\n",
            "Train Epoch: 3 | Batch Status: 36160/50000 (72%) | Loss: 2.159260\n",
            "Train Epoch: 3 | Batch Status: 36480/50000 (73%) | Loss: 2.135547\n",
            "Train Epoch: 3 | Batch Status: 36800/50000 (74%) | Loss: 1.707718\n",
            "Train Epoch: 3 | Batch Status: 37120/50000 (74%) | Loss: 1.742482\n",
            "Train Epoch: 3 | Batch Status: 37440/50000 (75%) | Loss: 1.919384\n",
            "Train Epoch: 3 | Batch Status: 37760/50000 (75%) | Loss: 1.975253\n",
            "Train Epoch: 3 | Batch Status: 38080/50000 (76%) | Loss: 2.106709\n",
            "Train Epoch: 3 | Batch Status: 38400/50000 (77%) | Loss: 2.037092\n",
            "Train Epoch: 3 | Batch Status: 38720/50000 (77%) | Loss: 2.097855\n",
            "Train Epoch: 3 | Batch Status: 39040/50000 (78%) | Loss: 1.984198\n",
            "Train Epoch: 3 | Batch Status: 39360/50000 (79%) | Loss: 1.928807\n",
            "Train Epoch: 3 | Batch Status: 39680/50000 (79%) | Loss: 2.508418\n",
            "Train Epoch: 3 | Batch Status: 40000/50000 (80%) | Loss: 2.773116\n",
            "Train Epoch: 3 | Batch Status: 40320/50000 (81%) | Loss: 1.495810\n",
            "Train Epoch: 3 | Batch Status: 40640/50000 (81%) | Loss: 1.955623\n",
            "Train Epoch: 3 | Batch Status: 40960/50000 (82%) | Loss: 2.039223\n",
            "Train Epoch: 3 | Batch Status: 41280/50000 (83%) | Loss: 1.574459\n",
            "Train Epoch: 3 | Batch Status: 41600/50000 (83%) | Loss: 2.372020\n",
            "Train Epoch: 3 | Batch Status: 41920/50000 (84%) | Loss: 1.725910\n",
            "Train Epoch: 3 | Batch Status: 42240/50000 (84%) | Loss: 1.949026\n",
            "Train Epoch: 3 | Batch Status: 42560/50000 (85%) | Loss: 2.123006\n",
            "Train Epoch: 3 | Batch Status: 42880/50000 (86%) | Loss: 2.313365\n",
            "Train Epoch: 3 | Batch Status: 43200/50000 (86%) | Loss: 1.660698\n",
            "Train Epoch: 3 | Batch Status: 43520/50000 (87%) | Loss: 1.772092\n",
            "Train Epoch: 3 | Batch Status: 43840/50000 (88%) | Loss: 1.824037\n",
            "Train Epoch: 3 | Batch Status: 44160/50000 (88%) | Loss: 1.859536\n",
            "Train Epoch: 3 | Batch Status: 44480/50000 (89%) | Loss: 2.609381\n",
            "Train Epoch: 3 | Batch Status: 44800/50000 (90%) | Loss: 2.098371\n",
            "Train Epoch: 3 | Batch Status: 45120/50000 (90%) | Loss: 2.129216\n",
            "Train Epoch: 3 | Batch Status: 45440/50000 (91%) | Loss: 1.891409\n",
            "Train Epoch: 3 | Batch Status: 45760/50000 (91%) | Loss: 2.155979\n",
            "Train Epoch: 3 | Batch Status: 46080/50000 (92%) | Loss: 1.916276\n",
            "Train Epoch: 3 | Batch Status: 46400/50000 (93%) | Loss: 2.090782\n",
            "Train Epoch: 3 | Batch Status: 46720/50000 (93%) | Loss: 1.794600\n",
            "Train Epoch: 3 | Batch Status: 47040/50000 (94%) | Loss: 1.682279\n",
            "Train Epoch: 3 | Batch Status: 47360/50000 (95%) | Loss: 2.079094\n",
            "Train Epoch: 3 | Batch Status: 47680/50000 (95%) | Loss: 1.705392\n",
            "Train Epoch: 3 | Batch Status: 48000/50000 (96%) | Loss: 2.113750\n",
            "Train Epoch: 3 | Batch Status: 48320/50000 (97%) | Loss: 2.035420\n",
            "Train Epoch: 3 | Batch Status: 48640/50000 (97%) | Loss: 1.926426\n",
            "Train Epoch: 3 | Batch Status: 48960/50000 (98%) | Loss: 1.664551\n",
            "Train Epoch: 3 | Batch Status: 49280/50000 (99%) | Loss: 2.347711\n",
            "Train Epoch: 3 | Batch Status: 49600/50000 (99%) | Loss: 2.081600\n",
            "Train Epoch: 3 | Batch Status: 49920/50000 (100%) | Loss: 1.962992\n",
            "Training time: 9m 2s\n",
            "Testing time: 9m 39s\n",
            "Train Epoch: 4 | Batch Status: 0/50000 (0%) | Loss: 1.820349\n",
            "Train Epoch: 4 | Batch Status: 320/50000 (1%) | Loss: 2.366711\n",
            "Train Epoch: 4 | Batch Status: 640/50000 (1%) | Loss: 1.725296\n",
            "Train Epoch: 4 | Batch Status: 960/50000 (2%) | Loss: 1.512517\n",
            "Train Epoch: 4 | Batch Status: 1280/50000 (3%) | Loss: 1.815424\n",
            "Train Epoch: 4 | Batch Status: 1600/50000 (3%) | Loss: 2.151994\n",
            "Train Epoch: 4 | Batch Status: 1920/50000 (4%) | Loss: 1.579276\n",
            "Train Epoch: 4 | Batch Status: 2240/50000 (4%) | Loss: 2.210012\n",
            "Train Epoch: 4 | Batch Status: 2560/50000 (5%) | Loss: 1.920040\n",
            "Train Epoch: 4 | Batch Status: 2880/50000 (6%) | Loss: 2.154814\n",
            "Train Epoch: 4 | Batch Status: 3200/50000 (6%) | Loss: 1.915495\n",
            "Train Epoch: 4 | Batch Status: 3520/50000 (7%) | Loss: 1.376577\n",
            "Train Epoch: 4 | Batch Status: 3840/50000 (8%) | Loss: 2.148432\n",
            "Train Epoch: 4 | Batch Status: 4160/50000 (8%) | Loss: 2.012662\n",
            "Train Epoch: 4 | Batch Status: 4480/50000 (9%) | Loss: 1.778770\n",
            "Train Epoch: 4 | Batch Status: 4800/50000 (10%) | Loss: 1.820437\n",
            "Train Epoch: 4 | Batch Status: 5120/50000 (10%) | Loss: 1.493897\n",
            "Train Epoch: 4 | Batch Status: 5440/50000 (11%) | Loss: 1.806433\n",
            "Train Epoch: 4 | Batch Status: 5760/50000 (12%) | Loss: 1.856280\n",
            "Train Epoch: 4 | Batch Status: 6080/50000 (12%) | Loss: 1.928136\n",
            "Train Epoch: 4 | Batch Status: 6400/50000 (13%) | Loss: 1.528938\n",
            "Train Epoch: 4 | Batch Status: 6720/50000 (13%) | Loss: 1.957133\n",
            "Train Epoch: 4 | Batch Status: 7040/50000 (14%) | Loss: 2.111671\n",
            "Train Epoch: 4 | Batch Status: 7360/50000 (15%) | Loss: 2.016612\n",
            "Train Epoch: 4 | Batch Status: 7680/50000 (15%) | Loss: 1.813525\n",
            "Train Epoch: 4 | Batch Status: 8000/50000 (16%) | Loss: 1.561239\n",
            "Train Epoch: 4 | Batch Status: 8320/50000 (17%) | Loss: 1.857700\n",
            "Train Epoch: 4 | Batch Status: 8640/50000 (17%) | Loss: 1.649027\n",
            "Train Epoch: 4 | Batch Status: 8960/50000 (18%) | Loss: 2.217568\n",
            "Train Epoch: 4 | Batch Status: 9280/50000 (19%) | Loss: 1.632374\n",
            "Train Epoch: 4 | Batch Status: 9600/50000 (19%) | Loss: 1.577429\n",
            "Train Epoch: 4 | Batch Status: 9920/50000 (20%) | Loss: 1.813878\n",
            "Train Epoch: 4 | Batch Status: 10240/50000 (20%) | Loss: 1.741881\n",
            "Train Epoch: 4 | Batch Status: 10560/50000 (21%) | Loss: 1.963876\n",
            "Train Epoch: 4 | Batch Status: 10880/50000 (22%) | Loss: 1.790469\n",
            "Train Epoch: 4 | Batch Status: 11200/50000 (22%) | Loss: 1.914050\n",
            "Train Epoch: 4 | Batch Status: 11520/50000 (23%) | Loss: 1.620013\n",
            "Train Epoch: 4 | Batch Status: 11840/50000 (24%) | Loss: 2.123221\n",
            "Train Epoch: 4 | Batch Status: 12160/50000 (24%) | Loss: 1.846993\n",
            "Train Epoch: 4 | Batch Status: 12480/50000 (25%) | Loss: 1.943146\n",
            "Train Epoch: 4 | Batch Status: 12800/50000 (26%) | Loss: 1.910039\n",
            "Train Epoch: 4 | Batch Status: 13120/50000 (26%) | Loss: 1.989681\n",
            "Train Epoch: 4 | Batch Status: 13440/50000 (27%) | Loss: 2.044560\n",
            "Train Epoch: 4 | Batch Status: 13760/50000 (28%) | Loss: 2.070060\n",
            "Train Epoch: 4 | Batch Status: 14080/50000 (28%) | Loss: 1.749133\n",
            "Train Epoch: 4 | Batch Status: 14400/50000 (29%) | Loss: 1.863205\n",
            "Train Epoch: 4 | Batch Status: 14720/50000 (29%) | Loss: 2.071856\n",
            "Train Epoch: 4 | Batch Status: 15040/50000 (30%) | Loss: 1.757302\n",
            "Train Epoch: 4 | Batch Status: 15360/50000 (31%) | Loss: 2.085440\n",
            "Train Epoch: 4 | Batch Status: 15680/50000 (31%) | Loss: 1.492348\n",
            "Train Epoch: 4 | Batch Status: 16000/50000 (32%) | Loss: 2.676486\n",
            "Train Epoch: 4 | Batch Status: 16320/50000 (33%) | Loss: 1.882836\n",
            "Train Epoch: 4 | Batch Status: 16640/50000 (33%) | Loss: 1.651358\n",
            "Train Epoch: 4 | Batch Status: 16960/50000 (34%) | Loss: 1.899123\n",
            "Train Epoch: 4 | Batch Status: 17280/50000 (35%) | Loss: 2.675665\n",
            "Train Epoch: 4 | Batch Status: 17600/50000 (35%) | Loss: 1.805479\n",
            "Train Epoch: 4 | Batch Status: 17920/50000 (36%) | Loss: 1.515756\n",
            "Train Epoch: 4 | Batch Status: 18240/50000 (36%) | Loss: 2.043368\n",
            "Train Epoch: 4 | Batch Status: 18560/50000 (37%) | Loss: 1.517211\n",
            "Train Epoch: 4 | Batch Status: 18880/50000 (38%) | Loss: 1.592244\n",
            "Train Epoch: 4 | Batch Status: 19200/50000 (38%) | Loss: 1.953964\n",
            "Train Epoch: 4 | Batch Status: 19520/50000 (39%) | Loss: 1.483317\n",
            "Train Epoch: 4 | Batch Status: 19840/50000 (40%) | Loss: 2.062271\n",
            "Train Epoch: 4 | Batch Status: 20160/50000 (40%) | Loss: 1.833902\n",
            "Train Epoch: 4 | Batch Status: 20480/50000 (41%) | Loss: 2.275773\n",
            "Train Epoch: 4 | Batch Status: 20800/50000 (42%) | Loss: 1.406438\n",
            "Train Epoch: 4 | Batch Status: 21120/50000 (42%) | Loss: 1.680597\n",
            "Train Epoch: 4 | Batch Status: 21440/50000 (43%) | Loss: 1.754331\n",
            "Train Epoch: 4 | Batch Status: 21760/50000 (44%) | Loss: 2.038120\n",
            "Train Epoch: 4 | Batch Status: 22080/50000 (44%) | Loss: 1.791198\n",
            "Train Epoch: 4 | Batch Status: 22400/50000 (45%) | Loss: 1.948758\n",
            "Train Epoch: 4 | Batch Status: 22720/50000 (45%) | Loss: 1.497794\n",
            "Train Epoch: 4 | Batch Status: 23040/50000 (46%) | Loss: 1.989702\n",
            "Train Epoch: 4 | Batch Status: 23360/50000 (47%) | Loss: 1.757556\n",
            "Train Epoch: 4 | Batch Status: 23680/50000 (47%) | Loss: 1.826346\n",
            "Train Epoch: 4 | Batch Status: 24000/50000 (48%) | Loss: 1.759096\n",
            "Train Epoch: 4 | Batch Status: 24320/50000 (49%) | Loss: 1.575621\n",
            "Train Epoch: 4 | Batch Status: 24640/50000 (49%) | Loss: 2.366501\n",
            "Train Epoch: 4 | Batch Status: 24960/50000 (50%) | Loss: 1.519485\n",
            "Train Epoch: 4 | Batch Status: 25280/50000 (51%) | Loss: 1.796468\n",
            "Train Epoch: 4 | Batch Status: 25600/50000 (51%) | Loss: 1.757460\n",
            "Train Epoch: 4 | Batch Status: 25920/50000 (52%) | Loss: 1.491459\n",
            "Train Epoch: 4 | Batch Status: 26240/50000 (52%) | Loss: 1.967433\n",
            "Train Epoch: 4 | Batch Status: 26560/50000 (53%) | Loss: 1.788616\n",
            "Train Epoch: 4 | Batch Status: 26880/50000 (54%) | Loss: 1.834605\n",
            "Train Epoch: 4 | Batch Status: 27200/50000 (54%) | Loss: 1.436496\n",
            "Train Epoch: 4 | Batch Status: 27520/50000 (55%) | Loss: 1.977642\n",
            "Train Epoch: 4 | Batch Status: 27840/50000 (56%) | Loss: 1.999106\n",
            "Train Epoch: 4 | Batch Status: 28160/50000 (56%) | Loss: 2.092753\n",
            "Train Epoch: 4 | Batch Status: 28480/50000 (57%) | Loss: 2.021893\n",
            "Train Epoch: 4 | Batch Status: 28800/50000 (58%) | Loss: 1.721438\n",
            "Train Epoch: 4 | Batch Status: 29120/50000 (58%) | Loss: 1.694770\n",
            "Train Epoch: 4 | Batch Status: 29440/50000 (59%) | Loss: 1.902217\n",
            "Train Epoch: 4 | Batch Status: 29760/50000 (60%) | Loss: 1.646405\n",
            "Train Epoch: 4 | Batch Status: 30080/50000 (60%) | Loss: 2.104060\n",
            "Train Epoch: 4 | Batch Status: 30400/50000 (61%) | Loss: 2.087821\n",
            "Train Epoch: 4 | Batch Status: 30720/50000 (61%) | Loss: 1.404522\n",
            "Train Epoch: 4 | Batch Status: 31040/50000 (62%) | Loss: 1.780800\n",
            "Train Epoch: 4 | Batch Status: 31360/50000 (63%) | Loss: 1.171301\n",
            "Train Epoch: 4 | Batch Status: 31680/50000 (63%) | Loss: 1.986372\n",
            "Train Epoch: 4 | Batch Status: 32000/50000 (64%) | Loss: 1.441853\n",
            "Train Epoch: 4 | Batch Status: 32320/50000 (65%) | Loss: 1.474684\n",
            "Train Epoch: 4 | Batch Status: 32640/50000 (65%) | Loss: 2.153019\n",
            "Train Epoch: 4 | Batch Status: 32960/50000 (66%) | Loss: 1.856757\n",
            "Train Epoch: 4 | Batch Status: 33280/50000 (67%) | Loss: 1.399288\n",
            "Train Epoch: 4 | Batch Status: 33600/50000 (67%) | Loss: 1.729178\n",
            "Train Epoch: 4 | Batch Status: 33920/50000 (68%) | Loss: 2.033522\n",
            "Train Epoch: 4 | Batch Status: 34240/50000 (68%) | Loss: 1.662262\n",
            "Train Epoch: 4 | Batch Status: 34560/50000 (69%) | Loss: 1.917205\n",
            "Train Epoch: 4 | Batch Status: 34880/50000 (70%) | Loss: 1.873714\n",
            "Train Epoch: 4 | Batch Status: 35200/50000 (70%) | Loss: 1.542490\n",
            "Train Epoch: 4 | Batch Status: 35520/50000 (71%) | Loss: 1.928813\n",
            "Train Epoch: 4 | Batch Status: 35840/50000 (72%) | Loss: 0.983699\n",
            "Train Epoch: 4 | Batch Status: 36160/50000 (72%) | Loss: 2.062010\n",
            "Train Epoch: 4 | Batch Status: 36480/50000 (73%) | Loss: 1.508537\n",
            "Train Epoch: 4 | Batch Status: 36800/50000 (74%) | Loss: 1.943344\n",
            "Train Epoch: 4 | Batch Status: 37120/50000 (74%) | Loss: 1.343434\n",
            "Train Epoch: 4 | Batch Status: 37440/50000 (75%) | Loss: 1.981213\n",
            "Train Epoch: 4 | Batch Status: 37760/50000 (75%) | Loss: 1.619262\n",
            "Train Epoch: 4 | Batch Status: 38080/50000 (76%) | Loss: 1.503541\n",
            "Train Epoch: 4 | Batch Status: 38400/50000 (77%) | Loss: 1.986570\n",
            "Train Epoch: 4 | Batch Status: 38720/50000 (77%) | Loss: 1.895791\n",
            "Train Epoch: 4 | Batch Status: 39040/50000 (78%) | Loss: 1.806597\n",
            "Train Epoch: 4 | Batch Status: 39360/50000 (79%) | Loss: 2.278600\n",
            "Train Epoch: 4 | Batch Status: 39680/50000 (79%) | Loss: 1.650560\n",
            "Train Epoch: 4 | Batch Status: 40000/50000 (80%) | Loss: 1.627247\n",
            "Train Epoch: 4 | Batch Status: 40320/50000 (81%) | Loss: 1.851999\n",
            "Train Epoch: 4 | Batch Status: 40640/50000 (81%) | Loss: 1.602880\n",
            "Train Epoch: 4 | Batch Status: 40960/50000 (82%) | Loss: 1.317650\n",
            "Train Epoch: 4 | Batch Status: 41280/50000 (83%) | Loss: 1.837589\n",
            "Train Epoch: 4 | Batch Status: 41600/50000 (83%) | Loss: 1.803064\n",
            "Train Epoch: 4 | Batch Status: 41920/50000 (84%) | Loss: 1.855316\n",
            "Train Epoch: 4 | Batch Status: 42240/50000 (84%) | Loss: 1.857574\n",
            "Train Epoch: 4 | Batch Status: 42560/50000 (85%) | Loss: 1.497665\n",
            "Train Epoch: 4 | Batch Status: 42880/50000 (86%) | Loss: 2.266344\n",
            "Train Epoch: 4 | Batch Status: 43200/50000 (86%) | Loss: 1.526999\n",
            "Train Epoch: 4 | Batch Status: 43520/50000 (87%) | Loss: 1.621264\n",
            "Train Epoch: 4 | Batch Status: 43840/50000 (88%) | Loss: 1.196272\n",
            "Train Epoch: 4 | Batch Status: 44160/50000 (88%) | Loss: 1.367089\n",
            "Train Epoch: 4 | Batch Status: 44480/50000 (89%) | Loss: 1.312732\n",
            "Train Epoch: 4 | Batch Status: 44800/50000 (90%) | Loss: 1.640057\n",
            "Train Epoch: 4 | Batch Status: 45120/50000 (90%) | Loss: 2.067566\n",
            "Train Epoch: 4 | Batch Status: 45440/50000 (91%) | Loss: 1.701281\n",
            "Train Epoch: 4 | Batch Status: 45760/50000 (91%) | Loss: 1.809232\n",
            "Train Epoch: 4 | Batch Status: 46080/50000 (92%) | Loss: 1.682049\n",
            "Train Epoch: 4 | Batch Status: 46400/50000 (93%) | Loss: 1.993518\n",
            "Train Epoch: 4 | Batch Status: 46720/50000 (93%) | Loss: 2.359915\n",
            "Train Epoch: 4 | Batch Status: 47040/50000 (94%) | Loss: 1.333172\n",
            "Train Epoch: 4 | Batch Status: 47360/50000 (95%) | Loss: 1.949104\n",
            "Train Epoch: 4 | Batch Status: 47680/50000 (95%) | Loss: 1.393692\n",
            "Train Epoch: 4 | Batch Status: 48000/50000 (96%) | Loss: 1.284018\n",
            "Train Epoch: 4 | Batch Status: 48320/50000 (97%) | Loss: 1.648381\n",
            "Train Epoch: 4 | Batch Status: 48640/50000 (97%) | Loss: 1.858122\n",
            "Train Epoch: 4 | Batch Status: 48960/50000 (98%) | Loss: 2.073264\n",
            "Train Epoch: 4 | Batch Status: 49280/50000 (99%) | Loss: 1.833064\n",
            "Train Epoch: 4 | Batch Status: 49600/50000 (99%) | Loss: 2.070178\n",
            "Train Epoch: 4 | Batch Status: 49920/50000 (100%) | Loss: 2.710147\n",
            "Training time: 9m 1s\n",
            "Testing time: 9m 38s\n",
            "Total Time: 38m 34s\n",
            "Model was trained on cuda!\n"
          ]
        }
      ]
    }
  ]
}